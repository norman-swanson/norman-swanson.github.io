%2multibyte Version: 5.50.0.2960 CodePage: 936


\documentclass[11pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage[onehalfspacing]{setspace}
\usepackage{setspace}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{Codepage=936}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Sunday, July 18, 2004 16:10:34}
%TCIDATA{LastRevised=Saturday, April 09, 2022 17:51:52}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=article.cst}
%TCIDATA{PageSetup=72,72,72,72,0}
%TCIDATA{AllPages=
%F=36,\PARA{038<p type="texpara" tag="Body Text" >\hfill \thepage}
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\renewcommand{\baselinestretch}{1.0} 
\textwidth=6.8in
\textheight=8.7in
\oddsidemargin=0in
\evensidemargin=0in
\topmargin=0in
\baselineskip=10pt
\linespread{1.3}
\input{tcilatex}
\geometry{left=1in,right=1in,top=1.25in,bottom=1.25in}

\begin{document}


\begin{center}
{\Large {Consistent Estimation, Variable Selection, and Forecasting in
Factor-Augmented VAR Models$^*$}}

\bigskip

John C. Chao$^{1}$ and Norman R. Swanson$^{2}$

\medskip

$^{1}$University of Maryland and $^{2}$Rutgers University

\medskip

March 18, 2022

\bigskip \bigskip

Abstract
\end{center}

\begin{spacing}{1.01}
\noindent In the context of latent factor models that are widely used in economics, a
common assumption made is one of factor pervasiveness, which implies that
all available predictor or informative variables in a dataset, with the
possible exception of a negligible number of them, load significantly on the
underlying factors. In this paper, we analyze the more likely scenario where
there is significant underlying heterogeneity in the sense that some of the
variables load significantly on the underlying factors, while others are
irrelevant, in the sense that they do not share any common dynamic structure
with each other or with the relevant variables in the data set. We show
that, even in such a setting, consistent factor estimation can be achieved
if one pre-screens the variables and successfully prunes out the irrelevant
ones. To do so, we introduce a new variable selection procedure that, with
probability approaching one, correctly distinguishes between relevant and
irrelevant variables. We study this problem within a factor-augmented VAR
(FAVAR) framework, and show that by using variables selected via our
pre-screening procedure to estimate the underlying factors and then
inserting these factor estimates into $h$-step ahead forecasting equations
implied by the FAVAR model, we can obtain consistent estimates of the
conditional mean function of said equations. In particular, our methodology
allows the conditional mean function of a factor-augmented forecast equation
to be consistently estimable in a wide range of situations, including cases
where violation of factor pervasiveness is such that consistent estimation
is precluded in the absence of variable pre-screening.
\end{spacing}

\bigskip \bigskip \bigskip

\noindent \textit{Keywords: }Factor analysis, factor augmented vector
autoregression, forecasting, moderate deviation, principal components,
self-normalization, variable selection.

\medskip

\noindent \textit{JEL Classification: }C32, C33, C38, C52, C53, C55.

\bigskip \bigskip

\begin{spacing}{1.01}
\noindent $^{\ast }$\textit{Corresponding Author:} John C. Chao, Department of Economics, 7343 Preinkert
Drive, University of Maryland, chao@econ.umd.edu.

\medskip

\noindent Norman R. Swanson, Department of Economics, 9500 Hamilton Street, Rutgers University,
nswanson@econ.rutgers.edu. The authors are grateful to Simon Freyaldenhoven,
Yuan Liao, Minchul Shin, Xiye Yang, and seminar participants at the Federal
Reserve Bank of Pihadelphia for useful comments received on earlier versions
of this paper. Chao thanks the University of Maryland for research support.
\end{spacing}

\newpage

\noindent \noindent \setcounter{page}{2}

\section{Introduction}

\noindent \qquad As a result of the astounding rate at which raw information
is currently being accumulated, there is a clear need for variable
selection, dimension reduction and shrinkage techniques when analyzing big
data using machine learning techniques. This has led to a profusion of novel
research in areas ranging from the analysis of high dimensional and/or high
frequency datasets to statistical learning methods. Needless to say, there
are many critical unanswered questions in this burgeoning literature. One
such question, which we address in this paper stems from the pathbreaking
work due to Bai and Ng (2002), Stock and Watson (2002a,b), Bai (2003),
Forni, Hallin, Lippi, and Reichlin (2005), and Bai and Ng (2008). In these
papers, the authors develop methods for constructing forecasts based on
factor-augmented regression models. An obvious appeal of using factor
analytical methods for this problem is the capacity for dimension reduction,
so that in terms of the specification of the forecasting equation,
employment of a factor structure allows information embedded in a possibly
high-dimensional vector of (predictor or information) variables to be
represented in a parsimonious manner.

Within this context, we note that a key assumption, commonly used in the
factor analysis literature to show consistent factor estimation, is the
so-called factor pervasiveness assumption which presupposes that all
available predictor or information variables in a dataset, with the possible
exception of a negligible number of them, load significantly on the
underlying factors\footnote{%
A more formal definition of factor pervasiveness in given in the next
section of this paper.}. Such an assumption places stringent requirements on
the relationship between variables in a given dataset and, thus, may not be
satisfied by many datasets that are available for empirical research. A more
likely scenario might be that there is significant underlying heterogeneity,
so that some of the available variables are relevant in the sense that they
load significantly on the underlying factors, whereas others are irrelevant,
in the sense that they do not share any common dynamic structure with each
other or with the relevant variables in the dataset. Given this lack of
pervasiveness, inconsistency in factor estimation may result if one were to
naively use all available variables to estimate the underlying factors,
without regard to whether they are relevant or not. This is demonstrated in
a counterexample introduced in the next section. Not being able to obtain
consistent estimates of the underlying factors, in turn, would clearly cause
problems for empirical researchers, such as when the objective is to
estimate forecast functions that incorporate estimated factors. On the other
hand, if one were to pre-screen the variables and successfully prune out the
irrelevant ones, then consistent estimation can be achieved, assuming of
course that the number of relevant variables is sufficiently large, in a
sense that will be made formal later. Hence, a main contribution of this
paper is to introduce a novel variable selection procedure which allows
empirical researchers to, with probability approaching one, correctly
distinguish the relevant from the irrelevant variables, prior to factor
estimation. We study this problem within a factor-augmented VAR (FAVAR)
framework, thus allowing time series forecasts to be made using information
sets much richer than that used in traditional VAR models. Our results show
that by using variables selected via our pre-screening procedure to estimate
the underlying factors, and then inserting these factor estimates into $h$%
-step ahead forecasting equations implied by a FAVAR model, we can
consistently estimate the conditional mean function of the said equations.
This allows the conditional mean function of a factor-augmented forecasting
equation to be consistently estimable in a wide range of situations, and in
particular in situations where violation of factor pervasiveness is such
that consistent estimation is precluded in the absence of variable
pre-screening. Moreover, there are also clear benefits to using our
procedure in cases where weaker pervasiveness assumptions, such as that
discussed in Bai and Ng (2021), characterize the data.

The research reported here is related to the well-known supervised principal
components method proposed by Bair, Hastie, Paul, and Tibshirani (2006).
Additionally, our research is related to some interesting recent work by
Giglio, Xiu, and Zhang (2021), who propose a method for selecting test
assets, with the objective of estimating risk premia in a Fama-MacBeth type
framework. A crucial difference between the variable selection procedure
proposed in our paper and those proposed in these papers is that we use a
score statistic that is self-nomalized, whereas the aforementioned papers do
not make use of statistics that involve self-normalization. An important
advantage of self-normalized statistics is their ability to accommodate a
much wider range of possible tail behavior in the underlying distributions,
relative to their non-self-normalized counterparts. This makes
self-normalized statistics better suited for various types of economic and
financial applications, where the data are known not to exhibit the type of
exponentially decaying tail behavior assumed in much of the statistics
literature on high-dimensional models. In addition, the type of models
studied in Bair, Hastie, Paul, and Tibshirani (2006) and Giglio, Xiu, and
Zhang (2021) differ significantly from the FAVAR model studied here. In
particular, Bair, Hastie, Paul, and Tibshirani (2006) study a one-factor
model in an $i.i.d.$ Gaussian framework so that complications introduced by
dependence and non-normality of distribution are not considered in their
paper. Giglio, Xiu, and Zhang (2021) do make certain high-level assumptions
which may potentially accommodate some dependence both cross-sectionally and
intertemporally, but the model that they consider is very different from the
type of dynamic vector time series model studied here.

In another important related paper, Bai and Ng (2021) provide results which
show that factors can still be estimated consistently in certain situations
where the factor loadings are weaker than that implied by the conventional
pervasiveness assumption, although in such cases the rate of convergence of
the factor estimator is slower and additional assumptions are needed. As
discussed in the next section of this paper, their factor consistency result
relies on a key condition, and the appropriateness of this condition depends
on how severely the condition of factor pervasiveness is violated, which is
ultimately an empirical issue. In this context, various authors have
documented cases in economics-related research where empirical results
suggest that the underlying factors may be quite weak, so that the rate
condition given in Bai and Ng (2021) may not be appropriate. See, for
example, the discussions in Jagannathan and Wang (1998), Kan and Zhang
(1999), Harding (2008), Kleibergen (2009), Ontaski (2012), Bryzgalova
(2016), Burnside (2016), Gospodinov, Kan, and Robotti (2017), Anatolyev and
Mikusheva (2021), and Freyaldenhoven (2021a,b).

Finally, it is worth pointing out that our variable selection procedure
differs substantially from the approach to variable/model selection taken in
much of the traditional econometrics literature. In particular, we show that
important moderate deviation results obtained recently by Chen, Shao, Wu,
and Xu (2016) can be used to help control the probability of a Type I error,
i.e., the error that an irrelevant variable which is not informative about
the underlying factors is falsely selected as a relevant variable. This is
so even in situations where the number of irrelevant variables is very large
and even if the tails of the underlying distributions do not satisfy the
kind of sub-exponential behavior typically assumed by large deviation
inequalities used in high-dimensional analysis. Hence, we are able to design
a variable selection procedure where the probability of a Type I error goes
to zero, as the sample sizes grows to infinity. This fact, taken together
with the fact that the probability of a Type II error for our procedure also
goes to zero asymptotically, allows us to establish that our variable
selection procedure is completely consistent, in the sense that both Type I
and Type II errors go to zero in the limit. This property of complete
consistency is important because if we try to simply control the probability
of a Type I error at some predetermined level, which is the typical approach
in multiple hypothesis testing, then we will not in general be able to
estimate the factors consistently, even up to an invertible matrix
transformation, and in consequence, we will have fallen short of our
ultimate goal of obtaining a consistent estimate of the conditional mean
function of the factor-augmented forecasting equation.

The rest of the paper is organized as follows. In Section 2 , we provide our
counterexample, stated formally as Theorem 1, which shows that a latent
factor may be inconsistently estimated when the standard assumption of
factor pervasiveness does not hold. In Section 3, we discuss the FAVAR model
and the assumptions that we impose on this model. We also describe our
variable selection procedure and provide theoretical results establishing
the complete consistency of the procedure. Section 4 provides theoretical
results on the consistent estimation of latent factors, up to an invertible
matrix transformation, as well as results on the consistent estimation of
the $h$-step ahead predictor, based on the FAVAR model. Section 5 presents
the results of a promising Monte Carlo study on the finite sample
performance of our variable selection method, and makes recommendations
regarding calibration of the tuning parameter used in said method. Finally,
Section 6 offers concluding remarks. Proofs of the main theorems as well as
additional supporting lemmas are given in a separate technical supplement.%
\footnote{%
The technical appendix is posted online at:
http://econweb.rutgers.edu/nswanson/papers/AppConFacVarSel-03-18-2022.pdf
and also at http://econweb.umd.edu/\symbol{126}chao/Research/research%
\_files/AppConFacVarSel-03-18-2022.pdf.} The technical supplement is
organized into four subappendices. Appendix A provides proofs of the main
theorems. Appendix B provides proofs of lemmas used in proving Theorem 1,
and Appendix C contains proofs of supporting lemmas, used primarily in the
proofs of Theorems 2 and 3. Finally, Appendix D contains the proofs of
supporting lemmas used primarily in the proofs of Theorems 4 and 5.

Before proceeding, we first say a few words about some of the frequently
used notation in this paper. Throughout, let $\lambda _{\left( j\right)
}\left( A\right) $, $\lambda _{\max }\left( A\right) $, $\lambda _{\min
}\left( A\right) $, and $tr\left( A\right) $ denote, respectively, the $%
j^{th}$ largest eigenvalue, the maximal eigenvalue, the minimal eigenvalue,
and the trace of a square matrix $A$. Similarly, let $\sigma _{\left(
j\right) }\left( B\right) $, $\sigma _{\max }\left( B\right) $, and $\sigma
_{\min }\left( B\right) $ denote, respectively, the $j^{th}$ largest
singular value, the maximal singular value, and the minimal singular value
of a matrix $B$, which is not restricted to be a square matrix. In addition,
let $\left\Vert a\right\Vert _{2}$ denote the usual Euclidean norm when
applied to a (finite-dimensional) vector $a$. Also, for a matrix $A$, $%
\left\Vert A\right\Vert _{2}\equiv \max \left\{ \sqrt{\lambda \left(
A^{\prime }A\right) }:\lambda \left( A^{\prime }A\right) \text{ is an
eigenvalue of }A^{\prime }A\right\} $ denotes the matrix spectral norm, and $%
\left\Vert A\right\Vert _{F}\equiv \sqrt{tr\left\{ A^{\prime }A\right\} }$
denotes the Frobenius norm. For two random variables $X$ and $Y$, write $%
X\sim Y,$ if $X/Y=O_{p}\left( 1\right) $ and $Y/X=O_{p}\left( 1\right) $.
Furthermore, let $\left\vert z\right\vert $ denote the absolute value or the
modulus of the number $z$; let $\left\lfloor \cdot \right\rfloor $ denote
the floor function, so that $\left\lfloor x\right\rfloor $ gives the integer
part of the real number $x$, and let $\iota _{p}=\left( 1,1,...,1\right)
^{\prime }$ denote a $p\times 1$ vector of ones. Finally, the abbreviation
w.p.a.1 stands for \textquotedblleft with probability approaching
one\textquotedblright .

\section{Inconsistency in High-Dimensional Factor Estimation}

To provide some motivation for the problem we will be studying in this
paper, consider the following simple, stylized one-factor model:

\begin{equation}
\underset{N\times 1}{Z_{t}}=\underset{N\times 1}{\gamma }\underset{1\times 1}%
{f_{t}}+\underset{N\times 1}{u_{t}}\text{, }t=1,...,T
\label{one factor model}
\end{equation}%
for which we make the following assumption.

\noindent \textbf{Assumption 2-1: }(a)\textbf{\ }$\left\{ u_{t}\right\}
\equiv i.i.d.N\left( 0,I_{N}\right) ;$(b) $\left\{ f_{t}\right\} \equiv
i.i.d.N\left( 0,1\right) ;$and (c) $u_{s}$ and $f_{t}$ are independent for
all $t,s.$

\noindent Much of the literature on factor analysis focuses on the case
where the factors are pervasive. In the special case of the simple one
factor model given in expression (\ref{one factor model}) above,
pervasiveness means that:%
\begin{equation*}
\frac{\left\Vert \gamma \right\Vert _{2}^{2}}{N}\rightarrow c,
\end{equation*}%
for some constant $c$ such that $0<c<\infty ,$ where $\left\Vert \gamma
\right\Vert _{2}=\sqrt{\gamma ^{\prime }\gamma }$. In practice, however, one
may have a high-dimensional data vector $Z_{t}$ such that not all of the
components of $Z_{t}$ load significantly on the underlying factor, $f_{t}$.
In particular, let $\mathcal{P}$ be a permutation matrix which reorders the
components of $Z_{t}$, so that $\mathcal{P}Z_{t}$ can be partitioned as
follows:%
\begin{equation*}
\mathcal{P}Z_{t}=\left( 
\begin{array}{c}
\underset{N_{1}\times 1}{Z_{t}^{\left( 1\right) }} \\ 
\underset{N_{2}\times 1}{Z_{t}^{\left( 2\right) }}%
\end{array}%
\right) ,
\end{equation*}%
where $Z_{t}^{\left( 1\right) }=\gamma ^{\left( 1\right)
}f_{t}+u_{t}^{\left( 1\right) }$ and $Z_{t}^{\left( 2\right) }=u_{t}^{\left(
2\right) }$ and where all components of the $N_{1}\times 1$ vector $\gamma
^{\left( 1\right) }$ are different from zero, so that the components of $%
Z_{t}^{\left( 1\right) }$ all load significantly on $f_{t}$, whereas the
components of $Z_{t}^{\left( 2\right) }$ do not. Of course, an empirical
researcher will not typically have \`{a} priori knowledge as to which
components of $Z_{t}$ will load significantly on $f_{t}$ and which will not.
The following result shows that if one proceeds with factor estimation
assuming that the factor is pervasive, then the usual estimator of a factor
based on principal component methods may be inconsistent and may, in fact,
behave in a rather pathological manner in large samples. To consider this
possibility, assume the following condition, which implies a violation of
the pervasiveness assumption.

\noindent \textbf{Assumption 2-2: }As $N,T\rightarrow \infty $, let $%
\left\Vert \gamma \right\Vert _{2}\rightarrow \infty $ such that: 
\begin{equation*}
\frac{N}{T\left\Vert \gamma \right\Vert _{2}^{2\left( 1+\kappa \right) }}%
=c+o\left( \frac{1}{\left\Vert \gamma \right\Vert _{2}^{2}}\right) \text{, }
\end{equation*}%
for some constant $c,$ such that $0<c<\infty ,$ and for some constant $%
\kappa ,$ such that $0<\kappa <1$. Note that under Assumption 2-2:%
\begin{equation*}
\frac{\left\Vert \gamma \right\Vert _{2}^{2}}{N}\sim \left( TN^{\kappa
}\right) ^{-\frac{{\large 1}}{\left( {\large 1{\LARGE +}}{\Large \kappa }%
\right) }}\rightarrow 0\text{ as }N,T\rightarrow \infty ,
\end{equation*}%
so that the factor does not satisfy the pervasiveness assumption. This can,
of course, occur if a significant proportion of the components of $\gamma $
are zero or are very small. Next, let $\widehat{\pi }_{1}/\left\Vert 
\widehat{\pi }_{1}\right\Vert _{2}$ denote the (normalized) eigenvector
associated with the largest eigenvalue of the sample covariance matrix, $%
\widehat{\Sigma }_{Z}=\mathbf{Z}^{\prime }\mathbf{Z}/T$, where $\mathbf{Z}%
=\left( Z_{1},...,Z_{T}\right) ^{\prime }.$ Then, the usual principal
component estimator of $f_{t}$ is given by:%
\begin{equation*}
\widehat{f}_{t}=\frac{\left\langle \widehat{\pi }_{1},Z_{t}\right\rangle }{%
\sqrt{N}\left\Vert \widehat{\pi }_{1}\right\Vert _{2}}\text{.}
\end{equation*}%
The following theorem characterizes the asymptotic behavior of this
estimator under the assumptions given above.

\noindent \textbf{Theorem 1:}\textit{\ Suppose that Assumptions 2-1 and 2-2
hold. Then, for all }$t$\textit{: }$\widehat{f}_{t}\overset{p}{\rightarrow }%
0,$ as $N,T\rightarrow \infty .$\textit{\ }

\noindent It is well-known that without further identifying assumptions,
such as those given in Assumption F1 of Stock and Watson (2002a), factors
can only be estimated consistently up to an invertible matrix
transformation. However, even in cases where we are not willing to specify
enough conditions so as to fully identify the factors, estimating the
factors consistently up to an invertible matrix transformation will often
suffice for many purposes. One such case is when we are trying to forecast
using a factor-augmented vector autoregression (FAVAR). As we will show in
results given in Section 4 of this paper, point forecasts constructed using
factors which are estimated consistently up to an invertible matrix
transformation will nevertheless converge in probability to the desired
infeasible forecast (i.e., the conditional mean of the FAVAR), that in turn
depends on the true unobserved factors. On the other hand, the problem
illustrated by the result given in Theorem 1 is different and is in some
sense more problematic and pathological. The estimated factor in Theorem 1
converges to zero regardless of what happens to be the realized value of the
true latent factor. In this case, one clearly cannot consistently estimate
the conditional mean of the FAVAR.

Theorem 1 is related to results previously given in the statistics
literature showing the possible inconsistency of sample eigenvectors as
estimators of population eigenvectors in high dimensional situations. See,
for example, Paul (2007), Johnstone and Lu (2009), Shen, Shen, Zhu, and
Marron (2016), and Johnstone and Paul (2018). However, most of the results
in the statistics literature are not explicitly framed in the setting of a
factor model, but are instead derived for the related spiked covariance
model. Theorem 1 is intended to give an inconsistency result of this type,
but in a context that may be more familiar to researchers in economics.

It should also be noted that, in an interesting and thought-provoking recent
paper, Bai and Ng (2021) provide results which show that factors can still
be estimated consistently in certain situations where the factor loadings
are weaker than that implied by the conventional pervasiveness assumption,
but that in such cases the rate of convergence is slower and additional
assumptions are needed. To understand the relationship between their results
and the example given above, note that a key condition for the consistency
result given in their paper, when expressed in terms of our notation, is the
assumption that $N/\left( T\left\Vert \gamma \right\Vert _{2}^{2}\right)
\rightarrow 0$\footnote{%
See Assumption A4 of Bai and Ng (2021). Note that Bai and Ng (2021) state
this condition in the form $N/\left( TN^{\alpha }\right) \rightarrow 0,$ for
some $\alpha \in \left( 0,1\right] $, but since part (ii) of their
Assumption A2, when specialized to the one factor model studied here,
simplifies to the condition that $\lim_{N\rightarrow \infty }\left\Vert
\gamma \right\Vert _{2}^{2}/N^{\alpha }=\sigma _{\Lambda }>0$, it is easy to
see that their Assumption A4 is equivalent to the condition that $N/\left(
T\left\Vert \gamma \right\Vert _{2}^{2}\right) \rightarrow 0$.}. On the
other hand, if $N/\left( T\left\Vert \gamma \right\Vert _{2}^{2}\right)
\rightarrow c_{1},$ for some positive constant $c_{1},$ or even worse, if $%
N/\left( T\left\Vert \gamma \right\Vert _{2}^{2}\right) \rightarrow \infty $%
, which is essentially what is specified in Assumption 2-2 above, then
consistent factor estimation cannot be achieved\footnote{%
Note that Assumption 2-2 is actually stronger than required in order to show
inconsistency, but that we impose this condition to highlight the fact that,
in this case, not only is the estimator of the factor inconsistent but it
actually converges to zero.}. Hence, whether or not consistent factor
estimation can be attained depends on how nonpervasive the factors are,
which is ultimately an empirical question, and which depends on the
application and on the dataset employed. Moreover, various authors have now
documented cases where empirical results suggest that the underlying factors
may be quite weak, so that the rate condition given in Bai and Ng (2021) may
not be appropriate, at least for some of the situations for which factor
modeling is of interest. For example, see Jagannathan and Wang (1998), Kan
and Zhang (1999), Harding (2008), Kleibergen (2009), Ontaski (2012),
Bryzgalova (2016), Burnside (2016), Gospodinov, Kan, and Robotti (2017),
Anatolyev and Mikusheva (2021), and Freyaldenhoven (2021a,b). In such cases,
it is of interest to explore the possibility that the weakness in the
loadings is not uniform across all variables, but rather is due to the fact
that only a small percentage of the variables loads significantly on the
underlying factors. Furthermore, even if the empirical situation of interest
is one where, strictly speaking, the condition $N/\left( T\left\Vert \gamma
\right\Vert _{2}^{2}\right) \rightarrow 0$ does hold, it may still be
beneficial in some such instances to do variable pre-screening. This is
particularly true in situations where the condition $N/\left( T\left\Vert
\gamma \right\Vert _{2}^{2}\right) \rightarrow 0$ is \textquotedblleft
barely\textquotedblright\ satisfied, in which case one would expect to pay a
rather hefty finite sample price for not pruning out variables that do not
load significantly on the underlying factors, since these variables will add
unwanted noise to the estimation process. For all these reasons, there is a
clear need to develop methods that will enable empirical researchers to
pre-screen the components of $Z_{t},$ so that variables which are
informative and helpful to the estimation process can be properly identified.

\section{\noindent Model, Assumptions, and Variable Selection in High
Dimensions}

\noindent \qquad Consider the following $p^{th}$-order factor-augmented
vector autoregression (FAVAR):%
\begin{equation}
W_{t+1}=\mu +A_{1}W_{t}+\cdot \cdot \cdot +A_{p}W_{t-p+1}+\varepsilon _{t+1}%
\text{,}  \label{FAVAR}
\end{equation}%
where%
\begin{eqnarray*}
\underset{\left( d+K\right) \times 1}{W_{t+1}} &=&\left( 
\begin{array}{c}
\underset{d\times 1}{Y_{t+1}} \\ 
\underset{K\times 1}{F_{t+1}}%
\end{array}%
\right) \text{, }\underset{\left( d+K\right) \times 1}{\varepsilon _{t+1}}%
=\left( 
\begin{array}{c}
\underset{d\times 1}{\varepsilon _{t+1}^{Y}} \\ 
\underset{K\times 1}{\varepsilon _{t+1}^{F}}%
\end{array}%
\right) \text{, }\underset{\left( d+K\right) \times 1}{\mu }=\left( 
\begin{array}{c}
\underset{d\times 1}{\mu _{Y}} \\ 
\underset{K\times 1}{\mu _{F}}%
\end{array}%
\right) ,\text{ and} \\
\text{ }\underset{\left( d+K\right) \times \left( d+K\right) }{A_{g}}
&=&\left( 
\begin{array}{cc}
\underset{d\times d}{A_{YY,g}} & \underset{d\times K}{A_{YF,g}} \\ 
\underset{K\times d}{A_{FY,g}} & \underset{K\times K}{A_{FF,g}}%
\end{array}%
\right) ,\text{ for }g=1,...,p.
\end{eqnarray*}%
Here, $Y_{t}$ denotes the vector of observable economic variables, and $%
F_{t} $ is a vector of unobserved (latent) factors. In our analysis of this
model, it will often be convenient to rewrite the FAVAR in several
alternative forms, such as when making assumptions used in the sequel. We
thus briefly outline two alternative representations of the above model.
First, it is easy to see that the system of equations given in (\ref{FAVAR})
can be written in the form:%
\begin{eqnarray}
Y_{t+1} &=&\mu _{Y}+A_{YY}\underline{Y}_{t}+A_{YF}\underline{F}%
_{t}+\varepsilon _{t+1}^{Y},  \label{Y component FAVAR} \\
F_{t+1} &=&\mu _{F}+A_{FY}\underline{Y}_{t}+A_{FF}\underline{F}%
_{t}+\varepsilon _{t+1}^{F},  \label{F component FAVAR}
\end{eqnarray}%
where%
\begin{eqnarray}
\underset{d\times dp}{A_{YY}} &=&\left( 
\begin{array}{cccc}
A_{YY,1} & A_{YY,2} & \cdots & A_{YY,p}%
\end{array}%
\right) ,\text{ }\underset{d\times Kp}{A_{YF}}=\left( 
\begin{array}{cccc}
A_{YF,1} & A_{YF,2} & \cdots & A_{YF,p}%
\end{array}%
\right) ,\text{ }  \notag \\
\underset{K\times dp}{A_{FY}} &=&\left( 
\begin{array}{cccc}
A_{FY,1} & A_{FY,2} & \cdots & A_{FY,p}%
\end{array}%
\right) ,\text{ }\underset{K\times Kp}{A_{FF}}=\left( 
\begin{array}{cccc}
A_{FF,1} & A_{FF,2} & \cdots & A_{FF,p}%
\end{array}%
\right) ,  \notag \\
\underset{dp\times 1}{\underline{Y}_{t}} &=&\left( 
\begin{array}{c}
Y_{t} \\ 
Y_{t-1} \\ 
\vdots \\ 
Y_{t-p{\LARGE +}1}%
\end{array}%
\right) \text{, and}\underset{Kp\times 1}{\underline{F}_{t}}=\left( 
\begin{array}{c}
F_{t} \\ 
F_{t-1} \\ 
\vdots \\ 
F_{t-p{\LARGE +}1}%
\end{array}%
\right) \text{. }  \label{Yunderscore and Funderscore}
\end{eqnarray}%
Another useful representation of the FAVAR model is the so-called companion
form, wherein the $p^{th}$-order model given in expression (\ref{FAVAR}) is
written in terms of a first-order model:%
\begin{equation*}
\underset{\left( d+K\right) p\times 1}{\underline{W}_{t}}=\alpha +A%
\underline{W}_{t-1}+E_{t}\text{,}
\end{equation*}%
where $\underline{W}_{t}=\left( 
\begin{array}{ccccc}
W_{t}^{\prime } & W_{t-1}^{\prime } & \cdots & W_{t-p{\LARGE +}2}^{\prime }
& W_{t-p{\LARGE +}1}^{\prime }%
\end{array}%
\right) ^{\prime }$ and where%
\begin{equation}
\alpha =\left( 
\begin{array}{c}
\mu \\ 
0 \\ 
\vdots \\ 
0 \\ 
0%
\end{array}%
\right) \text{, }A=\left( 
\begin{array}{ccccc}
A_{1} & A_{2} & \cdots & A_{p-1} & A_{p} \\ 
I_{d+K} & 0 & \cdots & 0 & 0 \\ 
0 & I_{d+K} & \ddots & \vdots & 0 \\ 
\vdots & \ddots & \ddots & 0 & \vdots \\ 
0 & \cdots & 0 & I_{d+K} & 0%
\end{array}%
\right) \text{, and }E_{t}=\left( 
\begin{array}{c}
\varepsilon _{t} \\ 
0 \\ 
\vdots \\ 
0 \\ 
0%
\end{array}%
\right) \text{.}  \label{companion form notations}
\end{equation}%
This companion form is convenient for establishing certain moment conditions
on $\underline{Y}_{t}$ and $\underline{F}_{t},$ given a moment condition on $%
\varepsilon _{t},$ and for establishing certain mixing properties of the
FAVAR model, as shown in the proofs of Lemmas C-5 and Lemma C-11 given in
Appendix C.

In addition to observations on $Y_{t}$, suppose that the data set available
to researchers includes a vector of time series variables which are related
to the unobserved factors in the following manner:%
\begin{equation}
\underset{N\times 1}{Z_{t}}\text{ }=\text{ }\Gamma \underline{F}_{t}+u_{t}%
\text{, }  \label{overspecified factor model}
\end{equation}%
where the properties of $u_{t}$ are given in Assumptions 3-3 and 3-4, below.
Now, assume that not all components of $Z_{t}$ provide useful information
for estimating the unobserved vector, $\underline{F}_{t}$, so that the $%
N\times Kp$ parameter matrix $\Gamma $ may have some rows whose elements are
all zero. More precisely, let the $1\times Kp$ vector, $\gamma _{i}^{\prime
},$ denote the $i^{th}$ row of $\Gamma $, and assume that the rows of the
matrix $\Gamma $ can be divided into two classes:%
\begin{eqnarray}
H &=&\left\{ k\in \left\{ 1,....,N\right\} :\gamma _{k}=0\right\} \text{ and}
\label{H} \\
H^{c} &=&\left\{ k\in \left\{ 1,....,N\right\} :\gamma _{k}\neq 0\right\} 
\text{.}  \label{Hc}
\end{eqnarray}%
Hence, similar to what has been discussed in the previous section, there
exists a permutation matrix $\mathcal{P}$ such that $\mathcal{P}Z_{t}=\left( 
\begin{array}{cc}
Z_{t}^{\left( 1\right) \prime } & Z_{t}^{\left( 2\right) \prime }%
\end{array}%
\right) ^{\prime },$ where%
\begin{eqnarray}
\underset{N_{1}\times 1}{Z_{t}^{\left( 1\right) }} &=&\Gamma _{1}\underline{F%
}_{t}+u_{t}^{\left( 1\right) }  \label{Z(1)} \\
\underset{N_{2}\times 1}{Z_{t}^{\left( 2\right) }} &=&u_{t}^{\left( 2\right)
}\text{.}  \label{Z(2)}
\end{eqnarray}%
The above representation suggests that the components of $Z_{t}^{\left(
1\right) }$ can be interpreted as some sort of \textquotedblleft
information\textquotedblright\ variables, as the information that they
supply will be helpful in estimating $\underline{F}_{t}$. On the other hand,
for the purpose of factor estimation, the components of the subvector $%
Z_{t}^{\left( 2\right) }$ are pure \textquotedblleft
noise\textquotedblright\ variables, as they do not load on the underlying
factors and only add noise if they are included in the factor estimation
process. Given that an empirical researcher will often not have prior
knowledge as to which variables are elements of $Z_{t}^{\left( 1\right) }$
and which are elements of $Z_{t}^{\left( 2\right) }$, Theorem 1 suggests the
need for a variable selection procedure which will allow us to properly
identify the components of of $Z_{t}^{\left( 1\right) }$ and to use only
these variables when we try to estimate $\underline{F}_{t}$; for, if we
unknowingly include too many components of $Z_{t}^{\left( 2\right) }$ in the
estimation process, then inconsistent estimation in the sense described in
the previous section can result.

To provide a variable selection procedure with provable guarantees, we must
first specify a number of conditions on the FAVAR model defined above.

\noindent \textbf{Assumption 3-1: }Suppose that:%
\begin{equation}
\det \left\{ I_{\left( d+K\right) }-A_{1}z-\cdot \cdot \cdot
-A_{p}z^{p}\right\} =0,\text{ implies that }\left\vert z\right\vert >1\text{.%
}  \label{stability cond}
\end{equation}

\noindent \textbf{Assumption 3-2: }Let $\varepsilon _{t}$ satisfy the
following set of conditions: (a) $\left\{ \varepsilon _{t}\right\} $ is an
independent sequence of random vectors with $E\left[ \varepsilon _{t}\right]
=0$ $\forall t$; (b) there exists a positive constant $C$ such that $%
\sup_{t}E\left\Vert \varepsilon _{t}\right\Vert _{2}^{6}\leq C<\infty $; (c) 
$\varepsilon _{t}$ admits a density $g_{\varepsilon _{t}}$ such that, for
some positive constant $M<\infty ,\sup_{t}\dint \left\vert g_{\varepsilon
_{t}}\left( \upsilon -u\right) -g_{\varepsilon _{t}}\left( \upsilon \right)
\right\vert d\varepsilon \leq M\left\vert u\right\vert $, whenever $%
\left\vert u\right\vert \leq \overline{\kappa }$ for some constant $%
\overline{\kappa }>0$; and (d) there exists a constant $\underline{C}>0$
such that $\inf_{t}\lambda _{\min }\left\{ E\left[ \varepsilon
_{t}\varepsilon _{t}^{\prime }\right] \right\} \geq \underline{C}>0$.

\noindent \textbf{Assumption 3-3: }Let $u_{i,t}$ be the $i^{th}$ element of
the error vector $u_{t}$ in expression (\ref{overspecified factor model}),
and we assume that it satisfies the following conditions: (a) $E\left[
u_{i,t}\right] =0$ for all $i$ and $t$; (b) there exists a positive constant 
$\overline{C}$ such that $\sup_{i,t}E\left\vert u_{i,t}\right\vert ^{7}\leq 
\overline{C}<\infty $, and there exists a constant $\underline{C}>0$ such
that $\inf_{i,t}E\left[ u_{i,t}^{2}\right] \geq \underline{C}$; (c) define $%
\mathcal{F}_{i,-\infty }^{t}=\sigma \left(
....,u_{i,t-2},u_{i,t-1},u_{t}\right) $, $\mathcal{F}_{i,t+m}^{\infty
}=\sigma \left( u_{i,t+m},u_{i,t+m+1},u_{i,t+m+2},....\right) $, and 
\begin{equation*}
\beta _{i}\left( m\right) =\sup_{t}E\left[ \sup \left\{ \left\vert P\left( B|%
\mathcal{F}_{i,-\infty }^{t}\right) -P\left( B\right) \right\vert :B\in 
\mathcal{F}_{i,t+m}^{\infty }\right\} \right] \text{.}
\end{equation*}%
Assume that there exist constants $a_{1}>0$ and $a_{2}>0$ such that%
\begin{equation*}
\beta _{i}\left( m\right) \leq a_{1}\exp \left\{ -a_{2}m\right\} ,\text{ for
all }i\text{;}
\end{equation*}%
and (d) there exists a positive constant $C$ such that $\sup_{t}\left( \frac{%
1}{N_{1}}\dsum\limits_{i\in H^{{\large c}}}\dsum\limits_{k\in H^{{\large c}%
}}\left\vert E\left[ u_{i,t}u_{k,t}\right] \right\vert \right) \leq C<\infty 
$ for every positive integer $N_{1}$, where $H^{{\large c}}$ is defined in
expression (\ref{Hc}) above.

\noindent \textbf{Assumption 3-4: }$\varepsilon _{t}$ and $u_{i,s}$ are
independent, for all $i,t,$ and $s$.

\noindent \textbf{Assumption 3-5: }There exists a positive constant $%
\overline{C},$ such that $\sup_{i\in H^{{\large c}}}\left\Vert \gamma
_{i}\right\Vert _{2}\leq \overline{C}<\infty $ and $\left\Vert \mu
\right\Vert _{2}\leq \overline{C}<\infty $, where $\mu =\left( \mu
_{Y}^{\prime },\mu _{F}^{\prime }\right) ^{\prime }$.

\noindent \textbf{Assumption 3-6: }There exists a positive constant $%
\overline{C},$ such that:%
\begin{equation*}
0<\frac{1}{\overline{C}}\leq \lambda _{\min }\left( \frac{\Gamma ^{\prime
}\Gamma }{N_{1}}\right) \leq \lambda _{\max }\left( \frac{\Gamma ^{\prime
}\Gamma }{N_{1}}\right) \leq \overline{C}<\infty \text{ for all }N_{1}\text{%
, }N_{2}\text{ sufficiently large,}
\end{equation*}%
where $N_{1}$ is the number of components of the subvector $Z_{t}^{\left(
1\right) }$ and $N_{2}$ is the number of components of the subvector $%
Z_{t}^{\left( 2\right) }$, as previously defined in expressions (\ref{Z(1)})
and (\ref{Z(2)}).

\noindent \textbf{Assumption 3-7: }Let $A$ be as defined in expression (\ref%
{companion form notations}) above, and let the eigenvalues of the matrix $%
I_{\left( d+K\right) p}-A$ be sorted so that:%
\begin{equation*}
\left\vert \lambda _{\left( 1\right) }\left( I_{\left( d+K\right)
p}-A\right) \right\vert \geq \left\vert \lambda _{\left( 2\right) }\left(
I_{\left( d+K\right) p}-A\right) \right\vert \geq \cdot \cdot \cdot \geq
\left\vert \lambda _{\left( \left( d+K\right) p\right) }\left( I_{\left(
d+K\right) p}-A\right) \right\vert =\overline{\phi }_{\min }\text{.}
\end{equation*}%
Suppose that there is a constant $\underline{C}>0$ such that%
\begin{equation}
\sigma _{\min }\left( I_{\left( d+K\right) p}-A\right) \geq \underline{C}%
\overline{\phi }_{\min }  \label{lower bd I-A}
\end{equation}%
In addition, there exists a positive constant $\overline{C}<\infty $ such
that, for all positive integer $j$, 
\begin{equation}
\sigma _{\max }\left( A^{j}\right) \leq \overline{C}\max \left\{ \left\vert
\lambda _{\max }\left( A^{j}\right) \right\vert ,\left\vert \lambda _{\min
}\left( A^{j}\right) \right\vert \right\} .  \label{upper bd A}
\end{equation}

\noindent \textbf{Remark 3.1:}

\noindent \textbf{(a)} Note that Assumption 3-1 is the stability condition
that one typically assumes for a stationary VAR process. One difference is
that we allow for possible heterogeneity in the distribution of $\varepsilon
_{t}$ across time, so that our FAVAR process is not necessarily a strictly
stationary process. Under Assumption 3-1, there exists a vector moving
average representation for the FAVAR process.

\noindent \textbf{(b) }It is well known that $\det \left\{ I_{\left(
d+K\right) }-Az\right\} =\det \left\{ I_{\left( d+K\right) }-A_{1}z-\cdot
\cdot \cdot -A_{p}z^{p}\right\} ,$ where $A$ is the coefficient matrix of
the companion form given in expression (\ref{companion form notations}).
See, for example, page 16 of L\"{u}tkepohl (2005). It follows that
Assumption 3-1 is equivalent to the condition that 
\begin{equation}
\det \left\{ I_{\left( d+K\right) }-Az\right\} =0\text{ implies that }%
\left\vert z\right\vert >1.  \label{alternative stability cond}
\end{equation}%
In addition, Assumption 3-1 is also, of course, equivalent to the assumption
that all eigenvalues of $A$ have modulus less than $1$.

\noindent \textbf{(c) }Since the factor loading matrix $\Gamma $ is an $%
N\times Kp$ matrix, where $N=N_{1}+N_{2}$, the matrix $\Gamma ^{\prime
}\Gamma $ will have order of magnitude equal to $N$ if the factors are
pervasive. Much of the factor analysis literature in both econometrics and
statistics has studied the case where factors are pervasive in this sense.
For example, see Bai and Ng (2002), Stock and Watson (2002a), Bai (2003),
and Fan, Liao, and Mincheva (2011, 2013). Assumption 3-6 allows for possible
violations of this conventional pervasiveness assumption, which will occur
in our setup when $N_{1}/N\rightarrow 0$.

\noindent \textbf{(d)} Assumption 3-7 imposes a condition whereby the
extreme singular values of the matrices $A^{j}$ and $I_{\left( d+K\right)
p}-A$ have bounds that depend on the extreme eigenvalues of these matrices.
More primitive conditions for such a relationship between the singular
values and the eigenvalues of a (not necessarily symmetric) matrix have been
studied in the linear algebra literature. In Appendix C of this paper, we
prove one such result which extends a well-known result by Ruhe (1975). More
specifically, we state and prove the following lemma:

\smallskip

\noindent \textbf{Lemma C-9: }\textit{Let }$A$\textit{\ be an }$n\times n$%
\textit{\ square matrix with (ordered) singular values given by:}%
\begin{equation*}
\sigma _{\left( 1\right) }\left( A\right) \geq \sigma _{\left( 2\right)
}\left( A\right) \geq \cdot \cdot \cdot \geq \sigma _{\left( n\right)
}\left( A\right) \geq 0\text{.}
\end{equation*}%
\textit{Suppose that }$A$\textit{\ is diagonalizable, i.e., }$A=S\Lambda
S^{-1},$\textit{\ where }$\Lambda $\textit{\ is diagonal matrix whose
diagonal elements are the eigenvalues of }$A$\textit{. Let the modulus of
these eigenvalues be ordered as follows: }%
\begin{equation*}
\left\vert \lambda _{\left( 1\right) }\left( A\right) \right\vert \geq
\left\vert \lambda _{\left( 2\right) }\left( A\right) \right\vert \geq \cdot
\cdot \cdot \geq \left\vert \lambda _{\left( n\right) }\left( A\right)
\right\vert \text{.}
\end{equation*}%
\textit{Then, for }$k\in \left\{ 1,...,n\right\} $\textit{\ and for any
positive integer }$j$\textit{, we have that:}%
\begin{equation*}
\chi \left( S\right) ^{-1}\left\vert \lambda _{\left( k\right) }\left(
A^{j}\right) \right\vert \leq \sigma _{\left( k\right) }\left( A^{j}\right)
\leq \chi \left( S\right) \left\vert \lambda _{\left( k\right) }\left(
A^{j}\right) \right\vert \text{ }
\end{equation*}%
\textit{where}%
\begin{equation*}
\chi \left( S\right) =\sigma _{\left( 1\right) }\left( S\right) \sigma
_{\left( 1\right) }\left( S^{-1}\right) \text{.}
\end{equation*}

\smallskip

\noindent Note that in the special case where the matrices $A$ and $%
I_{\left( d+K\right) p}-A$ are diagonalizable, the inequalities given in
expressions (\ref{lower bd I-A}) and (\ref{upper bd A}) are a direct
consequence of this lemma. On the other hand, Assumption 3-7 takes into
account other situations where expressions (\ref{lower bd I-A}) and (\ref%
{upper bd A}) are valid even though the matrices $A$ and $I_{\left(
d+K\right) p}-A$ are not diagonalizable.

\noindent \textbf{(e)} Assumptions 3-1, 3-2(a)-(c), and 3-7 together allow
us to show in Lemma C-11 of Appendix C that the process $\left\{
W_{t}\right\} $ generated by the FAVAR model given in expression (\ref{FAVAR}%
) is a $\beta $-mixing process with $\beta $-mixing coefficient satisfying:%
\begin{equation*}
\beta _{W}\left( m\right) \leq a_{1}\exp \left\{ -a_{2}m\right\} ,
\end{equation*}%
for some positive constants $a_{1}$ and $a_{2}$, with 
\begin{equation*}
\beta _{W}\left( m\right) =\sup_{t}E\left[ \sup \left\{ \left\vert P\left( B|%
\mathcal{A}_{-\infty }^{t}\right) -P\left( B\right) \right\vert :B\in 
\mathcal{A}_{t+m}^{\infty }\right\} \right] ,
\end{equation*}%
and with $\mathcal{A}_{-\infty }^{t}=\sigma \left(
...,W_{t-2},W_{t-1},W_{t}\right) $ and $\mathcal{A}_{t+m}^{\infty }=\sigma
\left( W_{t+m},W_{t+m+1},W_{t+m+2},....\right) $. Note that Assumption 3-2
(c) rules out situations such as that given in the famous counterexample
presented by Andrews (1984) which shows that a first-order autoregression
with errors having a discrete Bernoulli distribution is not $\alpha $%
-mixing, even if it satisfies the stability condition. Conditions similar to
Assumption 3-2(c) have also appeared in previous papers, such as Gorodetskii
(1977) and Pham and Tran (1985), which seek to provide sufficient conditions
for establishing the $\alpha $ or $\beta $ mixing properties of linear time
series processes.

Our variable selection procedure is based on a self-normalized statistic and
makes use of some pathbreaking moderate deviation results for weakly
dependent processes recently obtained by Chen, Shao, Wu, and Xu (2016). An
advantage of using a self-normalized statistic is that doing so allows us to
impose much weaker moment conditions, even when $N$ is much larger than $T$.
In particular, as can be seen from Assumptions 3-2 and 3-3 above, we only
make moment conditions that are of a polynomial order on the errors
processes $\left\{ \varepsilon _{t}\right\} $ and $\left\{ u_{it}\right\} $.
Such conditions are substantially weaker than assumptions of Gaussianity or
sub-Gaussianity which have been made in papers studying high-dimensional
factor models and/or high-dimensional covariance matrices, without employing
statistics that are self-normalized\footnote{%
For example, see Bickel and Levina (2008) and Fan, Liao, and Mincheva (2013).%
}.

To accommodate data dependence, we consider self-nomalized statistics that
are constructed from observations which are first split into blocks in a
manner similar to the kind of construction one would employ in implementing
a block bootstrap or in proving a central limit theorem using the blocking
technique. Two such statistics are proposed in this paper. The first of
these statistics has the form of an $\ell _{\infty }$ norm and is given by: 
\begin{equation}
\max_{1\leq \ell \leq d}\left\vert S_{i,\ell ,T}\right\vert =\max_{1\leq
\ell \leq d}\left\vert \frac{\overline{S}_{i,\ell ,T}}{\sqrt{\overline{V}%
_{i,\ell ,T}}}\right\vert ,  \label{max statistic}
\end{equation}%
where 
\begin{eqnarray}
\overline{S}_{i,\ell ,T} &=&\dsum\limits_{r=1}^{q}\dsum\limits_{t=\left(
r-1\right) \tau +p}^{\left( r-1\right) \tau +\tau _{1}+p-1}Z_{it}y_{\ell ,t%
{\LARGE +}1}\text{ and}  \label{numerator max stat} \\
\overline{V}_{i,\ell ,T} &=&\dsum\limits_{r=1}^{q}\left[ \dsum\limits_{t=%
\left( r-1\right) \tau +p}^{\left( r-1\right) \tau +\tau
_{1}+p-1}Z_{it}y_{\ell ,t{\LARGE +}1}\right] ^{2}\text{.}
\label{denominator max stat}
\end{eqnarray}%
Here, $Z_{it}$ denotes the $i^{th}$ component of $Z_{t}$ , $y_{\ell ,t+1}$
denotes the $\ell ^{th}$ component of $Y_{t+1}$, $\tau _{1}=\left\lfloor
T_{0}^{\alpha _{{\large 1}}}\right\rfloor $, and $\tau _{2}=\left\lfloor
T_{0}^{\alpha _{{\large 2}}}\right\rfloor $, where $1>\alpha _{1}\geq \alpha
_{2}>0$, $\tau =\tau _{1}+\tau _{2}$, $q=\left\lfloor T_{0}/\tau
\right\rfloor $, and $T_{0}=T-p+1$. Note that the statistic given in
expression (\ref{max statistic}) can be interpreted as the maximum of the
(self-normalized) sample covariances between the $i^{th}$ component of $%
Z_{t} $ and the components of $Y_{t+1}$. Our second statistic has the form
of a pseudo-$L_{1}$ norm and is given by: 
\begin{equation*}
\dsum\limits_{\ell =1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert
=\dsum\limits_{\ell =1}^{d}\varpi _{\ell }\left\vert \frac{\overline{S}%
_{i,\ell ,T}}{\sqrt{\overline{V}_{i,\ell ,T}}}\right\vert ,
\end{equation*}%
where $\overline{S}_{i,\ell ,T}$ and $\overline{V}_{i,\ell ,T}$ are as
defined in expressions (\ref{numerator max stat}) and (\ref{denominator max
stat}) above and where $\left\{ \varpi _{\ell }:\ell =1,..,d\right\} $
denotes pre-specified weights, such that $\varpi _{\ell }\geq 0,$ for every $%
\ell \in \left\{ 1,...,d\right\} $ and $\dsum\nolimits_{\ell =1}^{d}\varpi
_{\ell }=1$. Both of these statistics employ a blocking scheme similar to
that proposed in Chen, Shao, Wu, and Xu (2016), where, in order to keep the
effects of dependence under control, the construction of these statistics is
based only on observations in every other block. To see this, note that if
we write out the \textquotedblleft numerator\textquotedblright\ term $%
\overline{S}_{i,\ell ,T}$ in greater detail, we have that:%
\begin{eqnarray}
\overline{S}_{i,\ell ,T} &=&\dsum\limits_{t=p}^{\tau _{1}+p-1}Z_{it}y_{\ell
,t{\LARGE +}1}+\dsum\limits_{t=\tau +p}^{\tau +\tau _{1}+p-1}Z_{it}y_{\ell ,t%
{\LARGE +}1}  \notag \\
&&+\dsum\limits_{t=2\tau +p}^{2\tau +\tau _{1}+p-1}Z_{it}y_{\ell ,t{\LARGE +}%
1}+\cdot \cdot \cdot +\dsum\limits_{t=\left( q-1\right) \tau +p}^{\left(
q-1\right) \tau +\tau _{1}+p-1}Z_{it}y_{\ell ,t{\LARGE +}1}
\label{sum of Z times y}
\end{eqnarray}%
Comparing the first term and the second terms on the right-hand side of
expression (\ref{sum of Z times y}), we see that the observations $%
Z_{it}y_{\ell ,t{\LARGE +}1}$, for $t=\tau _{1}+p,...,\tau +p-1$, have not
been included in the construction of the sum. Similar observations hold when
comparing the second and the third terms, and so on.

It should also be pointed out that although we make use of some of their
fundamental results on moderate deviation, both the model studied in our
paper and the objective of our paper are very different from that of Chen,
Shao, Wu, and Xu (2016). Whereas Chen, Shao, Wu, and Xu \textbf{(}2016%
\textbf{)} focus their analysis on problems of testing and inference for the
mean of a scalar weakly dependent time series using self-normalized
Student-type test statistics, our paper applies the self-normalization
approach to a variable selection problem in a FAVAR setting. Indeed, the
problem which we study here is in some sense more akin to a classification
(or model selection) problem rather than a multiple hypothesis testing
problem. In order to consistently estimate the factors up to an invertible
matrix transformation, we need to develop a variable selection procedure
whereby both the probability of a false positive and the probability of a
false negative converge to zero as $N_{1}$, $N_{2}$, $T\rightarrow \infty $%
\footnote{%
Here, a false positive refers to mis-classifying a variable, $Z_{it},$ as a
relevant variable for the purpose of factor estimation when its factor
loading $\gamma _{i}^{\prime }=0$, whereas a false negative refers to the
opposite case, where $\gamma _{i}^{\prime }\neq 0,$ but the variable $Z_{it}$
is mistakenly classified as irrelevant.}. This is different from the typical
multiple hypothesis testing approach whereby one tries to control the
familywise error rate (or, alternatively, the false discovery rate), so that
it is no greater than $0.05,$ say, but does not try to ensure that this
probability goes to zero as the sample size grows.

To determine whether the $i^{th}$ component of $Z_{t}$ is a relevant
variable for the purpose of factor estimation, we propose the following
procedure. Define $i\in \widehat{H}^{c}$ to indicate that the procedure has
classified $Z_{it}$ to be a relevant variable for the purpose of factor
estimation. \textbf{Similarly,} define $i\in \widehat{H}$ to indicate that
the procedure has classified $Z_{it}$ to be an irrelevant variable. Now, let 
$\mathbb{S}_{i,T}^{+}$ denote either the statistic $\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $ or the statistic $%
\dsum\nolimits_{\ell =1}^{d}\varpi _{\ell }\left\vert S_{i,\ell
,T}\right\vert $. Our variable selection procedure is based on the decision
rule: 
\begin{equation}
i\in \left\{ 
\begin{array}{cc}
\widehat{H}^{c} & \text{ if }\mathbb{S}_{i,T}^{+}\geq \Phi ^{-1}\left( 1-%
\frac{\varphi }{2N}\right) \\ 
\widehat{H} & \text{if }\mathbb{S}_{i,T}^{+}<\Phi ^{-1}\left( 1-\frac{%
\varphi }{2N}\right)%
\end{array}%
\right. ,  \label{var selection decision rule}
\end{equation}%
where $\Phi ^{-1}\left( \cdot \right) $ denotes the quantile function or the
inverse of the cumulative distribution function of the standard normal
random variable, and where $\varphi $ is a tuning parameter which may depend
on $N$. Some conditions on $\varphi $ will be given in Assumptions 3-11 and
3-11* below.

\noindent \textbf{Remarks 3.2:}

\noindent \textbf{(a)} To understand why using the quantile function of the
standard normal as the threshold function for our procedure is a natural
choice, note first that, by a slight modification of the arguments given in
the proof of Lemma C-17\footnote{%
The statement and proof of Lemma C-17 are provided in Appendix C of the
Technical Appendix to this paper which can be found at
http://econweb.umd.edu/\symbol{126}chao/Research/research\_files/Appendix%
\_Consistent\_Estimation\_
\par
\noindent
Variable\_Selection\_and\_Forecasting\_in\_FAVAR\_Models-03-18-2022.pdf}, we
can show that, as $T\rightarrow \infty $%
\begin{equation}
P\left( \left\vert S_{i,\ell ,T}\right\vert \geq z\right) =2\left[ 1-\Phi
\left( z\right) \right] \left( 1+o\left( 1\right) \right) ,
\label{moderate dev result}
\end{equation}%
which holds for all $i$ and $\ell $ and for all $z$ such that

\noindent $0\leq z\leq c_{0}\min \left\{ T^{\left( 1-{\large \alpha }%
_{1}\right) /6}/L\left( T\right) ,T^{\alpha _{2}/2}\right\} $, where $%
L\left( T\right) $ denotes a slowly varying function such that $L\left(
T\right) \rightarrow \infty $ as $T\rightarrow \infty $. In view of
expression (\ref{moderate dev result}), we can interpret moderate deviation
as providing an asymptotic approximation of the (two-sided) tail behavior of
the statistic, $S_{i,\ell ,T},$ based on the tails of the standard normal
distribution. Now, suppose initially that we wish simply to control the
probability of a Type I error for testing the null hypothesis $H_{0}:\gamma
_{i}=0$ (i.e., the $i^{th}$ variable does not load on the underlying
factors) at some fixed level $\alpha $. Then, expression (\ref{moderate dev
result}) suggests that a natural way to do this is to set $z=\Phi
^{-1}\left( 1-\alpha /2\right) $. This is because, given that the quantile
function $\Phi ^{-1}\left( \cdot \right) $ is, by definition, the inverse
function of the cdf $\Phi \left( \cdot \right) $, we have that: 
\begin{eqnarray*}
P\left( \left\vert S_{i,\ell ,T}\right\vert \geq \Phi ^{-1}\left( 1-\alpha
/2\right) \right) &\leq &2\left[ 1-\Phi \left( \Phi ^{-1}\left( 1-\alpha
/2\right) \right) \right] \left( 1+o\left( 1\right) \right) \\
&=&\alpha \left( 1+o\left( 1\right) \right) ,
\end{eqnarray*}%
so that the probability of a Type I error is controlled at the desired
level, $\alpha ,$ asymptotically. Note also that an advantage of moderate
deviation theory is that it gives a characterization of the relative
approximation error, as opposed to the absolute approximation error. As a
result, the approximation given is useful and meaningful even when $\alpha $
is very small, which is of importance to us since we are interested in
situations where we might want to let $\alpha $ go to zero, as sample size
approaches infinity.

We give the above example to provide intuition concerning the form of the
threshold function that we have specified. The variable selection problem
that we actually consider is more complicated than what is illustrated by
this example, since we need to control the probability of a Type I error (or
of a false positive) not just for a single test involving the $i^{th}$
variable but for all variables simultaneously. Moreover, as noted
previously, we also need the probability of a false positive to go to zero
asymptotically, if we want to be able to estimate factors consistently, even
up to an invertible matrix transformation. We show in Theorem 2 below that
these objectives can all be accomplished using the threshold function
specified in expression (\ref{var selection decision rule}), since a
threshold function of this form makes it easy for us to properly control the
probability of a false positive in large samples.

\noindent \textbf{(b) }The threshold function used here is reminiscent of
the one employed in a celebrated paper by Belloni, Chen, Chernozhukov, and
Hansen (2012). More specifically, Belloni, Chen, Chernozhukov, and Hansen
(2012) use a similar threshold function to help set the penalty level for
Lasso estimation of the first-stage equation of an IV regression model
assuming $i.n.i.d$. data. In spite of the similarity in the form of the
threshold function, the problem studied in that paper is very different from
the one which we analyze here. In consequence, the conditions we specify for
setting the tuning parameter $\varphi $ will also be quite different from
what they recommend in their paper.

Under appropriate conditions, the variable selection procedure described
above can be shown to be consistent, in the sense that both the probability
of a false positive, i.e. $P\left( i\in \widehat{H}^{c}|i\in H\right) $, and
the probability of a false negative, i.e., $P\left( i\in \widehat{H}|i\in
H^{c}\right) $, approach zero as $N$, $T\rightarrow \infty $. To show this
result, we must first state a number of additional assumptions.

\noindent \textbf{Assumption 3-8: }There exists a positive constant, $%
\underline{c},$ such that for $T$ sufficiently large: 
\begin{equation*}
\min_{1\leq \ell \leq d}\min_{i\in H}\min_{r\in \left\{ 1,...,q\right\}
}E\left\{ \left[ \frac{1}{\sqrt{\tau _{1}}}\dsum\limits_{t=\left( r-1\right)
\tau +p}^{\left( r-1\right) \tau +\tau _{1}+p-1}y_{\ell ,t{\LARGE +}1}u_{it}%
\right] ^{2}\right\} \geq \underline{c},
\end{equation*}%
where, as defined earlier,%
\begin{equation*}
\tau _{1}=\left\lfloor T_{0}^{\alpha _{{\large 1}}}\right\rfloor \text{, }%
\tau _{2}=\left\lfloor T_{0}^{\alpha _{{\large 2}}}\right\rfloor \text{ for }%
1>\alpha _{1}\geq \alpha _{2}>0\text{ and }q=\left\lfloor \frac{T_{0}}{\tau
_{1}+\tau _{2}}\right\rfloor ,
\end{equation*}%
and $T_{0}=T-p+1$.

\noindent \textbf{Assumption 3-9: }Let $i\in H^{c}=\left\{ k\in \left\{
1,....,N\right\} :\gamma _{k}\neq 0\right\} .$Suppose that there exists a
positive constant, $\underline{c},$ such that, for all $N_{1},N_{2},$and $T$
sufficiently large:%
\begin{eqnarray*}
&&\min_{1\leq \ell \leq d}\min_{{\large i\in }H^{{\large c}}}\left\vert 
\frac{\mu _{i,\ell ,T}}{q\tau _{1}}\right\vert \\
&=&\min_{1\leq \ell \leq d}\min_{{\large i\in }H^{{\large c}}}\left\vert 
\frac{1}{q}\dsum\limits_{r=1}^{q}\frac{1}{\tau _{1}}\dsum\limits_{t=\left(
r-1\right) \tau +p}^{\left( r-1\right) \tau +\tau _{1}+p-1}\gamma
_{i}^{\prime }\left\{ E\left[ \underline{F}_{t}\right] \mu _{Y,\ell }+E\left[
\underline{F}_{t}\underline{Y}_{t}^{\prime }\right] \alpha _{YY,\ell }+E%
\left[ \underline{F}_{t}\underline{F}_{t}^{\prime }\right] \alpha _{YF,\ell
}\right\} \right\vert \\
&\geq &\underline{c}>0,
\end{eqnarray*}%
where $\mu _{Y,\ell }=e_{\ell ,d}^{\prime }\mu _{Y}$, $\alpha _{YY,\ell
}=A_{YY}^{\prime }e_{\ell ,d}$, and $\alpha _{YF,\ell }=A_{YF}^{\prime
}e_{\ell ,d}.$ Here, $e_{\ell ,d}$ is a $d\times 1$ elementary vector whose $%
\ell ^{th}$ component is $1$ and all other components are $0$.

\noindent \textbf{Assumption 3-10: }Suppose that, as $N_{1}$, $N_{2}$, and $%
T\rightarrow \infty $, the following rate conditions hold:

\begin{enumerate}
\item[(a)] 
\begin{equation*}
\frac{\sqrt{\ln N}}{T^{\min \left\{ \frac{{\large 1-\alpha }_{{\large 1}}}{%
{\large 6}},\frac{{\Large \alpha }_{{\large 2}}}{{\large 2}}\right\} }}%
\rightarrow 0\text{ }
\end{equation*}%
where $1>\alpha _{1}\geq \alpha _{2}>0$ and $N=N_{1}+N_{2}$.

\item[(b)] 
\begin{equation*}
\frac{N_{1}}{T^{3\alpha _{{\large 1}}}}\rightarrow 0\text{ where }1>\alpha
_{1}>0\text{.}
\end{equation*}
\end{enumerate}

\noindent \textbf{Assumption 3-11: }Let $\varphi $ satisfy the following two
conditions: (a) $\varphi \rightarrow 0$ as $N_{1},N_{2}\rightarrow \infty $,
and (b) there exists some constant $a>0,$ such that $\varphi \geq \frac{1}{%
N^{a}},$ for all $N_{1},N_{2}$ sufficiently large.

\noindent \textbf{Remarks 3.3:}

\noindent \textbf{(a) }Assumption 3-9 imposes the condition that there
exists a positive constant, $\underline{c},$ such that, for all $%
N_{1},N_{2}, $ and $T$ sufficiently large: 
\begin{eqnarray*}
&&\min_{1\leq \ell \leq d}\min_{{\large i\in }H^{{\large c}}}\left\vert 
\frac{1}{q}\dsum\limits_{r=1}^{q}\frac{1}{\tau _{1}}\dsum\limits_{t=\left(
r-1\right) \tau +p}^{\left( r-1\right) \tau +\tau _{1}+p-1}\gamma
_{i}^{\prime }\left\{ E\left[ \underline{F}_{t}\right] \mu _{Y,\ell }+E\left[
\underline{F}_{t}\underline{Y}_{t}^{\prime }\right] \alpha _{YY,\ell }+E%
\left[ \underline{F}_{t}\underline{F}_{t}^{\prime }\right] \alpha _{YF,\ell
}\right\} \right\vert \\
&\geq &\underline{c}>0\text{.}
\end{eqnarray*}%
This is a fairly mild condition which allows us to differentiate the
alternative hypothesis, $i\in H^{c},$ from the null hypothesis, $i\in H,$
since if $i\in H$, then it is clear that:%
\begin{equation*}
\frac{\mu _{i,\ell ,T}}{q\tau _{1}}=\frac{1}{q}\dsum\limits_{r=1}^{q}\frac{1%
}{\tau _{1}}\dsum\limits_{t=\left( r-1\right) \tau +p}^{\left( r-1\right)
\tau +\tau _{1}+p-1}\gamma _{i}^{\prime }\left\{ E\left[ \underline{F}_{t}%
\right] \mu _{Y,\ell }+E\left[ \underline{F}_{t}\underline{Y}_{t}^{\prime }%
\right] \alpha _{YY,\ell }+E\left[ \underline{F}_{t}\underline{F}%
_{t}^{\prime }\right] \alpha _{YF,\ell }\right\} =0,
\end{equation*}%
given that $\gamma _{i}=0$. Note that this assumption does rule out certain
specialized situations, such as the case when $\mu _{Y,\ell }=0$, $\alpha
_{YY,\ell }=0$, and $\alpha _{YF,\ell }=0,$ for some $\ell \in \left\{
1,...,d\right\} $. However, we do not consider such cases to be of much
practical interest since, for example, if $\mu _{Y,\ell }=0$, $\alpha
_{YY,\ell }=0$, and $\alpha _{YF,\ell }=0$ for some $\ell $ then expression (%
\ref{Y component FAVAR}) above implies that the $\ell ^{th}$ component of $%
Y_{t{\LARGE +}1}$ will have the representation 
\begin{eqnarray*}
y_{\ell ,t{\LARGE +}1} &=&\mu _{Y,\ell }+\underline{Y}_{t}^{\prime }\alpha
_{YY,\ell }+\underline{F}_{t}^{\prime }\alpha _{YF,\ell }+\varepsilon _{\ell
,t{\LARGE +}1}^{Y} \\
&=&\varepsilon _{\ell ,t{\LARGE +}1}^{Y},
\end{eqnarray*}%
so that, in this case, $y_{\ell ,t{\LARGE +}1}$ depends neither on $%
\underline{Y}_{t}=\left( Y_{t}^{\prime },Y_{t-1}^{\prime },...,Y_{t-p{\LARGE %
+}1}^{\prime }\right) ^{\prime }$ nor on $\underline{F}_{t}=\left(
F_{t}^{\prime },F_{t-1}^{\prime },...,F_{t-p{\LARGE +}1}^{\prime }\right) $.
This is, of course, an unrealistic model for $y_{\ell ,t{\LARGE +}1}$ since
it would not even be dependent.

\noindent \textbf{(b) }Bai and Ng (2008)\textbf{\ }address the important
issue that one should choose the predictor variables $Z_{it}$ based on their
predictability for $Y_{t+1}$. While we agree with their viewpoint overall,
it is worth stressing that for the FAVAR model considered here, whether $%
Z_{it}$ helps to predict some future values of $Y_{t}$ (say, $Y_{t+h}$)
depends on two things: (i) whether $Z_{it}$ loads significantly on the
underlying factors $\underline{F}_{t}$ (i.e., whether $\gamma _{i}\neq 0$ or
not) and (ii) whether at least some components of $\underline{F}_{t}$ are
helpful for predicting certain components of $Y_{t+h}$. The variable
selection procedure which we propose here focuses on the first issue but not
the second. This is because, in our view, it is important to first obtain
good factor estimates with certain desirable asymptotic properties before
trying to assess which factor may or may not be useful for predicting $Y_{t%
{\LARGE +}h}$. Note that, for a given $t$, the precision with which $%
\underline{F}_{t}$ is estimated depends primarily on the size of the
cross-sectional dimension, and the exclusion of any relevant $Z_{it}$ (with $%
\gamma _{i}\neq 0$) will have the negative effect of reducing the sample
size used for this estimation. More importantly, as we will discuss in
greater details in Remark 4.2 below, if we try to do too much at the
variable selection stage and end up excluding a significant number of
(predictor) variables that load strongly on at least some of the factors,
then, this can lead to the factor vector $\underline{F}_{t}$ being
inconsistently estimated. While the question of predictability is certainly
an important one, the answer we get for this question can, in some
situations, be at odds with the objective of achieving consistent factor
estimation. This is because while $\gamma _{i}^{\prime }=0$ does imply that $%
Z_{i\cdot }$ will not be helpful for predicting future values of $Y$, the
reverse is not necessarily true. On the other hand, to ensure consistent
estimation of the factors, we want to use every data point $Z_{it}$ for
which the null hypothesis $\gamma _{i}^{\prime }=0$ is rejected. Moreover,
if it is true that some of the factors load primarily on variables which are
uninformative predictors for certain components of $Y_{t+h}$, then that will
show up in the form of certain parameter restrictions on the forecasting
equation, in which case the best way to address this problem is to perform
hypothesis testing or model selection on the forecasting equation itself,
after the unobserved factors have first been properly estimated.

The following two theorems give our main theoretical results on the variable
selection procedure described above.

\noindent \textbf{Theorem 2: }\textit{Let\ }$H=\left\{ k\in \left\{
1,....,N\right\} :\gamma _{k}=0\right\} $\textit{. Suppose that Assumptions
3-1, 3-2(a)-(c), 3-3(a)-(c) 3-4, 3-5, 3-7, 3-8, 3-10 (a) and 3-11 hold. Let }%
$\Phi ^{-1}\left( \cdot \right) $\textit{\ denote the inverse of the
cumulative distribution function of the standard normal random variable, or,
alternatively, the quantile function of the standard normal distribution.
Then the following statements are true:}

\begin{enumerate}
\item[(a)] \textit{Let }$\left\{ \varpi _{\ell }:\ell =1,..,d\right\} $%
\textit{\ be pre-specified weights, such that }$\varpi _{\ell }\geq 0,$%
\textit{\ for every }$\ell \in \left\{ 1,...,d\right\} $\textit{\ and }$%
\dsum\nolimits_{\ell =1}^{d}\varpi _{\ell }=1$\textit{, then:}%
\begin{equation*}
P\left( \max_{{\large i\in }H}\dsum\limits_{\ell =1}^{d}\varpi _{\ell
}\left\vert S_{i,\ell ,T}\right\vert \geq \Phi ^{-1}\left( 1-\frac{\varphi }{%
2N}\right) \right) =O\left( \frac{N_{2}\varphi }{N}\right) =o\left( 1\right) 
\text{,}
\end{equation*}%
\textit{where }$N=N_{1}+N_{2}$\textit{.}

\item[(b)] 
\begin{equation*}
P\left( \max_{{\large i\in }H}\max_{1\leq \ell \leq d}\left\vert S_{i,\ell
,T}\right\vert \geq \Phi ^{-1}\left( 1-\frac{\varphi }{2N}\right) \right)
=O\left( \frac{N_{2}\varphi }{N}\right) =o\left( 1\right) \text{. }
\end{equation*}
\end{enumerate}

\noindent \textbf{Theorem 3: }\textit{Let }$H^{c}=\left\{ k\in \left\{
1,....,N\right\} :\gamma _{k}\neq 0\right\} $\textit{. Suppose that
Assumptions 3-1, 3-2(a)-(c), 3-3(a)-(c), 3-5, 3-7, 3-9, 3-10, and 3-11 hold.
Then the following statements are true.}

\begin{enumerate}
\item[(a)] \textit{Let }$\left\{ \varpi _{\ell }:\ell =1,..,d\right\} $%
\textit{\ be pre-specified weights, such that }$\varpi _{\ell }\geq 0,$%
\textit{\ for every }$\ell \in \left\{ 1,...,d\right\} $\textit{\ and }$%
\dsum\nolimits_{\ell =1}^{d}\varpi _{\ell }=1$\textit{, then: }%
\begin{equation*}
P\left( \min_{{\large i\in }H^{{\large c}}}\dsum\limits_{\ell =1}^{d}\varpi
_{\ell }\left\vert S_{i,\ell ,T}\right\vert \geq \Phi ^{-1}\left( 1-\frac{%
\varphi }{2N}\right) \right) \rightarrow 1\text{.}
\end{equation*}

\item[(b)] 
\begin{equation*}
P\left( \min_{{\large i\in }H^{{\large c}}}\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert \geq \Phi ^{-1}\left( 1-\frac{\varphi 
}{2N}\right) \right) \rightarrow 1\text{.}
\end{equation*}
\end{enumerate}

\noindent \textbf{Remark 3.4:}\noindent \noindent

\noindent \textbf{(a)} Theorem 2 shows that the probability of a false
positive, i.e., the probability that $i\in \widehat{H}^{c},$ even though $%
\gamma _{i}=0$, approaches zero, as $N,T\rightarrow \infty $. Theorem 3
shows that the probability of a false negative, i.e., the probability that $%
i\in \widehat{H}$ even though $\gamma _{i}\neq 0$, also approaches zero, as $%
N,T\rightarrow \infty $. Together, these two theorems show that our variable
selection procedure is (completely) consistent in the sense that the
probability of committing a misclassification error vanishes as $%
N,T\rightarrow \infty $.

\noindent \textbf{(b) }Note that as a by-product, our variable selection
procedure provides us with an estimate $\widehat{N}_{1}$ of the unobserved
quantity $N_{1}$, where the latter, in light of Assumption 3-6, can be
interpreted as giving the order of magnitude of $\Gamma ^{\prime }\Gamma $
and is, thus, a measure of the overall pervasiveness of the factors in a
given application. As we show in part (a) of Lemma D-15 in Appendix D, $%
\widehat{N}_{1}$ is a consistent estimator of $N_{1}$, in the sense that $%
\widehat{N}_{1}/N_{1}\overset{p}{\rightarrow }1$. Moreover, note that the
rate condition given in Bai and Ng (2021) for consistent factor estimation
(i.e., Assumption A4 in their paper) can be restated in our setup as the
assumption that $N/\left( TN_{1}\right) \rightarrow 0$. Now, because $N_{1}$
is not observed, this rate condition, by itself, only provides a rough guide
to empirical researchers wishing to assess whether factors can be estimated
accurately in the particular problem of interest to them. Viewed from this
perspective, what we propose here actually builds on the work of Bai and Ng
(2021), as our procedure helps to highlight the importance of the rate
condition they have introduced and provides additional information that is
useful to empirical researchers about the degree of pervasiveness of the
underlying factors.

\noindent \textbf{(c) }Note, in addition, that knowledge of the number of
factors is not needed to implement our variable selection procedure. Hence,
in the case where the number of factors needs to be determined empirically,
an applied researcher could first use our procedure to properly select the
relevant variables and then apply an information criterion such as that
proposed in Bai and Ng (2002) to estimate the number of factors. We plan to
write a sequel to the current paper to show that doing so will lead to
consistent estimation of the number of factors.

\section{\noindent Consistent Estimation of the h-Step Ahead Predictor Based
on the FAVAR Model}

In this section, we provide our main theoretical results on factor
estimation and on the estimation of the $h$-step predictor implied by the
FAVAR model. To obtain these results, we need to impose a further rate
condition on the tuning parameter, $\varphi $ (see part (c) of Assumption
3-11*).

\medskip

\noindent \textbf{Assumption 3-11*: }Let $\varphi $ satisfy the following
three conditions: (a) $\varphi \rightarrow 0$ as $N_{1},N_{2}\rightarrow
\infty $, (b)\ there exists some constant $a>0,$ such that $\varphi \geq 
\frac{1}{N^{a}}$ for all $N_{1},N_{2}$ sufficiently large, and (c) 
\begin{equation*}
\max \left\{ \frac{N^{\frac{{\large 2}}{{\large 7}}}\varphi ^{\frac{{\large 5%
}}{{\large 7}}}}{N_{1}},\frac{N^{\frac{{\large 1}}{{\large 3}}}\varphi }{%
N_{1}T}\right\} \rightarrow 0\text{ as }N_{1},N_{2},T\rightarrow \infty .
\end{equation*}

\noindent \textbf{Remark 4.1: }Note that\textbf{\ }the rate condition given
in part (c) of Assumption 3-11* depends on $N_{1}$. However, if we choose $%
\varphi $ so that:%
\begin{equation*}
\varphi N^{\frac{{\large 2}}{{\large 5}}}=O\left( 1\right) \text{,}
\end{equation*}%
then 
\begin{equation*}
\frac{N^{\frac{{\large 2}}{{\large 7}}}\varphi ^{\frac{{\large 5}}{{\large 7}%
}}}{N_{1}}=O\left( \frac{1}{N_{1}}\right) =o\left( 1\right) \text{ and }%
\frac{N^{\frac{{\large 1}}{{\large 3}}}\varphi }{N_{1}T}=O\left( \frac{1}{%
N_{1}N^{\frac{{\large 1}}{{\large 15}}}T}\right) =o\left( \frac{1}{N_{1}}%
\right) \text{.}
\end{equation*}%
Hence, with this choice of $\varphi $, Assumption 3-11* part (c) will be
satisfied as long as $N_{1}\rightarrow \infty $, and there is no need to
impose any further condition on the rate at which $N_{1}$ grows. Requiring
that $N_{1}\rightarrow \infty $ is a minimal condition, since if $%
N_{1}\nrightarrow \infty $; then consistent factor estimation, even up to an
invertible matrix transformation, is impossible. Moreover, Monte Carlo
results reported in Section 5 of this paper show that our variable selection
procedure performs very well in finite samples, under the tuning parameter
choice $\varphi =N^{-\frac{{\large 2}}{{\large 5}}}$, both in terms of
controlling the probability of a false positive (or Type I) error and in
terms of controlling the probability of a false negative (or Type II) error.

Next, consider the post-variable-selection principal component estimator

\noindent of $\underline{F}_{t}=\left( F_{t}^{\prime },F_{t-1}^{\prime
},...,F_{t-p{\LARGE +}1}^{\prime }\right) :$ 
\begin{equation}
\widehat{\underline{F}}_{t}=\frac{\widehat{\Gamma }^{\prime }Z_{t,N}\left( 
\widehat{H^{c}}\right) }{\widehat{N}_{1}}\text{,}  \label{factor estimator}
\end{equation}%
where%
\begin{equation*}
Z_{t,N}\left( \widehat{H^{c}}\right) =\left[ 
\begin{array}{cccc}
Z_{1,t}\mathbb{I}\left\{ 1\in \widehat{H^{c}}\right\} & Z_{2,t}\mathbb{I}%
\left\{ 2\in \widehat{H^{c}}\right\} & \cdots & Z_{N,t}\mathbb{I}\left\{
N\in \widehat{H^{c}}\right\}%
\end{array}%
\right] ^{\prime },
\end{equation*}%
with%
\begin{equation*}
\mathbb{I}\left\{ i\in \widehat{H^{c}}\right\} =\left\{ 
\begin{array}{cc}
1 & \text{if }i\in \widehat{H^{c}}\text{, i.e., if }\mathbb{S}%
_{i,T}^{+}>\Phi ^{-1}\left( 1-\frac{\varphi }{2N}\right) \\ 
0 & \text{if }i\in \widehat{H}\text{, i.e., if }\mathbb{S}_{i,T}^{+}\leq
\Phi ^{-1}\left( 1-\frac{\varphi }{2N}\right)%
\end{array}%
\right. ,
\end{equation*}%
and where $\widehat{N}_{1}=\#\left( \widehat{H^{c}}\right) $, i.e., the
cardinality of the set $\widehat{H^{c}}$. Here, $\widehat{\Gamma }$ denotes
the principal component estimator of the loading matrix $\Gamma $
constructed from taking $\sqrt{\widehat{N}_{1}}$ times the matrix whose
columns are the eigenvectors of the post-variable-selection sample
covariance matrix $\widehat{\Sigma }\left( \widehat{H^{c}}\right) $
associated with the $Kp$ largest eigenvalues of this matrix, where, in this
case,%
\begin{equation*}
\widehat{\Sigma }\left( \widehat{H^{c}}\right) =\frac{Z\left( \widehat{H^{c}}%
\right) ^{\prime }Z\left( \widehat{H^{c}}\right) }{\widehat{N}_{1}T_{0}}=%
\frac{1}{\widehat{N}_{1}T_{0}}\dsum\limits_{t=p}^{T}Z_{t,N}\left( \widehat{%
H^{c}}\right) Z_{t,N}\left( \widehat{H^{c}}\right) ^{\prime },\text{ }
\end{equation*}%
with $T_{0}=T-p+1$.

Our next result shows that the estimator given in expression (\ref{factor
estimator}) consistently estimates the unobserved factors $\underline{F}%
_{t}, $up to an invertible $Kp\times Kp$ matrix transformation.

\noindent \textbf{Theorem 4: }\textit{Suppose that Assumptions 3-1, 3-2,
3-3, 3-4, 3-5, 3-6, 3-7, 3-8, 3-9, and 3-10 hold. Let }$\widehat{\underline{F%
}}_{t}$\textit{\ be as defined in expression (\ref{factor estimator}).
Assume further that the specification of the tuning parameter, }$\varphi ,$%
\textit{\ in the decision rule (\ref{var selection decision rule}) satisfies
Assumption 3-11*. Then,}%
\begin{equation*}
\left\Vert \widehat{\underline{F}}_{t}-Q^{\prime }\underline{F}%
_{t}\right\Vert _{2}=o_{p}\left( 1\right) ,\text{ for all fixed }t\text{,}
\end{equation*}%
\textit{where}%
\begin{equation*}
Q=\left( \frac{\Gamma ^{\prime }\Gamma }{N_{1}}\right) ^{\frac{{\Large 1}}{%
{\Large 2}}}\Xi \widehat{V}\text{,}
\end{equation*}%
\textit{and where }$\widehat{V}$\textit{\ is the }$Kp\times Kp$\textit{\
orthogonal matrix given in Lemma D-14, and }$\Xi $\textit{\ is a }$Kp\times
Kp$\textit{\ orthogonal matrix whose columns are the eigenvectors of the
matrix }%
\begin{equation*}
M_{FF}^{\ast }=\left( \frac{\Gamma ^{\prime }\Gamma }{N_{1}}\right)
^{1/2}M_{FF}\left( \frac{\Gamma ^{\prime }\Gamma }{N_{1}}\right)
^{1/2}=\left( \frac{\Gamma ^{\prime }\Gamma }{N_{1}}\right) ^{1/2}\frac{1}{%
T_{0}}\dsum\limits_{t=p}^{T}E\left[ \underline{F}_{t}\underline{F}%
_{t}^{\prime }\right] \left( \frac{\Gamma ^{\prime }\Gamma }{N_{1}}\right)
^{1/2}\text{.}
\end{equation*}

\noindent \textbf{Remark 4.2:}

\noindent \qquad If we examine the proof of Theorem 4 in Appendix A and the
supporting arguments given in the proof of Lemma D-15 of Appendix D, we see
that two of the key components of the proof involve showing that:%
\begin{equation*}
\left\Vert \frac{\Gamma \left( \widehat{H^{c}}\right) -\Gamma }{\sqrt{N_{1}}}%
\right\Vert _{2}\overset{p}{\rightarrow }0
\end{equation*}%
and that 
\begin{equation*}
\frac{\widehat{N}_{1}-N_{1}}{N_{1}}\overset{p}{\rightarrow }0\text{.}
\end{equation*}%
This is one of the reasons why in Remark 3.3(b) above, we argue that initial
variable selection should focus on determining which variables load strongly
on the factors without worrying specifically at that stage about the related
issues of predictability or, for that matter, any other issue. By contrast,
if we make our initial variable selection based on some more stringent
criterion that takes into consideration not only variable relevance but also
other concerns such as predictability, then, we may end up with a much
smaller set $\widetilde{H}^{c}$ of selected variables relative to the set $%
\widehat{H^{c}}$ selected under our procedure. In particular, in this case,
it may be possible that even in large samples a significant number of rows
of $\Gamma \left( \widetilde{H^{c}}\right) $ may contain only zero elements
even though the corresponding row of $\Gamma $ is not a zero vector, so that
the result:%
\begin{equation*}
\left\Vert \frac{\Gamma \left( \widetilde{H^{c}}\right) -\Gamma }{\sqrt{N_{1}%
}}\right\Vert _{2}\overset{p}{\rightarrow }0
\end{equation*}%
may not hold. For the same reason, if we let $\widetilde{N}_{1}$ denote the
cardinality of the set of selected indices based on an alternative, more
stringent variable selection procedure, then, the result: 
\begin{equation*}
\frac{\widetilde{N}_{1}-N_{1}}{N_{1}}\overset{p}{\rightarrow }0
\end{equation*}%
also may not hold, since, by definition, $N_{1}$ is the number of rows of $%
\Gamma $ which have at least one non-zero element.

Although Theorem 4 shows that, without further identifying assumptions, we
can only estimate the factors $\underline{F}_{t}$ consistently up to an
invertible $Kp\times Kp$ matrix transformation, this result turns out to be
sufficient for us to estimate the $h$-step ahead predictor consistently.
More specifically, in Appendix D, we show that, for $h$-step ahead forecast,
the (infeasible) forecasting equation implied by the FAVAR model (\ref{FAVAR}%
) has the form%
\begin{equation}
Y_{t{\LARGE +}h}=\beta _{0}+B_{1}^{\prime }\underline{Y}_{t}+B_{2}^{\prime }%
\underline{F}_{t}+\eta _{t{\LARGE +}h},
\label{infeasible h step forecast eqn}
\end{equation}%
where $\underline{Y}_{t}$ and $\underline{F}_{t}$ are as defined in
expression (\ref{Yunderscore and Funderscore}) above and where:%
\begin{eqnarray}
\beta _{0} &=&\dsum\limits_{j=0}^{h-1}J_{d}A^{j}\alpha \text{, }%
B_{1}^{\prime }=J_{d}A^{h}\mathcal{P}_{\left( d{\LARGE +}K\right) p}^{\prime
}S_{d}\text{, }B_{2}^{\prime }=J_{d}A^{h}\mathcal{P}_{\left( d{\LARGE +}%
K\right) p}^{\prime }S_{K}\text{ and}  \label{beta0 B1 and B2} \\
\text{ }\eta _{t{\LARGE +}h} &=&\dsum\limits_{j=0}^{h-1}J_{d}A^{j}J_{d%
{\LARGE +}K}^{\prime }\varepsilon _{t{\LARGE +}h-j}\text{.}  \notag
\end{eqnarray}%
Here, $\alpha $ and $A$ are, respectively, the intercept (vector) and the
coefficient matrix of the companion form defined in expression (\ref%
{companion form notations}) above, $\mathcal{P}_{\left( d{\LARGE +}K\right)
p}$ is a permutation matrix such that:%
\begin{equation*}
\mathcal{P}_{\left( d{\LARGE +}K\right) p}\underline{W}_{t}=\left( 
\begin{array}{c}
\underline{Y}_{t} \\ 
\underline{F}_{t}%
\end{array}%
\right) \text{,}
\end{equation*}%
and%
\begin{eqnarray*}
S_{d} &=&\left( 
\begin{array}{c}
I_{dp} \\ 
\underset{Kp\times dp}{0}%
\end{array}%
\right) \text{, }S_{K}=\left( 
\begin{array}{c}
\underset{dp\times Kp}{0} \\ 
I_{Kp}%
\end{array}%
\right) \text{,}\underset{d\times \left( d{\LARGE +}K\right) p}{J_{d}}=\left[
\begin{array}{cccc}
I_{d} & 0 & \cdots & 0%
\end{array}%
\right] \text{, and} \\
\text{ }\underset{\left( d{\LARGE +}K\right) \times \left( d{\LARGE +}%
K\right) p}{J_{d{\LARGE +}K}} &=&\left[ 
\begin{array}{cccc}
I_{d{\LARGE +}K} & 0 & \cdots & 0%
\end{array}%
\right] \text{.}
\end{eqnarray*}%
See\ the beginning of Appendix D for a derivation of the equation given in
expression (\ref{infeasible h step forecast eqn}). The reason expression (%
\ref{infeasible h step forecast eqn}) is called an infeasible forecasting
equation is, of course, because $\underline{F}_{t}$ is not observed, so to
obtain a feasible version of this forecasting equation, we must replace $%
\underline{F}_{t}$ in equation (\ref{infeasible h step forecast eqn}) with
the estimate $\underline{\widehat{F}}_{t}$ given in expression (\ref{factor
estimator}). Doing so, we arrive at a feasible $h$-step ahead forecasting
equation of the form: 
\begin{eqnarray}
Y_{t{\LARGE +}h} &=&\beta _{0}+\dsum\limits_{g=1}^{p}B_{1,g}^{\prime }Y_{t-g%
{\LARGE +}1}+\dsum\limits_{g=1}^{p}B_{2,g}^{\prime }\widehat{F}_{t-g{\LARGE +%
}1}+\widehat{\eta }_{t{\LARGE +}h}  \notag \\
&=&\beta _{0}+B_{1}^{\prime }\underline{Y}_{t}+B_{2}^{\prime }\underline{%
\widehat{F}}_{t}+\widehat{\eta }_{t{\LARGE +}h}\text{, }
\label{feasible h step forecast eqn}
\end{eqnarray}%
where $\widehat{\eta }_{t{\LARGE +}h}=\eta _{t{\LARGE +}h}-B_{2}^{\prime
}\left( \underline{\widehat{F}}_{t}-\underline{F}_{t}\right) ,$ with $\eta
_{t{\LARGE +}h}=\dsum\nolimits_{j=0}^{h-1}J_{d}A^{j}J_{d{\LARGE +}K}^{\prime
}\varepsilon _{t{\LARGE +}h-j}$.

One can interpret expression (\ref{feasible h step forecast eqn}) as a
\textquotedblleft reduced form\textquotedblright\ formulation of the
forecasting equation where the reduced form parameters $\beta _{0}$, $B_{1}$%
, and $B_{2}$ are nonlinear functions of the parameters $\left( \mu
,A_{1},....,A_{p}\right) $ of the FAVAR model, in the case where $h>1$. For
forecasting purposes, while it is possible to estimate the conditional mean
of the forecasting equation (\ref{feasible h step forecast eqn}) by
estimating the underlying parameters directly by nonlinear least squares,
here we choose instead to estimate the conditional mean by estimating the
reduced form parameters $\beta _{0}$, $B_{1}$, and $B_{2}$ via linear least
squares. An important reason why we choose this latter approach is due to
complications that arise both because we are forecasting with a FAVAR which
contains unobserved factors that must first be estimated and because we do
not make enough identifying assumptions so that the factors can only be
estimated consistently up to an invertible $Kp\times Kp$ matrix
transformation. In fact, it turns out that estimating the underlying
parameters $\mu ,A_{1},....,A_{p}$ by nonlinear least squares and
constructing an estimator of the conditional mean of the forecasting
equation based on these estimates will not lead to a consistently estimated $%
h$-step predictor, unless further identifying assumptions are made. On the
other hand, as we will show in Theorem 5 below, estimating the reduced form
parameters $\beta _{0}$, $B_{1}$, and $B_{2}$ by linear least squares does
allow us to construct a consistent estimator of the conditional mean, even
in the absence of additional identifying assumptions.

More precisely, let $\underline{\widehat{F}}_{t}$ denotes the factor
estimates given in expression (\ref{factor estimator}). Our procedure
minimizes the least squares criterion function:%
\begin{eqnarray}
Q\left( \beta _{0},B_{1},B_{2}\right) &=&\dsum\limits_{t=p}^{T-h}\left\Vert
Y_{t{\LARGE +}h}-\beta _{0}-B_{1}^{\prime }\underline{Y}_{t}-B_{2}^{\prime }%
\underline{\widehat{F}}_{t}\right\Vert _{2}^{2}  \notag \\
&=&\dsum\limits_{t=p}^{T-h}\left\Vert Y_{t{\LARGE +}h}-\beta
_{0}-\dsum\limits_{g=1}^{p}B_{1,g}^{\prime }Y_{t-g{\LARGE +}%
1}-\dsum\limits_{g=1}^{p}B_{2,g}^{\prime }\widehat{F}_{t-g{\LARGE +}%
1}\right\Vert _{2}^{2}  \label{least squares criterion}
\end{eqnarray}%
with respect to the parameters $\beta _{0}$, $B_{1}$, and $B_{2},$ and
delivers the OLS estimates $\widehat{\beta }_{0}$, $\widehat{B}_{1}$, and $%
\widehat{B}_{2}$. We then forecast $Y_{T{\LARGE +}h}$ using the $h$-step
predictor:%
\begin{equation}
\widehat{Y}_{T{\LARGE +}h}=\widehat{\beta }_{0}+\widehat{B}_{1}^{\prime }%
\underline{Y}_{T}+\widehat{B}_{2}^{\prime }\underline{\widehat{F}}_{T}\text{%
. }  \label{h step ahead predictor}
\end{equation}%
The following result shows that $\widehat{Y}_{T{\LARGE +}h}$ is a consistent
estimator of the conditional mean of the infeasible forecast equation (\ref%
{infeasible h step forecast eqn}).

\medskip

\noindent \textbf{Theorem 5: }\textit{Let }$\widehat{Y}_{T{\LARGE +}h}$%
\textit{\ be as defined in expression (\ref{h step ahead predictor}).
Suppose that Assumptions 3-1, 3-2, 3-3, 3-4, 3-5, 3-6, 3-7, 3-8, 3-9, 3-10,
and 3-11* hold. Then,}

\begin{equation*}
\widehat{Y}_{T{\LARGE +}h}-\left( \beta _{0}+B_{1}^{\prime }\underline{Y}%
_{T}+B_{2}^{\prime }\underline{F}_{T}\right) \overset{p}{\rightarrow }0\text{
as }N_{1},N_{2},T\rightarrow \infty \text{.}
\end{equation*}

\medskip

\section{\noindent Monte Carlo Study}

In this section, we report some simulation results on the finite sample
performance of our variable selection procedure. The model used in the Monte
Carlo study is the following tri-variate FAVAR(1) process:

\begin{eqnarray}
W_{t} &=&\mu +AW_{t-1}+\varepsilon _{t},  \label{W eqn} \\
Z_{t} &=&\gamma F_{t}+u_{t}\text{,}  \label{Z eqn}
\end{eqnarray}%
where 
\begin{equation*}
W_{t}=\left( 
\begin{array}{c}
Y_{1t} \\ 
Y_{2t} \\ 
F_{t}%
\end{array}%
\right) \text{, }\mu =\left( 
\begin{array}{c}
2 \\ 
1 \\ 
2%
\end{array}%
\right) \text{, }A=\left( 
\begin{array}{ccc}
0.9 & 0.3 & 0.5 \\ 
0 & 0.7 & 0.1 \\ 
0 & 0.6 & 0.7%
\end{array}%
\right) \text{, and }\gamma =\left( 
\begin{array}{c}
\iota _{N_{1}} \\ 
\underset{N_{2}\times 1}{0}%
\end{array}%
\right) ,
\end{equation*}%
with $\iota _{N_{1}}$ denoting an $N_{1}\times 1$ vector of ones. We
consider different configurations of $N$, $N_{1}$, and $T,$ as given in the
tables below. For the error process in equation (\ref{W eqn}), we take $%
\left\{ \varepsilon _{t}\right\} \equiv i.i.d.N\left( 0,\Sigma _{\varepsilon
}\right) $, where: 
\begin{equation*}
\Sigma _{\varepsilon }=\left( 
\begin{array}{ccc}
1.3 & 0.99 & 0.641 \\ 
0.99 & 0.81 & 0.009 \\ 
0.641 & 0.009 & 5.85%
\end{array}%
\right) \text{.}
\end{equation*}%
The error process, $\left\{ u_{it}\right\} ,$ in equation (\ref{Z eqn}) is
allowed to exhibit both temporal and cross-sectional dependence and also
conditional heteroskedasticity. More specifically, we let:%
\begin{equation*}
u_{it}=0.8u_{it-1}+\zeta _{it}\text{,}
\end{equation*}%
and, following the approach for modeling cross-sectional dependence given in
the Monte Carlo design of Stock and Watson (2002a), we specify: 
\begin{equation*}
\zeta _{it}=\left( 1+b^{2}\right) \eta _{it}+b\eta _{i+1,t}+b\eta _{i-1,t}%
\text{,}
\end{equation*}%
and in the experiments given below, we set $b=1$. In addition, $\eta
_{it}=\omega _{it}\xi _{it},$ with $\left\{ \xi _{it}\right\} \equiv
i.i.d.N\left( 0,1\right) $ independent of $\left\{ \varepsilon _{t}\right\} $%
, and $\omega _{it}$ follows a GARCH(1,1) process given by%
\begin{equation*}
\omega _{it}^{2}=1+0.9\omega _{it-1}^{2}+0.05\eta _{it-1}^{2}\text{.}
\end{equation*}

To study the effects of varying the tuning parameter, we let $\varphi
=N^{-\vartheta }$, and consider six different values of $\vartheta $, i.e., $%
\vartheta =0.2$, $0.3$, $0.4$, $0.5$, $0.6$, and $0.7$. We also attempt to
shed light on the effects of forming blocks of different sizes on the
performance of our procedure. To do this, for $T=100$, we set $\tau _{1}=2$, 
$3$, $4$, and $5$; for $T=200$, we set $\tau _{1}=5$, $6$, $8$, and $10$;
and for $T=600$, we set $\tau _{1}=6$, $8$, $10$, and $12$. In addition, we
present results for both statistics, i.e. $\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $ and $\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $. Note that $d=2$
in our setup; and, for the statistic $\dsum\nolimits_{\ell =1}^{2}\varpi
_{\ell }\left\vert S_{i,\ell ,T}\right\vert $, we set $\varpi _{1}=\varpi
_{2}=1/2$.

The results of our Monte Carlo study are reported in Tables 1-8 below. In
these tables, we let FPR denote the \textquotedblleft False Positive
Rate\textquotedblright\ or the \textquotedblleft Type I\textquotedblright\
error rate, i.e., the proportion of cases where an irrelevant variable $%
Z_{it}$, with associated coefficient $\gamma _{i}=0$, is erroneously
selected as a relevant variable. We let FNR denote the \textquotedblleft
False Negative Rate\textquotedblright\ or the \textquotedblleft Type
II\textquotedblright\ error rate, i.e., the proportion of cases where a
relevant variable is erroneously identified as being irrelevant.

\noindent

\noindent \textbf{Table 1: }$\mathbb{S}_{i,T}^{+}=\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=100$ & $N_{1}=50$ & $T=100$ & $\tau =5$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
$\tau _{1}=2$ & FPR & 0.01690 & 0.00960 & 0.00464 & 0.00218 & 0.00096 & 
0.00034 \\ \hline
& FNR & 0.00218 & 0.00548 & 0.01328 & 0.03204 & 0.07274 & 0.15890 \\ \hline
$\tau _{1}=3$ & FPR & 0.02078 & 0.01156 & 0.00632 & 0.00288 & 0.00128 & 
0.00048 \\ \hline
& FNR & 0.00126 & 0.00350 & 0.00866 & 0.02234 & 0.05374 & 0.12050 \\ \hline
$\tau _{1}=4$ & FPR & 0.02544 & 0.01468 & 0.00826 & 0.00408 & 0.00194 & 
0.00070 \\ \hline
& FNR & 0.00090 & 0.00228 & 0.00582 & 0.01582 & 0.04010 & 0.09362 \\ \hline
$\tau _{1}=5$ & FPR & 0.03208 & 0.01980 & 0.01100 & 0.00584 & 0.00288 & 
0.00122 \\ \hline
& FNR & 0.00052 & 0.00164 & 0.00430 & 0.01140 & 0.02988 & 0.07190 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 2: }$\mathbb{S}_{i,T}^{+}=\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=100$ & $N_{1}=50$ & $T=100$ & $\tau =5$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
$\tau _{1}=2$ & FPR & \multicolumn{1}{|c|}{0.01460} & \multicolumn{1}{|c|}{
0.00810} & \multicolumn{1}{|c|}{0.00382} & 0.00174 & 0.00076 & 0.00028 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00284} & \multicolumn{1}{|c|}{0.00700} & 
\multicolumn{1}{|c|}{0.01674} & 0.04058 & 0.09412 & 0.19952 \\ \hline
$\tau _{1}=3$ & FPR & \multicolumn{1}{|c|}{0.01810} & \multicolumn{1}{|c|}{
0.00996} & \multicolumn{1}{|c|}{0.00526} & 0.00226 & 0.00092 & 0.00032 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00172} & \multicolumn{1}{|c|}{0.00450} & 
\multicolumn{1}{|c|}{0.01100} & 0.02860 & 0.06942 & 0.15378 \\ \hline
$\tau _{1}=4$ & FPR & \multicolumn{1}{|c|}{0.02224} & \multicolumn{1}{|c|}{
0.01276} & \multicolumn{1}{|c|}{0.00702} & 0.00338 & 0.00162 & 0.00044 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00118} & \multicolumn{1}{|c|}{0.00310} & 
\multicolumn{1}{|c|}{0.00828} & 0.02082 & 0.05194 & 0.12132 \\ \hline
$\tau _{1}=5$ & FPR & \multicolumn{1}{|c|}{0.02796} & \multicolumn{1}{|c|}{
0.01714} & \multicolumn{1}{|c|}{0.00924} & 0.00502 & 0.00232 & 0.00080 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00084} & \multicolumn{1}{|c|}{0.00222} & 
\multicolumn{1}{|c|}{0.00574} & 0.01508 & 0.03948 & 0.09456 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 3: }$\mathbb{S}_{i,T}^{+}=\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=200$ & $N_{1}=100$ & $T=100$ & $\tau =5$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
$\tau _{1}=2$ & FPR & 0.00578 & 0.00239 & 0.00085 & 0.00020 & 0.00005 & 
0.00000 \\ \hline
& FNR & 0.01074 & 0.02997 & 0.07812 & 0.18957 & 0.39889 & 0.68275 \\ \hline
$\tau _{1}=3$ & FPR & 0.00775 & 0.00324 & 0.00126 & 0.00038 & 0.00006 & 
0.00001 \\ \hline
& FNR & 0.00724 & 0.02088 & 0.05676 & 0.14547 & 0.32908 & 0.60780 \\ \hline
$\tau _{1}=4$ & FPR & 0.00981 & 0.00457 & 0.00170 & 0.00057 & 0.00014 & 
0.00002 \\ \hline
& FNR & 0.00517 & 0.01494 & 0.04224 & 0.11350 & 0.27048 & 0.53471 \\ \hline
$\tau _{1}=5$ & FPR & 0.01334 & 0.00609 & 0.00266 & 0.00094 & 0.00023 & 
0.00004 \\ \hline
& FNR & 0.00362 & 0.01133 & 0.03244 & 0.08901 & 0.22162 & 0.46424 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 4: }$\mathbb{S}_{i,T}^{+}=\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=200$ & $N_{1}=100$ & $T=100$ & $\tau =5$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
$\tau _{1}=2$ & FPR & \multicolumn{1}{|c|}{0.00486} & \multicolumn{1}{|c|}{
0.00196} & \multicolumn{1}{|c|}{0.00064} & 0.00014 & 0.00002 & 0.00000 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.01415} & \multicolumn{1}{|c|}{0.03813} & 
\multicolumn{1}{|c|}{0.09966} & 0.23933 & 0.48356 & 0.77511 \\ \hline
$\tau _{1}=3$ & FPR & \multicolumn{1}{|c|}{0.00657} & \multicolumn{1}{|c|}{
0.00268} & \multicolumn{1}{|c|}{0.00098} & 0.00024 & 0.00005 & 0.00001 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00921} & \multicolumn{1}{|c|}{0.02714} & 
\multicolumn{1}{|c|}{0.07372} & 0.18714 & 0.40894 & 0.70884 \\ \hline
$\tau _{1}=4$ & FPR & \multicolumn{1}{|c|}{0.00841} & \multicolumn{1}{|c|}{
0.00378} & \multicolumn{1}{|c|}{0.00133} & 0.00043 & 0.00004 & 0.00002 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00661} & \multicolumn{1}{|c|}{0.01975} & 
\multicolumn{1}{|c|}{0.05564} & 0.14734 & 0.34279 & 0.63906 \\ \hline
$\tau _{1}=5$ & FPR & \multicolumn{1}{|c|}{0.01124} & \multicolumn{1}{|c|}{
0.00509} & \multicolumn{1}{|c|}{0.00213} & 0.00069 & 0.00017 & 0.00002 \\ 
\hline
& FNR & \multicolumn{1}{|c|}{0.00477} & \multicolumn{1}{|c|}{0.01475} & 
\multicolumn{1}{|c|}{0.04258} & 0.11741 & 0.28620 & 0.56845 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 5: }$\mathbb{S}_{i,T}^{+}=\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=400$ & $N_{1}=200$ & $T=200$ & $\tau =10$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
\multicolumn{1}{|c|}{$\tau _{1}=5$} & FPR & 0.00035 & 0.00009 & 0.00003 & 
0.00001 & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00200 & 0.01116 & 0.05764 & 0.23070 & 
0.61173 & 0.94453 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=6$} & FPR & 0.00040 & 0.00010 & 2.5$\times $%
10$^{-5}$ & 5.0$\times $10$^{-6}$ & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00128 & 0.00740 & 0.04154 & 0.18482 & 
0.54582 & 0.92176 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=8$} & FPR & 0.00054 & 0.00015 & 0.00005 & 
0.00001 & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00054 & 0.00369 & 0.02191 & 0.11627 & 
0.41851 & 0.85806 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=10$} & FPR & 0.00093 & 0.00031 & 0.00008 & 
1.5$\times $10$^{-5}$ & 5.0$\times $10$^{-6}$ & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00026 & 0.00194 & 0.01218 & 0.07226 & 
0.30765 & 0.76833 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 6: }$\mathbb{S}_{i,T}^{+}=\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=400$ & $N_{1}=200$ & $T=200$ & $\tau =10$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
\multicolumn{1}{|c|}{$\tau _{1}=5$} & FPR & 0.00030 & 8.5$\times $10$^{-5}$
& 2.5$\times $10$^{-5}$ & 5.0$\times $10$^{-6}$ & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00231 & 0.01355 & 0.06894 & 0.26683 & 
0.67266 & 0.96749 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=6$} & FPR & 0.00034 & 9.5$\times $10$^{-5}$
& 0.00002 & 5.0$\times $10$^{-6}$ & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00148 & 0.00901 & 0.05058 & 0.21713 & 
0.60968 & 0.95287 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=8$} & FPR & 0.00046 & 0.00013 & 0.00004 & 
0.00001 & 0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00068 & 0.00448 & 0.02712 & 0.14045 & 
0.48133 & 0.90649 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=10$} & FPR & 0.00079 & 0.00026 & 7.5$\times $%
10$^{-5}$ & 0.00001 & 5.0$\times $10$^{-6}$ & 0.00000 \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00034 & 0.00246 & 0.01535 & 0.08934 & 
0.36382 & 0.83510 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 7: }$\mathbb{S}_{i,T}^{+}=\max_{1\leq \ell \leq
d}\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=1000$ & $N_{1}=500$ & $T=600$ & $\tau =12$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
\multicolumn{1}{|c|}{$\tau _{1}=6$} & FPR & 0.00044 & 0.00017 & 7.4$\times $%
10$^{-5}$ & 2.8$\times $10$^{-5}$ & 0.00001 & 2.0$\times $10$^{-6}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=8$} & FPR & 0.00054 & 0.00023 & 9.6$\times $%
10$^{-5}$ & 4.2$\times $10$^{-5}$ & 1.6$\times $10$^{-5}$ & 8.0$\times $10$%
^{-6}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=10$} & FPR & 0.00080 & 0.00038 & 0.00018 & 
0.00007 & 3.6$\times $10$^{-5}$ & 2.0$\times $10$^{-5}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=12$} & FPR & 0.00127 & 0.00068 & 0.00031 & 
0.00015 & 6.8$\times $10$^{-5}$ & 3.0$\times $10$^{-5}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\noindent \textbf{Table 8: }$\mathbb{S}_{i,T}^{+}=\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $

\begin{tabular}{|l|l|c|c|c|c|c|c|}
\hline
&  & $N=1000$ & $N_{1}=500$ & $T=600$ & $\tau =12$ &  &  \\ \hline
&  & \multicolumn{1}{|l|}{$\varphi =N^{-0.2}$} & \multicolumn{1}{|l|}{$%
\varphi =N^{-0.3}$} & \multicolumn{1}{|l|}{$\varphi =N^{-0.4}$} & 
\multicolumn{1}{|l|}{$\varphi =N^{-0.5}$} & $\varphi =N^{-0.6}$ & $\varphi
=N^{-0.7}$ \\ \hline\hline
\multicolumn{1}{|c|}{$\tau _{1}=6$} & FPR & 0.00038 & 0.00015 & 0.00006 & 2.6%
$\times $10$^{-5}$ & 0.00001 & 2.0$\times $10$^{-6}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=8$} & FPR & 0.00049 & 0.00020 & 8.2$\times $%
10$^{-5}$ & 3.4$\times $10$^{-5}$ & 1.4$\times $10$^{-5}$ & 6.0$\times $10$%
^{-6}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=10$} & FPR & 0.00072 & 0.00033 & 0.00016 & 
0.00006 & 3.2$\times $10$^{-5}$ & 1.8$\times $10$^{-5}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\multicolumn{1}{|c|}{$\tau _{1}=12$} & FPR & 0.00115 & 0.00062 & 0.00028 & 
0.00014 & 6.0$\times $10$^{-5}$ & 2.8$\times $10$^{-5}$ \\ \hline
\multicolumn{1}{|c|}{} & FNR & 0.00000 & 0.00000 & 0.00000 & 0.00000 & 
0.00000 & 0.00000 \\ \hline
\end{tabular}

{\small Results based on 1000 simulations}

\medskip

Looking across each row of each table given above, we see that, as we move
from left to right, the FPR's are decreasing, whereas the FNR's are
increasing. This is not surprising since, as we move from $\varphi =N^{-0.2}$
to $\varphi =N^{-0.7}$ for a given $N$, the size of the tuning parameter $%
\varphi $ is becoming smaller which means that our specified threshold $\Phi
^{-1}\left( 1-\frac{\varphi }{2N}\right) $ is becoming larger. Overall, our
simulation results indicate that choosing $\varphi =N^{-\vartheta }$ with $%
\vartheta =0.2$, $0.3$, or $0.4$ leads to very good performance, since with
these choices of $\vartheta $, neither FPR nor FNR exceeds $0.1$ in any of
the cases studied here. In fact, both are smaller than $0.05$ in a vast
majority of the cases. On the other hand, choosing $\vartheta =0.6$ or $0.7$
can lead to high values of FNR, as these values can set our threshold at
such a high level that our procedure ends up having very little power. A
particularly attractive choice of the tuning parameter is to take $\varphi
=N^{-0.4},$ since as previously discussed in Section 4 above, this choice of
the tuning parameter allows the rate condition in part (c) of Assumption
3.11* to be satisfied\textbf{\ }as long as $N_{1}\rightarrow \infty $,
without imposing further conditions on the rate at which $N_{1}$ grows.

Looking down the columns of each table, we see that FPR tends to increase as 
$\tau _{1}$ increases, whereas FNR tends to decrease as $\tau _{1}$
increases. To provide an explanation for this result, note first that the
smaller is $\tau _{1}$ relative to $\tau $, the larger is $\tau _{2}$ (since 
$\tau =\tau _{1}+\tau _{2}$), and thus the larger is the number of
observations that have been removed in constructing our self-normalized
block sums. Intuitively, this can lead to better accommodation of the
effects of dependence and better moderate deviation approximations under the
null hypothesis, thus, resulting in a lower FPR. However, the removal of a
larger number of observations can also lead to a reduction in the power of
our procedure when the alternative hypothesis is correct, so that a negative
consequence of having a smaller $\tau _{1}$ relative to $\tau $ is that FNR
will tend to be higher in this case. The opposite, of course, occurs when we
try to specify a larger $\tau _{1}$ relative to $\tau $.

Our results also show that when the sample sizes are large enough such as
the cases presented in Tables 7 and 8, where $T=600$ and $N=1000$, then both
FPR and FNR are small for all of the cases that we consider. This is in
accord with the results of our theoretical analysis, which shows that our
variable selection procedure is completely consistent in the sense that both
the probability of a false positive and the probability of a false negative
approach zero, as the sample sizes go to infinity.

A final observation based on these Monte Carlo results is that there does
not seem to be a great deal of difference in the performance of the
statistic $\max_{1\leq \ell \leq d}\left\vert S_{i,\ell ,T}\right\vert $ vis-%
\`{a}-vis the statistic $\dsum\nolimits_{\ell =1}^{d}\varpi _{\ell
}\left\vert S_{i,\ell ,T}\right\vert $. Overall, the statistic $\max_{1\leq
\ell \leq d}\left\vert S_{i,\ell ,T}\right\vert $ seems to be a bit better
at controlling FNR, whereas the statistic $\dsum\nolimits_{\ell
=1}^{d}\varpi _{\ell }\left\vert S_{i,\ell ,T}\right\vert $ seems a bit
better at controlling FPR.

\section{\noindent Conclusion}

In this paper, we study the problem of consistently estimating the
conditional mean of a factor-augmented forecasting equation based on the
FAVAR model. When the underlying dynamic factor model generating the latent
factors is high-dimensional, we show that it is important to pre-screen the
variables in terms of their association with the underlying factors prior to
estimation, particularly in cases where one suspects that the conventional
assumption of factor pervasiveness may not hold. For this purpose, we
propose a new variable selection procedure based on a self-normalized score
statistic and provide asymptotic analyses showing that our procedure
correctly identify the set of variables which load significantly on the
underlying factors, with probability approaching one, as the sample sizes go
to infinity. In addition, we show that estimating the factors using only
those variables selected by our method allows the factors to be consistently
estimated, up to an invertible matrix transformation, even if the standard
pervasiveness assumption does not hold, provided that the number of relevant
variables is sufficiently large. Using the factors estimated in such a
manner, we then show that the conditional mean function of a
factor-augmented forecasting equation can be consistently estimated, even
for the case of multi-step ahead forecasts. Finally, we perform a small
Monte Carlo study to examine the finite sample properties of our variable
selection procedure. The results of this study yield insights on the range
of choices for the tuning parameter for which our variable selection
procedure exhibits good finite sample performance.

\bigskip

\begin{thebibliography}{99}
\bibitem{} Anatolyev, S. and A. Mikusheva (2021): \textquotedblleft Factor
Models with Many Assets: Strong Factors, Weak Factors, and the Two-Pass
Procedure,\textquotedblright\ \textit{Journal of Econometrics}, forthcoming.

\bibitem{} Andrews, D.W.K. (1984): \textquotedblleft Non-strong Mixing
Autoregressive Processes,\textquotedblright\ \textit{Journal of Applied
Probability}, 21, 930-934.

\bibitem{} Bai, J. and S. Ng (2002): \textquotedblleft Determining the
Number of Factors in Approximate Factor Models,\textquotedblright\ \textit{%
Econometrica}, 70, 191-221.

\bibitem{} Bai, J. (2003): \textquotedblleft Inferential Theory for Factor
Models of Large Dimensions,\textquotedblright\ \textit{Econometrica}, 71,
135-171.

\bibitem{} Bai, J. and S. Ng (2008): \textquotedblleft Forecasting Economic
Time Series Using Targeted Predictors,\textquotedblright\ \textit{Journal of
Econometrics}, 146, 304-317.

\bibitem{} Bai, J. and S. Ng (2021): \textquotedblleft Approximate Factor
Models with Weaker Loading,\textquotedblright\ Working Paper, Columbia
University.

\bibitem{} Bai, Z. D. and Y. Q. Yin (1993): \textquotedblleft Limit of the
Smallest Eigenvalue of a Large Dimensional Sample Covariance Matrix," 
\textit{Annals of Probability}, 21, 1275-1294.

\bibitem{} Bair, E., T. Hastie, D. Paul, and R. Tibshirani (2006):
\textquotedblleft Prediction by Supervised Principal
Components,\textquotedblright\ \textit{Journal of the American Statistical
Association}, 101, 119-137.

\bibitem{} Belloni, A., D. Chen, V. Chernozhukov, and C. Hansen (2012):
\textquotedblleft Sparse Models and Methods for Optimal Instruments with an
Application to Eminent Domain,\textquotedblright\ \textit{Econometrica}, 80,
2369-2429.

\bibitem{} Billingsley, P. (1995): \textit{Probability and Measure}. New
York: John Wiley \& Sons.

\bibitem{} Borovkova, S., R. Burton, and H. Dehling (2001):
\textquotedblleft Limit Theorems for Functionals of Mixing Processes to
U-Statistics and Dimension Estimation,\textquotedblright\ \textit{%
Transactions of the American Mathematical Society}, 353, 4261-4318.

\bibitem{} Bryzgalova, S. (2016): \textquotedblleft Spurious Factors in
Linear Asset Pricing Models,\textquotedblright\ Working Paper, Stanford
Graduate School of Business.

\bibitem{} Burnside, C. (2016): \textquotedblleft Identification and
Inference in Linear Stochastic Discount Factor Models with Excess
Returns,\textquotedblright\ \textit{Journal of Financial Econometrics}, 14,
295-330.

\bibitem{} Chen, X., Q. Shao, W. B. Wu, and L. Xu (2016): \textquotedblleft
Self-normalized Cram\'{e}r-type Moderate Deviations under
Dependence,\textquotedblright\ \textit{Annals of Statistics}, 44, 1593-1617.

\bibitem{} Davidson. J. (1994): \textit{Stochastic Limit Theory: An
Introduction for Econometricians}. New York: Oxford University Press.

\bibitem{} Davidson, K. R. and S. J. Szarek (2001): \textquotedblleft Local
Operator Theory, Random Matrices and Banach Spaces.\textquotedblright\ In 
\textit{Handbook of the Geometry of Banach Spaces}, 1, 317-366. Amsterdam:
North-Holland.

\bibitem{} Fan, J., Y. Liao, and M. Mincheva (2011): \textquotedblleft
High-dimensional Covariance Matrix Estimation in Approximate Factor
Models,\textquotedblright\ \textit{Annals of Statistics}, 39, 3320-3356.

\bibitem{} Fan, J., Y. Liao, and M. Mincheva (2013): \textquotedblleft Large
Covariance Estimation by Thresholding Principal Orthogonal Complements," 
\textit{Journal of the Royal Statistical Society, Series B}, 75, 603-680.

\bibitem{} Freyaldenhoven, S. (2021a): \textquotedblleft Factor Models with
Local Factors - Determining the Number of Relevant
Factors,\textquotedblright\ \textit{Journal of Econometrics}, forthcoming.

\bibitem{} Freyaldenhoven, S. (2021b): \textquotedblleft Identification
through Sparsity in Factor Models: The $\ell _{1}$-Rotation
Criterion,\textquotedblright\ Working Paper, Federal Reserve Bank of
Philadelphia.

\bibitem{} Giglio, S., D. Xiu, and D. Zhang (2021): \textquotedblleft Test
Assets and Weak Factors,\textquotedblright\ Working Paper, Yale School of
Management and the Booth School of Business, University of Chicago.

\bibitem{} Golub, G. H. and C. F. van Loan (1996): \textit{Matrix
Computations}, 3rd Edition. Baltimore: The Johns Hopkins University Press.

\bibitem{} Goroketskii, V. V. (1977): \textquotedblleft On the Strong Mixing
Property for Linear Sequences,\textquotedblright\ \textit{Theory of
Probability and Applications}, 22, 411-413.

\bibitem{} Gospodinov, N., R. Kan, and C. Robotti (2017): \textquotedblleft
Spurious Inference in Reduced-Rank Asset Pricing Models,\textquotedblright\ 
\textit{Econometrica}, 85, 1613-1628.

\bibitem{} Harding, M. C. (2008): \textquotedblleft Explaining the Single
Factor Bias of Arbitrage Pricing Models in Finite
Samples,\textquotedblright\ \textit{Economics Letters}, 99, 85-88.

\bibitem{} Horn, R. and C. Johnson (1985): \textit{Matrix Analysis}.
Cambridge University Press.

\bibitem{} Jagannathan, R. and Z. Wang (1998): \textquotedblleft An
Asymptotic Theory for Estimating Beta-Pricing Models Using Cross-Sectional
Regression,\textquotedblright\ \textit{Journal of Finance}, 53, 1285-1309.

\bibitem{} Johnstone, I. M. and A. Lu (2009): \textquotedblleft On
Consistency and Sparsity for Principal Components Analysis in High
Dimensions,\textquotedblright\ \textit{Journal of the American Statistical
Association}, 104, 682-697.

\bibitem{} Johnstone, I. M. and D. Paul (2018): \textquotedblleft PCA in
High Dimensions: An Orientation,\textquotedblright\ \textit{Proceedings of
the IEEE}, 106, 1277-1292.

\bibitem{} Kan, R. and C. Zhang (1999): \textquotedblleft Two-Pass Tests of
Asset Pricing Models with Useless Factors,\textquotedblright\ \textit{%
Journal of Finance}, 54, 203-235.

\bibitem{} Kleibergen, F. (2009): \textquotedblleft Tests of Risk Premia in
Linear Factor Models,\textquotedblright\ \textit{Journal of Econometrics},
149, 149-173.

\bibitem{} L\"{u}tkepohl, H. (2005): \textit{New Introduction to Multiple
Time Series Analysis}. New York: Springer.

\bibitem{} Nadler, B. (2008): \textquotedblleft Finite Sample Approximation
Results for Principal Component Analysis: A Matrix Perturbation
Approach,\textquotedblright\ \textit{Annals of Statistics}, 36, 2791-2817.

\bibitem{} Onatski, A. (2012): \textquotedblleft Asymptotics of the
Principal Components Estimator of Large Factor Models with Weakly
Influential Factors,\textquotedblright\ \textit{Journal of Econometrics},
168, 244-258.

\bibitem{} Paul, D. (2007): \textquotedblleft Asymptotics of Sample
Eigenstructure for a Large Dimensional Spiked Covariance
Model,\textquotedblright\ \textit{Statistica Sinica}, 17, 1617-1642.

\bibitem{} Pham, T. D. and L. T. Tran (1985): \textquotedblleft Some Mixing
Properties of Time Series Models,\textquotedblright\ \textit{Stochastic
Processes and Their Applications}, 19, 297-303.

\bibitem{} Ruhe, A. (1975): \textquotedblleft On the Closeness of
Eigenvalues and Singular Values for Almost Normal
Matrices,\textquotedblright\ \textit{Linear Algebra and Its Applications},
11, 87-94.

\bibitem{} Shen, D., H. Shen, H. Zhu, J.S. Marron (2016): \textquotedblleft
The Statistics and Mathematics of High Dimension Low Sample Size
Asymptotics,\textquotedblright\ \textit{Statistica Sinica}, 26, 1747-1770.

\bibitem{} Stewart, G.W. (1973): \textquotedblleft Error and Perturbation
Bounds for Subspaces Associated with Certain Eigenvalue
Problems,\textquotedblright\ \textit{SIAM Review}, 15, 727-764.

\bibitem{} Stewart, G.W. and J. Sun (1990): \textit{Matrix Perturbation
Theory}. Boston: Academic Press.

\bibitem{} Stock, J. H. and M. W. Watson (2002a): \textquotedblleft
Forecasting Using Principal Components from a Large Number of
Predictors,\textquotedblright\ \textit{Journal of the American Statistical
Association}, 97, 1167-1179.

\bibitem{} Stock, J. H. and M. W. Watson (2002b): \textquotedblleft
Macroeconomic Forecasting Using Diffusion Indexes,\textquotedblright\ 
\textit{Journal of Business and Economic Statistics}, 20, 147-162.

\bibitem{} Vershynin, R. (2012): \textquotedblleft Introduction to the
Non-asymptotic Analysis of Random Matrices,\textquotedblright\ In \textit{%
Compressed Sensing}, \textit{Theory and Applications, }210-268. Cambridge
University Press.

\bibitem{} Wang, W. and J. Fan (2017): \textquotedblleft Asymptotics of
Empirical Eigenstructure for High Dimensional Spiked
Covariance,\textquotedblright\ \textit{Annals of Statistics}, 45, 1342-1374.
\end{thebibliography}

\end{document}
