%2multibyte Version: 5.50.0.2960 CodePage: 936
%Enable rescale the size of a table
%Enable customized table
% for tabular's columns aligned on decimal symbol
% for siunitx with bold font


\documentclass[final,notitlepage]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{adjustbox}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{etoolbox}

\usepackage{epstopdf} % Add figures from .eps files
\usepackage{lscape}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{Codepage=936}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wed May 15 17:28:15 2002}
%TCIDATA{LastRevised=Monday, May 15, 2017 14:54:06}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=LaTeX article (bright).cst}
%TCIDATA{PageSetup=65,65,72,72,0}
%TCIDATA{AllPages=
%H=36
%F=29,\PARA{038<p type="texpara" tag="Body Text" > \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  }
%}


\robustify\bfseries
\newcommand{\superscript}[1]{\ensuremath{^{\textrm{#1}}}}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\textwidth=16.0cm
\oddsidemargin=0cm \evensidemargin=0cm
\topmargin=-20pt
\numberwithin{equation}{section}
\baselineskip=100pt
\textheight=21cm
\def\baselinestretch{1.2}

\begin{document}

\title{Big Data Analytics in Economics: What Have We Learned so Far, and
Where Should We Go From Here?*}
\author{Norman R. Swanson and Weiqi Xiong \\
%EndAName
Rutgers University}
\date{September 2017}
\maketitle

\begin{abstract}
Research into predictive accuracy testing remains at the forefront of the forecasting field. 
One reason for this is that
rankings of predictive accuracy across alternative models, which under misspecification are loss function
dependent, are universally utilized to assess the usefulness of econometric models. A second reason, which corresponds to
the objective of this paper, is that researchers are currently focusing
considerable attention on so-called big data, and on new (and old) tools
that are available for the analysis of this data. One of the objectives in this field
is the assessment of whether big-data leads to improvement in forecast accuracy. 
In this survey paper, we discuss some of the latest (and most interesting) methods currently available for analyzing and
utilizing big data when the objective is improved prediction. 
Our discussion includes a summary of various so-called dimension reduction, shrinkage, and machine learning methods, as well as a summary
of recent tools that are useful for ranking prediction models associated with the implementation of these methods. We also
provide a brief empirical illustration of big-data in action, in which we show that big data are indeed useful when predicting the term structure of interest rates.
\end{abstract}

\noindent \emph{JEL Classification: }C12, C22, C53\emph{.}\vspace{0.15cm}

\noindent \emph{Keywords: }Convex loss function, Empirical processes,
Forecast superiority, General loss function.

\medskip \bigskip

\noindent $^{\ast }$ {\footnotesize Corresponding Author: Norman R. Swanson,
Department of Economics, Rutgers University, 75 Hamilton Street, New
Brunswick, NJ 08901, USA, nswanson@econ.rutgers.edu. }

\noindent {\footnotesize Weiqi Xiong, Department of Economics, Rutgers
University, 75 Hamilton Street, New Brunswick, NJ 08901, USA,
wxiong@economics.rutgers.edu. }

\noindent {\footnotesize This paper has been prepared for a State of the Art
Lecture at the 2017 Canadian Economic Association Conference at St. Francis
Xavier University.} {\footnotesize We are grateful to the editor, Francisco Ruge-Murcia and an anonymous referee for useful comments on
earlier drafts of this paper. We also thank Mingmian Cheng,
Valentina Corradi, Frank Diebold, Xu Jiang, Hyun Hak Kim, Dan Levenstein, Yuan Liao, Matt Lightwood, Hal Pedersen, Byung-Dong Seo, Rachel Swanson,
Greg Tkacz, Xiye Yang, and participants of the 2017 Deutsche Bundesbank Workshop on Forecasting for useful
comments and suggestions.}

{\footnotesize \noindent }

\renewcommand{\baselinestretch}{1.4}%
%TCIMACRO{\TeXButton{nomalsize}{\normalsize{}}}%
%BeginExpansion
\normalsize{}%
%EndExpansion

\setcounter {page} {0}\newpage

\section{Introduction}

Methods for analyzing \textquotedblleft big
data\textquotedblright\ have received considerable attention by economists
in recent years. This is not surprising, given that applied practitioners now have an
incredible amount of data available to them, and given that a whole host of new methods have
been developed in various disciplines over the last 20 years or so for
processing these big data. Two key questions that economists continue to pose are, 
correspondingly, what are the forecasting gains associated with using big data,
and which new methods should we use in our
analyses? A third question, which is related, concerns which tools, such as
predictive accuracy tests, to use for model selection with big data.
In the context of forecasting, this third question is relevant because many
critical advances have recently been made in the field of model selection and testing. In this paper, we
address all three questions. First, we discuss select state of the art methods for
big data analysis. These include dimension reduction and shrinkage
approaches that are currently being utilized not only in
economics, but also in a whole host of other fields ranging from aerospace
engineering to neuroscience. Second, we discuss recent advances in
predictive accuracy testing and model selection, from the perspective of
picking the \textquotedblleft best\textquotedblright\ forecasting model.
Finally, we tie our discussions together by considering the usefulness of
big data when forecasting the term structure of interest rates.

In its inception, machine learning was a field of computer science concerned
with designing computers (and computer programs) with the ability to learn,
without the need for further programming. Many types of machine learning
have been developed in recent years. For example, in computer science, key
areas now include deep learning, shrinkage, and recall. Neural networks are
perhaps the most ubiquitous variety of machine learning method that
economists have, up until recently, been interested in. However, the
landscape has changed dramatically in recent years, largely because of the
explosion in big data. One strand of research in
big data analysis uses dimension reduction methods, two main examples of
which are principal components analysis (PCA) and partial least squares. A closely related strand considers
shrinkage (penalized regression) methods, including the likes of ridge regression, the least
absolute shrinkage selection operator (lasso), the elastic net, and the 
non-negative garrote. These and other shrinkage related methods are discussed in Bai and
Ng (2008,2009), Schumacher (2009), Stock and Watson (2012), Kim and Swanson
(2014,2016), and Hirano and Wright (2017), for example. Broadly speaking, the
number of such methods available to empiricists is now immense. 

In the first part of this paper, we discuss a very few of the latest such
techniques, and suggest where we might go from here. For example, we discuss
PCA and sparse PCA, in which the lasso is applied to PCA in order to induce
sparseness in the number of observable variables utilized in the
construction of latent factors or diffusion indexes resulting from
application of PCA. We also discuss a related latent factor dimension
reduction technique called independent component analysis, that takes the
orthogonality condition imposed by PCA one step further by imposing
statistically independence. Finally, we discuss ridge regression, the lasso, and
the elastic net, in the context of penalized regression,
where the number of regressors can be larger than the number of observations
in a dataset.

In the second part of this paper, we discuss out-of-sample predictive accuracy testing, given the
importance of accuracy assessment when comparing the many new
\textquotedblleft big data\textquotedblright\ methods available for
constructing forecasts. There is now a rich
literature on predictive accuracy testing. One of the most
important contributions in the last 25 years is the seminal paper
of Diebold and Mariano (1995, hereafter DM), in which tests of equal
predictive accuracy between two competing models are proposed. Tests that
generalize DM-type tests in order to account for parameter estimation error
include West (1996) and West and McCracken (1998), McCracken (2000), and Corradi and Swanson (2007).
Conditional predictive accuracy tests are developed in Giacomini and White
(2006), in which the \textquotedblleft estimated\textquotedblright\ model is
conditioned on. Tests allowing for integrated and cointegrated variables are
discussed in Clements and Hendry (1999,2001) and Corradi, Swanson\ and
Olivetti (2001). The important issue of the joint comparison of more than
two competing models is addressed in Sullivan, Timmermann and White (1999),
White (2000), Hansen (2005), Romano and Wolf (2005), and Corradi and Distaso
(2011). Papers that consider predictive accuracy testing via the use of
encompassing and related tests include Phillips (1996), Harvey, Leybourne
and Newbold (1997), Chao, Corradi and Swanson (2001), Clark and McCracken
(2001), Corradi and Swanson (2002), and Giacomini and Komunjer (2005).
Broadly speaking, predictive accuracy is assessed by comparing point
measures such as mean square forecast error (MSFE) and mean absolute
forecast error deviation (MAFD) in the above papers. The notion of considering predictive (error) densities
rather than point error loss, model evaluation using predictive
intervals, conditional quantiles, and predictive densities is addressed by
Christoffersen (1998), Giacomini and Komunjer (2005), and Corradi and
Swanson (2005,2006a,b). For comprehensive surveys of this burgeoning
literature, see West (2006), Clark and McCracken (2013), Corradi and Swanson
(2013), and Diebold (2014).\footnote{%
Alternatives to the use of traditional moment-based forecast evaluation
methods include decision based approaches. For example, Granger and Pesaran
(2000) argue in favor of a close link between the decision and the forecast
evaluation problems. Pesaran and Skouras (2002) discuss a decision-based
approach for evaluation and comparison of forecasts. Granger and Machina
(2006) propose a class of realistic decision-based loss functions for
forecast evaluation.}

Recently, a new type of predictive accuracy tests have been devised that
generalize the tests in all of the above papers, in one key dimension. In order to understand how this is done, note
that most of the above papers consider forecast comparison based upon the
examination of moments or conditional moments of the forecast errors, and
researchers must specify the objective function (say, loss function or
likelihood function) used in test formulation. As mentioned above, examples
of relevant loss functions include MSFE and mean absolute forecast error\
MAFD. Unfortunately, the forecast superiority of one model, relative to
other models, is dependent \ on the loss function that is specified. To
circumvent this issue, Granger (1999a) proposes the use of generalized loss
functions, $L$($\cdot $), with the following properties: (1) $L(e)=0,$\ if
the forecast error $e=0;$\ (2) $L(e)\geq 0$ and $Min_{e}L(e)=0;$\ and (3) $%
L(e)$\ is monotonically non-decreasing as $e$\ moves away from zero (this
means that $L(e_{1})\geq L(e_{2})$\ if $e_{1}>e_{2}\geq 0$\ or $%
e_{1}<e_{2}\leq 0)$. Corradi, Jin and Swanson (2017, hereafter CJS) term the
class of loss functions that satisfy the above three properties as general
loss (GL or $\mathcal{L}_{G}$) functions. A second class of loss functions
are defined as convex loss (CL or $\mathcal{L}_{C}$) functions, if in
addition to satisfying the above three properties, they are convex\textbf{. }%
Examples of convex functions include MSFE and MAFD, as well as asymmetric
functions including lin-lin and linex functions (see Elliott and Timmermann
(2004) for further discussion). In CJS, it is supposed that there are $l$
sets of forecasts, with corresponding sequences of one-step-ahead 
forecast errors, $\{e_{1t}\},$ $\{e_{2t}\}...,\{e_{lt}\},$ and the objective is to rank forecast
sequences (or models), regardless of loss function. They
establish links between tests for GL (CL) forecast superiority and tests for
first (second) order stochastic dominance. This allows them to develop a
forecast evaluation procedure that is based on an out-of-sample
generalization of the stochastic dominance tests introduced by Linton,
Maasoumi and Whang (2005, hereafter LMW), which is robust not only to the
choice of loss function, but also to the possible presence of outliers.
In addition to summarizing DM\ and related tests, the CJS test is discussed
in detail below.\footnote{%
The approach of using stochastic dominance to rank distributions of forecast
errors was first introduced in Corradi and Swanson (2013), although they
provide no theory, and their proposed tests are loss function specific. An
alternative somewhat related measure called stochastic error loss is
discussed in Diebold and Shin (2015).}

In our empirical illustration, we show how important big data can be. This
is done in a series of simple prediction experiments where the objective is
to predict the term structure of interest rates, and models used include
benchmark econometric models, dynamic Nelson Siegel (DNS) models, diffusion
index models, and hybrids of the three. The diffusion indexes in our
experiments are estimates of the latent factors from principle component
analysis of a macroeconomic dataset including 103 U.S. variables. Although
the experimental setup that we utilize is limited in its scope, it is
nevertheless interesting that the vast majority of mean square forecast
error \textquotedblleft best\textquotedblright\ models are hybrid DNS\
models that include diffusion indexes. Moreover, these hybrid models
generally outperform standard econometric models, as well as various
forecast combinations.

The rest of the paper is organized as follows. Section 2 summarizes recent
advances in dimension reduction and penalized regression - both of which are
key areas in machine learning. In Section 3, forecast evaluation is
discussed, with emphasis on what the latest methods are, and where we need
to go. An empirical illustration based on predicting the term structure of
interest rates is given in Section 4. Finally, concluding remarks are
gathered in Section 5.

\section{Dimension Reduction and Penalized Regression}

Dimension reduction and variable selection has never been more important in
economics, given recent massive increases in the amount of data available to
forecasters.\footnote{%
See the 2015 issue of the \textit{Journal of Econometrics}
entitled \textbf{High Dimensional Problems in Econometrics}.} 
A key objective, given big data, is to remove redundant and irrelevant
information from datasets. This problem has historically been be tackled via
step-wise regression, for example. However, variables are typically highly correlated in
time series applications. Hence, statistical significance tests used in many regression type
algorithms suffer from severe size distortion issues. Ghysels, Hill, and
Motegi (2017) address this issue by examining multiple parsimonious
regressions, each with one key regressor, while jointly accounting for sequential testing problems.

A second solution to the dimension reduction problem with correlated
regressors is the use of partial least squares (PLS), which was originally
proposed by Herman Wold in the mid 1960s. Broadly speaking, PLS is a latent
variable approach to modeling the covariance structure between two sets of
variables. One set might be a target variable or variables to be predicted
(say $Y$), while the other might be a very large set of correlated predictor
variables, say $X$. More precisely, the model underlying PLS has

\begin{eqnarray*}
Y &=&F_{1}L_{1}+E_{1} \\
X &=&F_{2}L_{2}+E_{2},
\end{eqnarray*}%
where $F_{1}$ and $F_{2}$ are projection matrices of $X$ and $Y$; and $L_{1}$%
and $L_{2}$ are so-called factor loading matrices that operate on the latent
factors $F_{1}$ and $F_{2}.$ Additionally, the error terms, $E_{1}$ and $E_{2}$
are assumed to be identically and independently distributed, and all
matrices are conformably defined, given the dimensions of $X$ and $Y$. In
this setup, the decompositions of $X$ and $Y$ maximize the covariance
between the latent factors $F_{1}$ and $F_{2}.$

A third solution uses principle components analysis (PCA), in which latent
factors (often called diffusion indexes) are again estimated, but this time via use of an
eigenvalue-eigenvector decomposition of the covariance or correlation matrix of the data,
for example. Just as in PLS, the objective is to \textquotedblleft
explain\textquotedblright\ the data\textquotedblright\ using a reduced set
of (latent) explanatory variables, with the idea being that the useful
information in a large set of predictors is often contained in a (much
smaller) set of latent factors, which are themselves simply linear
combinations of the original variables. A key difference between PCA\ and
PLS is that PLS directly attempts to account for correlation between the
target variable and the predictors, while PCA is \textquotedblleft
unsupervised\textquotedblright , in the sense that correlation with any given target variable is not
emphasized in the construction of the latent factors. Rather, overall
explanation of the entire dataset is the focus of PCA. Needless to say, this
particular feature of PCA is of potential concern when targeting
(predicting) a specific variable or variables. For this reason, many
supervised versions of PCA have been developed. For example, Carrasco and
Rossi (2016) use cross validation methods to supervise PCA, while Bai and Ng
(2008) consider targeted forecasting using subsets of $X$ (see also Armah
and Swanson (2010a,b)) and Cheng, Swanson, And Yang (2017). Given its ease of application as well as recent empirical evidence on its usefulness,
PCA (which is the oldest of the methods discussed in this paper; see
Spearman (1904) and the discussion in Swanson (2016) for further details),
has received the most attention in economics recently, and hence will be
discussed in considerably more detail below.

Penalized regression or shrinkage methods, which reduce or shrink redundant
or irrelevant variables are also important in big data analysis. Key
examples include ridge regression, the lasso, and the elastic net. When
viewed through the lens of multivariate regression analysis, all of these
methods involve shrinking the magnitude of coefficients in regression
models. When the \textquotedblleft penalty functions\textquotedblright\ are
carefully designed, and when the \textquotedblleft regularization
parameters\textquotedblright\ used to regulate the strength of the penalties
in these functions \ are of sufficient magnitude, then substantial dimension
reduction can be achieved. For example, when shrinkage is used in
conjunction with PCA, factor loading matrices can be induced to be sparse,
in the sense that certain coefficients in the linear combinations of the
predictor variables are identically zero. This nice feature
imposes parsimony on the number of variables used to form latent factors in
PCA, whereas under standard PCA; all predictors receive non-zero weight in
each latent factor. Just as in the case of PLS, the number of
predictors may be greater than the number of observations in the dataset
being analyzed using PCA.

To fix ideas, let's consider the \textquotedblleft
original\textquotedblright\ shrinkage estimator. Namely, assume that we are
interested in the model: 
\begin{equation*}
Y=X\theta +\varepsilon ,
\end{equation*}%
where $Y$ contains data on a single variable, there are many (possibly
highly correlated) variables represented in the data matrix, $X$, and $%
\varepsilon $ is an error term. Later, we shall introduce the ridge
estimator slightly differently, but for now, note that the ridge estimator
can be expressed as:%
\begin{equation*}
\widehat{\theta }_{ridge}=(X^{\prime }X+\lambda I)^{-1}X^{\prime }Y.
\end{equation*}%
The \textquotedblleft ridge\textquotedblright\ down the diagonal in this
estimator is equivalent to adding a penalty of $\lambda \sum_{i=1}^{N}%
\widehat{\theta }_{i}^{2}$ to the usual residual sum of squares term that is
minimized in least squares estimation, where $N$ is the number of predictors
in $X$. Here, as $\lambda \rightarrow 0$, $\widehat{\theta }%
_{ridge}\rightarrow \widehat{\theta }_{ols}$, and as $\lambda \rightarrow
\infty $, $\widehat{\theta }_{ridge}\rightarrow 0.$ Evidently, applying the
ridge penalty shrinks parameter estimates towards zero, which increase bias
and reduces estimator variance. One very important feature of ridge
regression is that invertibility problems associated with $X^{\prime }X$
when the number of predictors is too large relative to the number of
observations are no longer an issue, and there is always a unique solution
(i.e., $\widehat{\theta }_{ridge})$. Other shrinkage estimators that shall
be discussed in the sequel include one where the penalty function is $%
\lambda \sum_{i=1}^{N}\left\vert \widehat{\theta }_{i}\right\vert $ (the
lasso) and another that combines both
of the above penalty functions (the elastic net).

Another shrinkage estimator is based on bootstrap aggregation (bagging), and
was introduced by Breiman (1996). Stock and Watson (2012) note that
predictions of $Y$, at a point in time, $T+1$, conditional on information
available up through period $T,$ say $y_{T+1|T}^{f}$ can be constructed as
follows: 
\begin{equation*}
y_{T+1|T}^{f}=\sum_{i=1}^{N}\psi (\lambda t_{\widehat{\theta }(i)})\widehat{%
\theta }(i)X_{T}(i),
\end{equation*}%
where $X_{T}(i)$ is the datum on the $i^{th}$ variable in $X$ for period $T$%
, $\widehat{\theta }(i)$ is the least squares estimator from regressing $%
X_{T-1}(i)$ on $Y_{T}$, and $\psi (\lambda t_{\widehat{\theta }(i)})$ is a
regularized (through $\lambda )$ function of the t-statistic associated with
the aforementioned regression.\footnote{%
In their setup, Stock and Watson (2012) assume that the predictors are zero
mean random orthonormal variables. Also, $Y_{t}$ is assumed to be zero mean,
and the underlying model is assumed to be: 
\begin{equation*}
Y_{t}=\theta ^{\prime }X_{t-1}+\varepsilon _{t},
\end{equation*}%
where $\varepsilon _{t}$ is an error term with fixed variance.} For bagging $%
\lambda =1$, while various Bayesian predictors, including Bayesian model
averaging and empirical Bayes can also be formulated in this manner, by
setting $\lambda $ appropriately. Interestingly, Hirano and Wright (2017)
show that forecasting models constructed using out-of-sample or split sample
schemes perform well only when combined with other methods, such as bagging.
Broadly speaking, their results offer a glimpse into the benefits of using
state of the art (asymptotic) statistical analysis in order to examine new
methods that combine conventional out-of-sample approaches to model
selection and estimation with algorithmic approaches such as bagging. In
their paper, they show that out-of-sample schemes so regularly used for
model selection (and estimation are inefficient when applied in the
conventional manner. This finding is reversed when bagging or other risk
reduction methods are combined with conventional out-of-sample schemes,
however.

\subsection{Static and Dynamic Factor Augmented Forecasting Models}

Some of the most highly touted recent developments in forecasting 
center around estimation and asymptotic properties of diffusion indexes based on PCA; 
and the use of diffusion indexes in the construction of forecasting models.
Following the discussion of Stock and
Watson (2002a,b) and Armah and Swanson (2010a,b), we summarize key features
of recent developments by considering static and dynamic factor models in
order to motivate the use of diffusion indexes in forecasting.

Let $y_{t+h}$ be the scalar target forecast variable and $X_{t}$ be an $N$%
-dimensional vector of predictor variables, for $t=1,\ldots ,T$. Assume that 
$(y_{t+1},X_{t})$ has a dynamic factor model representation with $\overline{r%
}$ common dynamic factors, $f_{t},$ which can be written as:%
\begin{equation}
y_{t+h}=\beta ^{\prime }W_{t}+\alpha (L)f_{t}+\varepsilon _{t+h}
\label{equA}
\end{equation}%
and%
\begin{equation}
x_{it}=\lambda _{i}(L)f_{t}+e_{it},  \label{equB}
\end{equation}%
for $i=1,2,\ldots ,N$, where $W_{t}$ is an $l\times 1$\ vector of observable
variables with $l<<N,$ including lags of $y_{t};$ $\alpha (L)$ $%
=\sum\nolimits_{j=0}^{q}\alpha _{j}L^{j}$ and $\lambda
_{i}(L)=\sum\nolimits_{j=0}^{q}\lambda _{ij}L^{j}$ are finite order lag
polynomials in nonnegative powers of $L$; and $h>0$ is the forecast horizon.
Thus, all variables in $X_{t}$ can be expressed as a linear function of the
dynamic factors (and an idiosyncratic shock, $e_{it})$. This dimension
reducing feature of the model is the key feature worth noting. Now, we can
write (\ref{equA}) and (\ref{equB}) in static form as:%
\begin{equation}
y_{t+h}=\beta ^{\prime }W_{t}+\alpha ^{\prime }F_{t}+\varepsilon _{t+h}
\label{equF}
\end{equation}%
and%
\begin{equation}
x_{it}=\Lambda _{i}^{\prime }F_{t}+e_{it},  \label{equC}
\end{equation}%
where $F_{t}=(f_{t}^{\prime },\ldots ,f_{t-q}^{\prime })^{\prime }$ is an $%
r\times 1$ vector of static factors, with $r=(q+1)\overline{r},$ $\alpha $
is an $r\times 1$ vector, and $\Lambda _{i}=(\lambda _{i0}^{\prime },\ldots
,\lambda _{iq}^{\prime })^{\prime }$ is a vector of factor loadings on the
static factors, where $\lambda _{ij}$ is an $\overline{r}\times 1$ vector
for $j=0,\ldots ,q$ and $\beta =(\beta _{1},\ldots ,\beta _{l})^{\prime }$.
The model in (\ref{equF}) is the \textquotedblleft factor augmented
forecasting model\textquotedblright\ presented in the diffusion index
forecasting framework of Stock and Watson (2002a,b), and discussed further
in Bai and Ng (2007). The static factor in (\ref{equC}) is thus named because
the contemporaneous relationship between $x_{it}$ and $F_{t}$. One major
advantage of the static representation of the dynamic factor model is it
enables us to use principal component analysis to estimate the factors. This
involves estimating $F_{t}$ using an eigenvalue-eigenvector decomposition of
the sample covariance matrix of the data, after standardizing said data.
Moreover, an important theoretical feature of the model in (\ref{equF}) is
that consistent estimation of the factors in $F_{t}$, which can be achieved
via simple application of PCA, allows for subsequent $\sqrt{T}$ consistent
estimation of $\alpha $ and $\beta $ in (\ref{equF}) using quasi-maximum
likelihood, as long as $\sqrt{T}/N\rightarrow 0$, as $N,T\rightarrow \infty.$
Thus, as shown in Bai and Ng (2006), $F_{t}$, when estimated using the PCA
method outlined in Stock and Watson (2002a,b), can be treated as a vector of
observed regressors, eschewing the need to address the generated regressor
problem that often arises in applied econometrics. For a discussion of
alternative methods for factor forecasting based on estimation of
generalized dynamic factor (GDF) models, see Forni, Hallin, Lippi and
Reichlin (2005) and Forni, Hallin, Lippi and Zaffaroni (2015). For further
discussion of consistent estimation of factors in static as well as GDF
models, see Ding and Hwang (1999), Forni, Hallin, Lippi and Reichlin (2000),
Stock and Watson (2002b), Bai and Ng (2002) and Bai (2003), who show that
the space spanned by both the static and dynamic factors can be consistently
estimated when $N$ and $T$ are both large.

For forecasting purposes, little is gained from a clear distinction between
static and dynamic factors (see Schumacher (2007) for a comparison
of forecasts based on the use of factors estimated using static, dynamic,
and other estimation methods). Moreover, Boivin and Ng (2005) compare
alternative factor based forecast methodologies, and conclude that when the
dynamic structure is unknown and the model is characterized by complex
dynamics, the approach of Stock and Watson performs favorably.

Many important issues have been addressed in recent papers on diffusion
index forecasting. For example, Bai and Ng (2006a) stress that
the regressors (factors) in diffusion index models are estimated, which
substantially increases forecast error variances, relative to a simpler setup where diffusion indexes are not estimated. In a related paper,
Bai and Ng (2006b) examine whether observable economic variables can serve
as proxies for the underlying unobserved factors. In particular, they use a
variety of statistics to determine whether a group of observed variables
yields the same information as that contained in the latent
factors. Stock and Watson (2002a) have also attempted to link factors to
observed variables. Armah and Swanson (2010) argue that if individual observable
economic variables are indeed good proxies of the unobserved factors, then
these proxies can be used in place of the factors in the diffusion index
model for prediction. Once the set of factor proxies is fixed, one
effectively eliminates the incremental increase in forecast error variance
(i.e., uncertainty) associated with the use of estimated factors. Along
these lines, they consider \textquotedblleft smoothed\textquotedblright\
versions of the Bai and Ng (2006b) statistics that pre-select a set of
factor proxies prior to the ex-ante construction of a sequence of
predictions. Stock and Watson (1998,2009) demonstrate that when PCA is used
in estimation, factors remain consistent even when there is some time
variation in factor loadings and small amounts of data contamination, so
long as the number of variables in the panel dataset or the number of
predictors is very large (i.e., $N>>T)$. The usefulness of factor augmented
models that include cointegration restrictions is discussed in Banerjee,
Marcellino and Marsten (2014). The importance of assessing and testing for
structural breaks in these models is discussed in
Banerjee, Marcellino and Marsten (2008), Stock and Watson (2009), and Chen, Dolado and Gonzalo (2014). Factor loading and parameter
stability testing is addressed in Corradi and Swanson (2014), Breitung
and Eickmeier (2011), Goncalves and
Perron (2014), and Han and Inoue (2014). Finally, the empirical and
theoretical properties of factor augmented VARMA models are investigated in
Dufour and Stevanovic (2013).

For readers interested in estimation of
factors used in (\ref{equF}), we close this section by outlining further details, drawing
directly on Armah and Swanson (2010a,b). Let $k$ $(k<\min
\{N,T\})$ be an arbitrary number of factors, $\Lambda ^{k}$ be $N\times k$
factor loadings matrix, $(\Lambda _{1}^{k},\ldots ,\Lambda _{N}^{k})^{\prime
},$ and $F^{k}$ be the $T\times k$ matrix of factors $(F_{1}^{k},\ldots
,F_{T}^{k})^{\prime }$. From (\ref{equC}), estimates of $\Lambda _{i}^{k}$
and $F_{t}^{k}$ are obtained by solving the optimization problem:%
\begin{equation}
V(k)=\underset{\Lambda ^{k},F^{k}}{\min }(NT)^{-1}\underset{i=1}{\overset{N}{%
\sum }}\overset{T}{\underset{t=1}{\sum }}(x_{it}-\Lambda _{i}^{k\prime
}F_{t}^{k})^{2}.  \label{equ3}
\end{equation}%
Let $\widetilde{F}^{k}$ and $\widetilde{\Lambda }^{k}$ be the minimizers of
equation (\ref{equ3}). Since $\Lambda ^{k}$ and $F^{k}$ are not separately
identifiable, if $N>T$, a computationally expedient approach would be to
concentrate out $\widetilde{\Lambda }^{k}$ and minimize (\ref{equ3}) subject
to the normalization $F^{k\prime }F^{k}/T=I_{k}$. Minimizing (\ref{equ3}) is
equivalent to maximizing $tr[F^{k\prime }(XX^{\prime })F^{k}]$. This
optimization is solved by setting $\widetilde{F}^{k}$ to be the matrix of
the $k$ eigenvectors of $XX^{\prime }$ that correspond to the $k$ largest
eigenvalues of $XX^{\prime }$. Note that $tr[\cdot ]$ represents the matrix
trace. Let $\widetilde{D}$ be a $k\times k$ diagonal matrix consisting of
the $k$ largest eigenvalues of $XX^{\prime }$. The estimated factor matrix,
denoted by $\widetilde{F}^{k}$, is $\sqrt{T}$ times the eigenvectors
corresponding to the $k$ largest eigenvalues of the $T\times T$ matrix $%
XX^{\prime }$. Given $\widetilde{F}^{k}$ and the normalization $F^{k\prime
}F^{k}/T=I_{k}$, $\widetilde{\Lambda }^{k\prime }=(\widetilde{F}^{k\prime }%
\widetilde{F}^{k})^{-1}\widetilde{F}^{k\prime }X=\widetilde{F}^{k\prime }X/T$
is the corresponding factor loadings matrix.

The solution to the optimization problem in (\ref{equ3}) is not unique. If $%
N<T$, it becomes computationally advantageous to concentrate out $\overline{F%
}^{k}$ and minimize (\ref{equ3}) subject to $\overline{\Lambda }^{k\prime }%
\overline{\Lambda }^{k}/N=I_{k}$. This minimization is the same as
maximizing $tr[\Lambda ^{k\prime }X^{\prime }X\Lambda ^{k}]$, the solution
of which is to set $\overline{\Lambda }^{k}$ equal to the eigenvectors of
the $N\times N$ matrix $X^{\prime }X$ that correspond \ to its $k$ largest
eigenvalues. One can thus estimate the factors as $\overline{F}%
^{k}=X^{\prime }\overline{\Lambda }^{k}/N$. $\widetilde{F}^{k}$ and $%
\overline{F}^{k}$ span the same column spaces, hence for forecasting
purposes, they can be used interchangeably. Given $\widetilde{F}^{k}$ and $%
\widetilde{\Lambda }^{k}$, let $\widehat{V}(k)=(NT)^{-1}\underset{i=1}{%
\overset{N}{\sum }}\overset{T}{\underset{t=1}{\sum }}(x_{it}-\widetilde{%
\Lambda }_{i}^{k\prime }\widetilde{F}_{t}^{k})^{2}$ be the sum of squared
residuals from regressions of $X_{i}$ on the $k$ factors, $\forall i$. A
penalty function for over fitting, $g(N,T)$, is chosen such that the loss
function 
\begin{equation}
IC(k)=\log (\widehat{V}(k))+kg(N,T)  \label{equD}
\end{equation}%
can consistently estimate $r$. Let \textit{kmax} be a bounded integer such
that $r\leq k\max $. Bai and Ng (2002) propose three versions of the penalty
function $g(N,T)$, namely, $g_{1}(N,T)=\left( \frac{N+T}{NT}\right) \log
\left( \frac{NT}{N+T}\right) ,$ $g_{2}(N,T)=\left( \frac{N+T}{NT}\right)
\log C_{NT}^{2},$ and $g_{3}(N,T)=\left( \frac{\log (C_{NT}^{2})}{C_{NT}^{2}}%
\right) ,$ all of which lead to consistent estimation of $r$. Additional
details on the estimation of $r$ are contained in Bai and Ng (2002). 
Alternative methods for selecting $r$ are discussed in Chen, Huang, and Tu (2010), Onatski (2015), Carrasco and Rossi (2016),
and the references cited therein.

For further reading in the area of factor models, including high dimensional
covariance matrix estimation in approximate factor models and projected
principal components analysis in factor models, see Fan, Liao and Wang
(2016) and Fan, Laio and Mincheva (2011).

\subsection{New Directions in Diffusion Index Estimation}

As discussed earlier, ongoing research efforts in the study of factor
augmented forecasting models include the analysis of problems associated
with the \textquotedblleft selection\textquotedblright\ of diffusion indexes
that are most useful for predicting $y_{t+1}$. For example, see Bai and Ng
(2008,2009) and Schumacher (2009), who discuss using targeted predictors
based on quadratic principal components and thresholding rules for variable
subset selection to estimate diffusion indexes. Armah and Swanson (2010a,b)
also discuss this issue. Further, Carrasco and Rossi (2016) propose cross
validation methods for selecting the \textquotedblleft
best\textquotedblright\ diffusion index for use in forecasting. A related
area of research, which is the subject of this subsection, is the development of
alternative diffusion index estimators, important examples of which use
shrinkage methods in order to impose sparseness on the factor loadings used in the construction of 
diffusion indexes. Two of the many interesting new estimators in this context include 
sparse principal components analysis (SPCA) and
independent component analysis (ICA).

Zou, Hastie, and Tibshirani (2006) note that diffusion indexes
estimated using PCA are linear combinations of all underlying predictor variables, and
factor loadings are hence all nonzero, which adversely affects
the parsimony of forecasting models, a property known to be important in
time series forecasting. Moreover, they stress that diffusion indexes are thus difficult to interpret. In light of this, they
propose SPCA, in which the least absolute shrinkage selection
operator (lasso) or the related shrinkage estimator called the elastic net
is utilized in order to construct principal components with sparse loadings.
This is done this by first reformulating PCA as a regression type
optimization problem, and then by using a lasso (elastic net) on the
coefficients in a suitably constrained regression model.

Before further discussing SPCA, it is worth noting that the lasso and
elastic net are important techniques for big data analysis in and of
themselves, and are related to the venerable ridge regression estimator.
Using the above notation, say that%
\begin{equation*}
y_{t}=X_{t}^{\prime }\theta +\varepsilon _{t}.
\end{equation*}%
Here, penalized (shrinkage type) regression is carried out as follows: For
the ridge estimator, construct:%
\begin{equation*}
\widehat{\theta }_{ridge}=\arg \min_{\theta } \left\{ \left\Vert y-\Sigma
_{i=1}^{N}X_{i}\theta _{i}\right\Vert ^{2}+\lambda _{2}\Sigma
_{i=1}^{N}\theta _{i}^{2} \right\},
\end{equation*}%
where $y$ is the $T$x$1$ target variable, $X=[X_{1},...,X_{N}],$ $i=1,...,N$
is the $T$x$N$ predictor matrix, with $X_{i}=(X_{1,i},...,X_{T,i})^{\prime }$,
and $\lambda >0$ is the tuning parameter. Notice that this is an alternative formulation of 
$\widehat{\theta }_{ridge}$ to that given earlier.
The more recently developed lasso and the
elastic net estimators involve imposition of $L_{1}$ (lasso) and $L_{1}$+$%
L_{2}$ $-norm$ penalties on parameter magnitudes, and are formulated as:

\begin{equation*}
\widehat{\theta }_{lasso}=\arg \min_{\theta } \left\{ \left\Vert y-\Sigma
_{i=1}^{N}X_{i}\theta _{i}\right\Vert ^{2}+\lambda _{1}\Sigma
_{i=1}^{N}\left\vert \theta _{i}\right\vert \right\},
\end{equation*}%
and%
\begin{equation*}
\widehat{\theta }_{elastic\text{ }net}=(1+\lambda _{2}) \arg
\min_{\theta } \left\{ \left\Vert y-\Sigma _{i=1}^{N}X_{i}\theta _{i}\right\Vert
^{2}+\lambda _{1}\Sigma _{j=1}^{N}\left\vert \theta _{j}\right\vert +\lambda
_{2}\Sigma _{j=1}^{N}\theta _{j}^{2}\right\} .
\end{equation*}

The choice of regularization parameters can impact on the predictive performance of models specified using these sorts of methods.
For a discussion further of the regularization parameters, including values to use thereof, 
please refer to Kim and Swanson (2017), as well as the papers cited in Kim and Swanson 
where the various estimation algorithms for these methods are developed. 

Interestingly, SPCA follows directly by formulating PCA as a regression-type
optimization problem, and then by subsequently imposing lasso (elastic net)
constraints on the regression coefficients in the optimization problem. Put
simply, factor loading can be recovered by regressing principal components
on the $N$ variables in $X_{t}$, as shown in Zou, Hastie, and
Tibshirani (2006). Here, imposition of the $L_{2}$ $-norm$ penalty in
ridge regression allows for $N>T.$ Moreover, when the lasso or elastic net
is utilized in this context, then large enough $\lambda _{1}$ yields sparse $%
\widehat{\theta }.$ In this sense, SPCA is a natural data reduction method.
Since the important paper by Zou et al., many authors have proposed
modifications to SPCA, as discussed in Kim and Swanson (2017).

Broadly speaking, the lasso and elastic net constitute two of the most
important penalized regression methods currently available, in which all
predictor variables are retained in a model, but are constrained
(regularized) by shrinking them towards zero. For important descriptions of
these methods, see Tibshirani (1996), Zou and Hastie (2005), and Zou (2006).

All of the above penalized regression or shrinkage type methods are examples
of machine learning. Other machine learning algorithms have also recently
been explored in economics. Two examples are bagging and boosting. Bagging
(also called bootstrap aggregation) involves first drawing bootstrap samples
from an in-sample training dataset, and then constructing predictions, which
are later combined. This algorithm is discussed above. Boosting is another
so-called machine learning ensemble meta-algorithm algorithm that utilizes a
supervised and user-determined set of functions or $learners$ (e.g., least
square estimators), and uses the set repeatedly on filtered data, which are
typically outputs from previous iterations of the learning algorithm.
Broadly speaking, boosting isolates which variables, from amongst a large
group of variables, are useful for predicting a target variable. More
specifically, boosting estimates an unknown function (e.g., the conditional
mean) using sequential step-wise forward regression, with learners that may
not only be least squares estimators, but may also be smoothing splines and
kernel regressions, for example. For further discussion of boosting, see
Freund and Schapire (1997), Bai and Ng (2009), Kim and Swanson (2014), and
the references therein.

Two further examples include the non-negative garrote (see Breiman (1995)
and Yuan and Lin (2007)) and least angle regression (see Efron, Hastie,
Johnstone and Tibshirani (2004) and Bai and Ng (2008)), both of which are
closely related to the elastic net.

Returning to the main subject of this section, we now discuss independent
component analysis, which is predicated on the idea of \textquotedblleft
opening\textquotedblright\ the black box in which principal components often
reside, and is an alternative to PCA and SPCA. ICA is used in many applications, from brain imaging to stock price
return modeling. In all cases, there is a large set of observed individual
signals, and it is assumed that each signal depends on several factors,
which are unobserved. In this sense, the motivation is exactly the same as
that used to justify PCA.

The starting point for ICA\ is the very simple assumption that the
components, say $F,$ are statistically independent in equation (\ref{equF}).
This assumption is potentially much stronger than the orthogonality imposed
under PCA. The key issue in ICA is the measurement of the \textquotedblleft
level\textquotedblright\ of independence between components. More
specifically, ICA begins with statistically independent (and unobserved)
source data, $S$, which are mixed according to an unknown \textquotedblleft
mixing matrix\textquotedblright , $\Omega $; and $X,$ which is observed, is
a mixture of $S,$ weighted by $\Omega .$ For simplicity, we assume that the
unknown mixing matrix, $\Omega ,$ is square, although this assumption can be
relaxed. Thus, it is assumed that $X=S\Omega .$ Stated differently, assume
that: 
\begin{eqnarray}
X_{1} &=&\omega _{11}S_{1}+\cdots +\omega _{1N}S_{N}  \label{eq121} \\
X_{2} &=&\omega _{21}S_{1}+\cdots +\omega _{2N}S_{N}  \notag \\
&&\vdots  \notag \\
X_{N} &=&\omega _{1N}S_{1}+\cdots +\omega _{NN}S_{N},  \notag
\end{eqnarray}%
where $\omega _{ij}$ is the $(i,j)$ element of $\Omega .$ Since $\Omega $
and $S$ are unobserved, one must estimate the \textquotedblleft demixing
matrix\textquotedblright , $\Psi ,$ which transforms the observed $X$ into
the independent components, $F$. That is, $F=X\Psi $, or $F=S\Omega \Psi .$
As detailed in Kim and Swanson (2017), if $\Omega $ is square, then so is $%
\Psi $, and $\Psi =\Omega ^{-1}$, so that $F$ is exactly the same as $S$,
and perfect separation occurs. In general, it is only possible to find $\Psi 
$ such that $\Omega \Psi =PD,$ where $P$ is a permutation matrix and $D$ is
a diagonal scaling matrix. The independent components, $F$ are latent
variables, and are analogous to the principal components discussed in the
case of PCA. In summary, upon estimation of $\Omega $ and $S$, it is
feasible to estimate the demixing matrix $\Psi ,$ and the independent
components, $F.$ However (\ref{eq121}) is not identified unless several
assumptions are made. The first assumption is that the sources, $S,$ are
statistically independent. Since various sources of information (for
example, consumer's behavior, political decisions, etc.) may have an impact
on the values of macroeconomic variables, this assumption is not strong. The
second assumption is that the signals are stationary. For further details,
see Tong, Liu, Soon, Huan (1991). ICA maps the $N$ components of $X$ into
the rank $N$ matrix, $F$. However, we can simply construct factors using up
to $r$ $\left( <N\right) $ components, without loss of generality, for
comparability with PCA. Alternatively, one might carry out ICA using $r$
principal components, hence further filtering diffusion indexes constructed
using PCA in order to obtain statistically independent variants thereof (see
Stone (2004) for further details). In general, the above model would be more
realistic if there were noise terms added. See Hyv\"{a}rinen and Oja (2000)
for a detailed discussion of the noise-free model, and Hyv\"{a}rinen
(1998,1999) for a discussion of the model with noise added.

For a detailed comparison of ICA with PCA, see Kim and Swanson (2016), who
note that the main difference between ICA and PCA is in the properties of
the factors obtained. Principal components are uncorrelated and have
descending variance so that they are naturally ordered in terms of their
variances. While setting the diffusion index in equation (\ref{equA}) equal
to the highest variance (correlation) principal components may well not
equate with the specification of the indexes that are most useful for
forecasting a given variable, say $y_{t}$, it is certainly the case that
components explaining the largest share of the variance are often assumed to
be the \textquotedblleft relevant\textquotedblright\ ones. For simplicity,
consider two observables, $X=\left( X_{1},X_{2}\right) .$ PCA finds a matrix
which transforms $X$ into uncorrelated components $F=\left(
F_{1},F_{2}\right) ,$ such that the uncorrelated components have a joint
probability density function, $p_{F}\left( F\right) $ with: 
\begin{equation}
E\left( F_{1}F_{2}\right) =E\left( F_{1}\right) E\left( F_{2}\right) .
\label{eq05}
\end{equation}%
On the other hand, ICA\ finds a demixing matrix which transforms the
observed $X=\left( X_{1},X_{2}\right) $ into independent components $F^{\ast
}=\left( F_{1}^{\ast },F_{2}^{\ast }\right) ,$ such that the independent
components have a joint pdf $p_{F^{\ast }}\left( F^{\ast }\right) $ with: 
\begin{equation}
E\left[ F_{1}^{\ast p}F_{2}^{\ast q}\right] =E\left[ F_{1}^{\ast p}\right] E%
\left[ F_{2}^{\ast q}\right] ,  \label{eq06}
\end{equation}%
for every positive integer value of $p$ and $q$. Evidently, ICA is more
restrictive, and it should thus not be surprising that implementation is
much more difficult than PCA, in which estimation is much simpler, since it
just involves finding a linear transformation of components which are
uncorrelated. Moreover, there is no natural ordering of latent factors in
ICA. This is perhaps a blessing in disguise. Namely, as stated above, there
is no a priori reason why the ordinal (correlation) ranking of diffusion
indexes corresponds to a ranking of their usefulness for predicting $y_{t}$
(see Kim and Swanson (2014), Bai and Ng (2008) and Carrasco and Rossi (2016)
for further discussion of this issue).

Even given all of the recent progress in the area, much remains to be done. There are innumerable possible
estimators and algorithms than can potentially be utilized for machine
learning (indeed we have touched in our discussion on only a very few of
those already available). What will probably differentiate the
\textquotedblleft good methods\textquotedblright\ from the \textquotedblleft
not so good\textquotedblright\ is their ability to properly marry the latest
tools in statistical inference with the latest algorithmic techniques. For
example, step-wise methods now often rely on learning functions and
thresholding variables (such as t-statistics) centered around conditional
mean type prediction, while there is a clearly a need to fully incorporate
conditional or predictive density type prediction in new methods. As another
example, recall our earlier discussion on the use of asymptotic analysis to
examine the combination of conventional out-of-sample schemes with bootstrap
aggregation. Many of these sorts of analyses remain to be done in the
context of combining conventional forecasting approaches with state of the
art dimension reduction, machine learning, and penalized regression
algorithms.

\section{Forecast Evaluation}

One of the reasons why machine learning has taken so long to
\textquotedblleft catch on\textquotedblright\ in economics is the problem of
over-fitting. This issue is made very clear by considering the case of
neural networks. We know, from Hornik, Stinchcombe, and White\ (1989) that
neural networks are universal approximators, in the sense that properly
designed neural networks with numbers of parameters that grow appropriately, as the sample
grows, can approximate an arbitrary function arbitrarily well. However, we
also know, from numerous empirical experiments, that more heavily
parameterized models often tend to be outperformed, in a predictive sense,
by more parsimonious models. The reasons for this are many, and include the
effect of specifying models that are crude approximations of reality, and
the fact that structural change is prevalent in time series models. Loosely
speaking, then, it was the poor predictive accuracy of models that have been too heavily
parameterized, or over-fitted, that led economists to eschew adopting machine
learning and related big data methods. 
This is all changing, though, in part because a plethora of new tests for assessing
predictive accuracy which account for over-fitting, have recently been developed.
However, just as is the case in machine learning, much remains to be done in
the area of predictive accuracy testing. 

We begin this section by discussing standard predictive accuracy tests
that are used every day by applied practitioners. Thereafter, we discuss
novel new tests currently being developed that allow for model forecast
comparison without specification of a loss function.

\subsection{Loss Function Dependent Model Evaluation and Selection}

As previously, assume that the objective is to predict $y_{t}.$The null
hypothesis of equal predictive accuracy between two models of $y_t$, say model 0 and
model 1, is specified as: 
\begin{equation*}
H_{0}:E(L(u_{0,t+h})-L(u_{1,t+h}))=0
\end{equation*}%
and 
\begin{equation*}
H_{A}:E(L(u_{0,t+h})-L(u_{1,t+h}))\neq 0,
\end{equation*}%
where $L(\cdot )$ is a loss function. In practice, we do not observe $u_{0,t+h}$ and $u_{1,t+h},$ which are
assumed to be out-of-sample $h$-step ahead forecast errors, but only
estimates thereof (i.e., say $\widehat{u}_{0,t+h}$ and $\widehat{u}_{1,t+h},$
respectively). When $P/R\rightarrow \pi =0,$ as $P,R\rightarrow \infty $
(asymptotically negligible parameter estimation error), where $P$ is the
number of forecast errors that we have constructed for each model being
compared, and $R$ is the initial \textquotedblleft
in-sample\textquotedblright\ estimation period (i.e., $P+R=T),$ under
recursive or rolling estimation, say, then we can construct the standard
version of DM predictive accuracy
test in order to test $H_{0}$. Namely: 
\begin{equation*}
DM_{P}=\frac{\overline{d}_{t}}{\widehat{\sigma }_{\overline{d}_{t}}}\overset{%
d}{\rightarrow }N(0,1),
\end{equation*}%
where 
\begin{equation*}
\overline{d}_{t}=\frac{1}{P}\sum_{t=R+1}^{T}d_{t},\text{ }d_{t}=L(\widehat{u}%
_{0,t+h})-L(\widehat{u}_{1,t+h})\text{, and }\widehat{\sigma }_{\overline{d}%
_{t}}=\frac{\widehat{\sigma }_{d_{t}}}{\sqrt{P}}.
\end{equation*}%
In the above test, for which a heteroscedasticity and autocorrelation
consistent estimator of $\widehat{\sigma }_{d_{t}}$ is utilized whenever $%
h>1 $, the assumption that parameter estimation error is asymptotically
negligible allows for the use of any loss function, $L(\cdot )$, including
one that is non-differentiable. However, if accounting for parameter
estimation error, one can consider only differentiable loss functions (see
Corradi and Swanson (2006b) for complete details). Moreover, regardless of
loss function, the normal limiting distribution does not obtain if models 0
and 1 are nested; in which case non-standard critical values must be used,
as outlined in McCracken (2000) and Clark and McCracken (2001,2013). An
alternative test, which does not require correct dynamic specification
and/or conditional homoskedasticity, and which is robust to nonnestedness is
proposed by Chao, Corradi, and Swanson (2001). The test statistic is: 
\begin{equation}
m_{P}=P^{-1/2}\sum_{t=R+1}^{T}\widehat{u}_{0,t+h}X_{t},  \label{stat}
\end{equation}%
where $\widehat{u}_{0,t+1}$ is the estimated prediction error, and $X_{t}$ is some (possibly vector values) set
of variables that might be useful for predicting our target variable, $y_t$. Here $X_{t}$ may include lags$.$ A simple example of where this sort of test is useful involves testing for linear (predictive) Granger causality, where the null and alternative models are (respectively):
\begin{eqnarray*}
y_{t}=\sum_{j=1}^{q}\beta_{j}y_{t-j}+u_{0,t} 
\end{eqnarray*}
and
\begin{eqnarray*}
	y_{t}=\sum_{j=1}^{q}\beta_{j}y_{t-j}+\sum_{j=1}^{k}\alpha_{j}x_{t-j}+u_{1,t}  \label{unrest}
\end{eqnarray*}
In this example, the practitioner estimates the null model, constructs (recursive or rolling, say) predictions, and utilizes the prediction errors 
(i.e., the $\widehat{u}_{0,t+1}$, for forecast horizon $h=1$) in the construction of the test statistic, $m_P$, where $P$ denotes the number of prediction errors. A key advantage of using this test is that models may be nested, thus avoiding issues associated with the testing of nested models that arise when implementing $DM_P$ type tests.

More complex versions of this test
that are consistent against generic nonlinear (Granger causal) alternatives are discussed in
Corradi and Swanson (2002). In this test, the hypotheses of interest are: 
\begin{eqnarray*}
\widetilde{H}_{0} &:&E(u_{0,t+h}X_{t-j})=0,\text{ }j=0,1,\ldots k.\text{ } \\
\text{ }\widetilde{H}_{A} &:&E(u_{0,t+h}X_{t-j})\neq 0\text{ for some }j,%
\text{ \ }j=0,1,\ldots k.
\end{eqnarray*}%
As an example, note that if the model being tested does not include a
variable, say $Z_{t},$ then inclusion of $Z_{t}$ in $X_{t}$ is equivalent to
testing for out-of-sample Granger causality from $Z_{t}$ to $y_{t}.$ Notice
also that this test is a variety of the well known Bierens specification test, rather than
a test which directly compares two models, such as the DM test. When $P/R\rightarrow
\pi =0,$ as $P,R\rightarrow \infty ,$ then $m_{p}^{\prime }\widehat{S}%
_{11}m_{P}\overset{d}{\rightarrow }\chi _{k}^{2},$ where $k$ is the number
of new variables in $X_{t}$, and $\widehat{S}_{11}$ is an estimator of a $%
k\times k$ matrix $S_{11}$, with: 
\begin{equation*}
S_{11}=\sum_{j=-\infty }^{\infty }E\left( (X_{t}u_{0,t+h}-\mu
_{1})(X_{t-j}u_{0,t+h-j}-\mu _{1})^{\prime }\right) ,
\end{equation*}%
where $\mu _{1}=E(X_{t}u_{t+h}).$ In empirical applications, one estimates $%
S_{11}$ as follows: 
\begin{eqnarray*}
\widehat{S}_{11} &=&\frac{1}{P}\sum_{t=R}^{T-1}(\widehat{u}_{0,t+h}X_{t}-%
\widehat{\mu }_{1})(\widehat{u}_{0,t+h}X_{t}-\widehat{\mu }_{1})^{\prime } \\
&&+\frac{1}{P}\sum_{t=\tau }^{l_{T}}w_{\tau }\sum_{t=R+\tau }^{T-1}(\widehat{%
u}_{0,t+h}X_{t}-\widehat{\mu }_{1})(\widehat{u}_{0,t+h-\tau }X_{t-\tau }-%
\widehat{\mu }_{1})^{\prime } \\
&&+\frac{1}{P}\sum_{t=\tau }^{l_{T}}w_{\tau }\sum_{t=R+\tau }^{T-1}(\widehat{%
u}_{0,t+h-\tau }X_{t-\tau }-\widehat{\mu }_{1})(\widehat{u}_{0,t+h}X_{t}-%
\widehat{\mu }_{1})^{\prime },
\end{eqnarray*}%
where $\widehat{\mu }_{1}=\frac{1}{P}\sum_{t=R}^{T-1}\widehat{u}%
_{0,t+1}X_{t}.$

Alternatively, when comparing multiple different models, Sullivan,
Timmermann and White (1999) and White (2000) proposes using the following
test statistic: 
\begin{equation*}
S_{P}=\max_{k=1,...,m}S_{P}(1,k),
\end{equation*}%
where 
\begin{equation*}
S_{P}(1,k)=\frac{1}{\sqrt{P}}\sum_{t=R+1}^{T}\left( L(\widehat{u}_{0,t+h})-L(%
\widehat{u}_{k,t+1})\right) ,\text{ }k=1,...,m.
\end{equation*}%
The hypotheses are formulated as 
\begin{equation*}
H_{0}:\max_{k=1,...,m}E(L(u_{0,t+1})-L(u_{k,t+1}))\leq 0.
\end{equation*}%
\begin{equation*}
H_{A}:\max_{k=1,...,m}E(L(u_{0,t+1})-L(u_{k,t+1}))>0.
\end{equation*}

Thus, under the null hypothesis, no competitor model, amongst the set of the 
$m$ alternatives, can provide a more (loss function specific) accurate
prediction than the benchmark model (i.e., model 0). On the other hand,
under the alternative, at least one competitor (and in particular, the best
competitor) provides more accurate predictions than the benchmark. Critical
values for this test can be constructed using the block
bootstrap, as discussed in Corradi and Swanson (2007). An interesting
extension of this test, in which rolling data windows are used in model
estimation and all estimated parameters are conditioned on, is discussed in
Giacomini and White (2006). For extensions of the above tests to predictive
density evaluation, see Corradi and Swanson (2005,2006a,b).

\subsection{Loss Function Free Model Evaluation and Selection}

In this section we summarize new developments in forecast evaluation which
is valid under generalized loss functions, and which is\ based directly on
the evaluation of $F(u),$ the CDF of the forecast error. In particular, note
that Corradi, Jin, and Swanson (2017) discuss testing for GL and CL forecast
superiority. Their tests allow for parameter estimation error, data
dependence, and comparison of multiple models,\ but require the underlying
processes to be strictly stationary. To start, assume that the loss function ($%
L)$ is defined such that \textbf{\ }$L:\mathbb{R\rightarrow R}^{+}$ is
continuously differentiable, except for finitely many points, with
derivative $L^{\prime },$ such that $L^{\prime }(z)\leq 0,$ for all $z\leq
0, $ and $L^{\prime }(z)\geq 0,$ for all $z\geq 0.$\smallskip

\textbf{Definition (Forecast Superiority): }$u_{1}$ General-Loss (GL)
outperforms $u_{2},$ denoted as $u_{1}\succeq _{G}u_{2},$ if and only if $%
E(L(u_{1}))\leq E(L(u_{2})),$ for all $L\in \mathcal{L}_{G}$; and $u_{1}$
Convex-Loss (CL) outperforms $u_{2},$ denoted as $u_{1}\succeq _{C}u_{2},$
if and only if $E(L(u_{1}))\leq E(L(u_{2}))$, for all $L\in \mathcal{L}_{C}$.

Here, $u_{1}$\ and $u_{2}$\ are sequences of forecast errors, as above. 
In order to connect the notion of forecast superiority to that of
stochastic dominance, CJS establish a mapping between GL\ forecast
superiority and first order stochastic dominance. They also establish linkages
between CL\ forecast superiority and second order stochastic dominance.
They then derive direct tests for GL/CL\ forecast
superiority.\textbf{\ }Define:
\begin{equation}
G(x)=(F_{2}(x)-F_{1}(x))sgn(x),  \label{Gx}
\end{equation}%
where $sgn(x)=1$ if $x\geq 0,$ and $=-1$ if $x<0;$ and%
\begin{equation}
C(x)=\int_{-\infty }^{x}(F_{1}(t)-F_{2}(t))dt1(x<0)+\int_{x}^{\infty
}(F_{2}(t)-F_{1}(t))dt1(x\geq 0),  \label{Cx}
\end{equation}%
where 1$\left( \cdot \right) $ denotes the indicator function, which takes
the value 1 if the condition is met, and 0 otherwise. CJS show that $%
E(L(u_{1}))\leq E(L(u_{2})),$ for all $L\in \mathcal{L}_{G},$ if and only if 
$G(x)\leq 0,$ \textrm{for} \textrm{all} $x\in \mathcal{X}$, where $\mathcal{X%
}$ is the union of the supports of all forecast errors; and $E(L(u_{1}))\leq
E(L(u_{2})),$ for all $L\in \mathcal{L}_{C},$ if and only if $C(x)\leq 0$
for all $x\in \mathcal{X}.$

\noindent Before implementing GL forecast superiority tests, one can
construct a graph that contains a plot of $G(x)$\ against $x.$\ When $%
u_{1}\succeq _{G}u_{2}$, we expect all points to lie below or on the zero
line. In other words, a crossing of the zero line in the graph indicates a
violation of GL\ forecast superiority. Similarly, one can construct a graph
that contains a plot of $C(x)$\ against $x.$\ When $u_{1}\succeq _{C}u_{2}$,
we expect all points to lie below or on the zero line. In other words, a
crossing of the zero line in the graph indicates a violation of CL\ forecast
superiority.

Now, suppose that there are $m$ sets of forecast errors $u_{1},...,u_{m},$
resulting from $m$ forecasting models, and that we wish to test the null that
$E(L(u_{1}))\leq E(L(u_{2})),$ for all $L\in \mathcal{L}_{G}$, against the negation thereof (see CJS (2017) for complete details). When testing this null of no forecast superiority, it suffices to construct statistics as follows.
For $k=1,...,m,$\ define:

\begin{eqnarray*}
F_{k}\left( x\right) &=&P(u_{k,t}\leq x) \text{ and} \\
\overline{F}_{k,n}\left( x\right) &=&P^{-1}\sum_{t=R}^{T}1\left( u_{k,t}\leq
x\right) ,
\end{eqnarray*}

The statistics discussed by CJS (2017) are constructed by calculating: 
\begin{equation*}
TG_{n}^{+}=\underset{k=2,..,m}{\max }\underset{x\in \mathcal{X}^{+}}{\sup }%
\sqrt{n}G_{k,n}(x)\text{ and }TG_{n}^{-}=\underset{k=2,..,m}{\max }\underset{%
x\in \mathcal{X}^{-}}{\sup }\sqrt{n}G_{k,n}(x)
\end{equation*}%
and%
\begin{equation*}
TC_{n}^{+}=\underset{k=2,..,m}{\max }\underset{x\in \mathcal{X}^{+}}{\sup }%
\sqrt{n}C_{k,n}(x)\text{ and }TC_{n}^{-}=\underset{k=2,..,m}{\max }\underset{%
x\in \mathcal{X}^{-}}{\sup }\sqrt{n}C_{k,n}(x),
\end{equation*}%
where 
$G_{k,n}(x)\ =\left( \overline{F}_{k,n}\left( x\right) -\overline{F}%
_{1,n}\left( x\right) \right) sgn(x)$ 

and 

$C_{k,n}(x)=\left\{ \int_{-\infty
}^{x}\left( \overline{F}_{1,n}\left( s\right) \right. \right. $

\noindent $\left. \left. -\overline{F}_{k,n}\left( s\right) \right)
ds1(x<0)+\int_{x}^{\infty }\left( \overline{F}_{k,n}\left( s\right) - \overline{F}_{1,n}\left( s\right) \right) ds1(x\geq 0)\right\} .$ 

Note that the positive and negative parts of $\mathcal{X}$ are treated separately in the above statistics. This is because stochastic equicontinuity of the empirical processes cannot be otherwise established, precluding inference based on statistics constructed without separately considering the positive and negative regions of the support.

For discussion of computation of the suprema in these statistics, as well as
discussion of more general versions of the test statistics that explicitly
account for parameter estimation error and different model estimation
schemes (e.g., rolling versus recursive model estimation), see CJS (2017).

Critical values are constructed by using bootstrap methods, as
discussed in CJS (2017). Although CJS make a substantial contribution in the nascent loss function robust
forecast evaluation, their tests are not uniformly valid, as they have
correct asymptotic size only under the least favorable case under the null
hypothesis. It remains to develop tests that are uniformly asymptotically
valid. Many theoretical
questions of this sort remain unanswered in the predictive accuracy and
model selection literature, and as new and increasingly complex machine
learning methods are developed, theorists will have their hands full keeping
up. For a key example of the type of analytically sophisticated analysis
that is necessary in order to continue advancing our understanding of model
selection, see Hirano and Wright (2017).

\section{Empirical Illustration: Predicting Interest Rates Using Big Data
versus Small Data Methods}

In order to fix some of the ideas discussed in this paper, we carry out a
small empirical investigation that utilizes a subset of the leading methods
discussed above. Our objective is to predict U.S. Treasury yields of various
maturities (i.e., the term structure of interest rates). Predictions will be
made using \textquotedblleft small data\textquotedblright\ models, including
autoregressive, vector autoregressive, and dynamic Nelson-Siegel models, and
\textquotedblleft big data\textquotedblright\ models that utilize diffusion
indexes estimated from a largescale macroeconomic dataset.

\subsection{Experimental Setup}

All models in all experiments are re-estimated prior to the construction of
each new prediction, using rolling 120 month windows of data; and estimation
is carried out using least squares and principal components analysis.
Monthly yield forecasts for horizons $h=1-$, $3-$, and $12-$ steps ahead are
constructed for a variety of bond maturities, and these are aggregated using
mean square forecast error (MSFE) criteria, and evaluated using the $DM_{P}$
predictive accuracy test discussed above. The development of a more
exhaustive set of experiments is left to future research, and all
conclusions made based on our experiments should thus be viewed with caution.

\noindent A summary of the models used in our prediction experiments is
given below. \newline

\noindent {\Large Small Data Models}

\noindent \textbf{Autoregressive (AR) and Vector Autoregressive (VAR) Models:%
}

\noindent \textit{(Models in this section are summarized in Table 1, and
include: AR(1), VAR(1), AR(SIC), and VAR(SIC))}

We utilize a number of benchmark time series models, specified as follows: 
\begin{equation}
y_{t+h}(\tau )= c +\beta ^{\prime }W_{t}+\varepsilon _{t+h},  \label{var}
\end{equation}%
where $\tau $ denotes the maturity of a bond (bill) for which the scalar, $%
y_{t+h}(\tau ),$ measures the annual yield. Additionally, $W_{t}$ contains
lags of $y_{t+h}(\tau )$ in autoregressive specifications, and contains lags
of $y_{t+h}(\tau )$ and additional explanatory variables in vector
autoregressive specifications, with $\beta $ a conformably defined
coefficient vector.\footnote{%
When specifying VAR models, equation (\ref{var}) constitutes only one ($\tau 
$-maturity) equation in the VAR. As the same set of explanatory variables is
utilized in each equation in the VAR, the SUR (seemingly unrelated
regression)\ result ensures that consistent and efficient parameter
estimates can be obtained via application of equation by equation least
squares.} In AR and VAR specifications, up to 5 lags of $y_{t+h}(\tau )$ are
included in our models, with the number of lags selected using the Schwarz
information criterion (SIC). In addition to AR(SIC) and VAR(SIC) models,
straw-man AR(1) and VAR(1) models are estimated. Additionally,
in our unrestricted VAR models, $W_{t}$ includes bonds of five different
maturities (i.e. 1 year, 2 years, 3 years, 5 years, 10 years).

\noindent \textbf{Dynamic Nelson Siegel (DNS) Models:}

\noindent \textit{(Models in this section are summarized in Table 1, and
include: DNS(1), DNS(2), DNS(3), DNS(4), DNS(5), and DNS(6))}

The DNS model introduced by Li and Diebold (2006) is a dynamic version of
the term structure based upon Nelson and Siegel (1987), where the
cross-sectional movement of the term structure model is summarized by the dynamics
of three underlying latent factors interpreted as \textquotedblleft
level\textquotedblright , \textquotedblleft slope\textquotedblright , and
\textquotedblleft curvature\textquotedblright\ factors. We refer to the
three latent factors as \textquotedblleft Nelson-Siegel
factors\textquotedblright , and in our prediction experiments, both AR(1)
and VAR(1) DNS type models are specified in order to predict these factors
for subsequent use in the prediction of $y_{t+h}(\tau )$. For a detailed
discussion of yield curve modeling using the DNS models, see Diebold and
Rudebusch (2013). For detailed discussions comparing
arbitrage free dynamic latent factor models, arbitrage free DNS models, and DNS models, 
refer to Ang and Piazzesi (2003), Diebold, Rudebusch and Aruoba (2006), 
Christensen, Diebold, and Rudebusch (2011), Duffie (2011), and the references cited therein.
For a discussion of the usefulness of survey information in related term structure
modeling, see Altavilla, Giacomini, and Ragusa (2016). 

In the DNS model, estimates of the Nelson-Siegel factors are constructed at
each point in time by regressing \{1, $[\frac{1-\text{exp}(-\lambda _{t}\tau
)}{\lambda _{t}\tau }],$ $[\frac{1-\text{exp}(-\lambda _{t}\tau )}{\lambda
_{t}\tau }-$exp$(-\lambda _{t}\tau )]\}$ on $\mathbf{y}_{t}(\tau ),$ where $\lambda_t$ is a decay parameter (see below discussion). 
Namely, in a first step, the DNS model 
\begin{equation}
\mathbf{y}_{t}(\tau )=\beta _{1,t}+\beta _{2,t}[\frac{1-\text{exp}(-\lambda
_{t}\tau )}{\lambda _{t}\tau }]+\beta _{3,t}[\frac{1-\text{exp}(-\lambda
_{t}\tau )}{\lambda _{t}\tau }-\text{exp}(-\lambda _{t}\tau )]+\varepsilon
_{t},
\end{equation}%
is fitted at each point in time, $t,$ yielding sequences of estimates, $%
\widehat{\beta }_{1,t},$ $\widehat{\beta }_{2,t},$ and $\widehat{\beta }%
_{3,t}$, for $t=1,...,T.$ Note that in this step, 3 model variants are
considered. One variant defines: 
\begin{equation*}
\mathbf{y}_{t}^{10}(\tau )=[y_{t}(12)\enspace y_{t}(24)\enspace y_{t}(36)%
\enspace y_{t}(48)\enspace y_{t}(60)\enspace y_{t}(72)\enspace y_{t}(84)%
\enspace y_{t}(96)\enspace y_{t}(108)\enspace y_{t}(120)]^{\prime }.
\end{equation*}%
In a second variant, 
\begin{equation*}
\mathbf{y}_{t}^{6}(\tau )=[y_{t}(12)\enspace y_{t}(24)\enspace y_{t}(36)%
\enspace y_{t}(60)\enspace y_{t}(84)\enspace y_{t}(120)]^{\prime },
\end{equation*}%
and in a third variant%
\begin{equation*}
\mathbf{y}_{t}^{4}(\tau )=[y_{t}(12)\enspace y_{t}(36)\enspace %
y_{t}(60)y_{t}(120)]^{\prime }.
\end{equation*}

Predictions of $y_{t+h}$ are constructed using the model:%
\begin{equation}
y_{t+h}(\tau )=\widehat{\beta }_{1,t+h}^{f}+\widehat{\beta }_{2,t+h}^{f}[%
\frac{1-\text{exp}(-\lambda _{t}\tau )}{\lambda _{t}\tau }]+\widehat{\beta }%
_{3,t+h}^{f}[\frac{1-\text{exp}(-\lambda _{t}\tau )}{\lambda _{t}\tau }-%
\text{exp}(-\lambda _{t}\tau )],
\end{equation}%
where $y_{t+h}(\tau )$ is a scalar, and $\widehat{\beta }_{1,t+h}^{f}$, $%
\widehat{\beta }_{2,t+h}^{f}$, and $\widehat{\beta }_{3,t+h}^{f}$ and
predictions constructed by specifying simple AR or VAR models for $\widehat{%
\beta }_{1,t},$ $\widehat{\beta }_{2,t},$ and $\widehat{\beta }_{3,t},$
including: 
\begin{equation}
\hat{\beta}_{i,t+h}^{f}=\widehat{c}_{i}+\widehat{\gamma }_{ii}\widehat{\beta 
}_{i,t},\quad \text{for }i=1,2,3,  \label{bet}
\end{equation}%
where $\hat{\beta}_{i,t+h}^{f},$ $\widehat{\beta }_{i,t},$ $\widehat{c}_{i}$
and $\widehat{\gamma }_{ii}$ are scalars. We also construct predictions by
using the following VAR(1) model: 
\begin{equation}
\hat{\beta}_{t+h}^{f}=\widehat{c}+\widehat{\mathbf{\gamma }}\widehat{\beta }%
_{t},  \label{varbet}
\end{equation}%
where $\hat{\beta}_{t+h}^{f}=\left( \widehat{\beta }_{1,t+h}^{f},\widehat{%
\beta }_{2,t+h}^{f},\widehat{\beta }_{3,t+h}^{f}\right) ^{\prime }$, $%
\widehat{c}$ is 3x1 vector$,$ and $\widehat{\mathbf{\gamma }}=\left( 
\widehat{\gamma }_{1},\widehat{\gamma }_{2},\widehat{\gamma }_{3}\right) ,$
with $\widehat{\gamma }_{j}$ a 3x1 vector, for $j=1,2,3.$\ Note that the
loading on $\hat{\beta}_{1,t}$ is one, so it is often interpreted as the
\textquotedblleft level\textquotedblright\ factor. Also, $\hat{\beta}_{2,t}$
decreases as maturity increases, resulting in an increase in the
\textquotedblleft slope\textquotedblright\ of bond yield curve. Finally, $%
\hat{\beta}_{3,t}$ has initial loading zero, on the short end of yield
curve, and reaches its peak at around the 30 month maturity (when the
rate of decay, $\lambda _{t},$ is fixed to 0.0609, as discussed by Diebold
and Li (2006)), and gradually decays to
zero as the maturity goes to infinity. We set the decay parameter equal to 0.0609.
Since an increase in $\hat{\beta}%
_{3t} $ has a larger effect on medium-term yields than on short- and
long-term yields, it is often called a \textquotedblleft
curvature\textquotedblright\ factor. \newline

\noindent \textbf{DNS Models with Macroeconomic Variables:}

\noindent \textit{(Models in this section are summarized in Table 1, and
include: DNS(1)+MAC, DNS(2)+MAC, DNS(3)+MAC, DNS(4)+MAC, DNS(5)+MAC, and
DNS(6)+MAC)}

DNS models of the variety discussed above are also estimated, where latent
factor prediction models include macroeconomic variables. Namely, we consider predictions
constructed using:%
\begin{equation*}
\hat{\beta}_{i,t+h}^{f}=\widehat{c}_{i}+\widehat{\gamma }_{ii}\widehat{\beta 
}_{i,t}+\widehat{\alpha }_{i}^{\prime }M_{t},\quad \text{for \ }i=1,2,3,
\end{equation*}%
where $M_{t}$ includes selected key macroeconomic variables discussed in
Diebold and Li (2006), and $\widehat{\alpha }$ is a 3x1 vector. Here, $M_{t}$
includes manufacturing capacity utilization, the federal funds rate, and the
annual personal consumption expenditures price deflator. Analogous to the
VAR(1) model given in (\ref{varbet}), we additionally construct predictions
according to:%
\begin{equation*}
\hat{\beta}_{t+h}^{f}=\widehat{c}+\widehat{\mathbf{\gamma }}\widehat{\beta }%
_{i,t}+\widehat{\mathbf{\alpha }}M_{t},\quad \text{for \ }i=1,2,3,
\end{equation*}%
where $\widehat{\mathbf{\alpha }}=\left( \widehat{\alpha }_{1},\widehat{%
\alpha }_{2},\widehat{\alpha }_{3}\right) ,$ with $\widehat{\alpha }_{j}$ a
3x1 vector, for $i=1,2,3.$ \newline

\noindent \textbf{Diffusion Index Models:}

\noindent \textit{(Models in this section are summarized in Table 1, and
include: DIF(1), DIF(2), DIF(3))}

We construct predictions using the diffusion index model discussed
extensively above, where latent factors, $F_{t}^{s}$ are estimated using PCA
with a set of 10 yields given by $\mathbf{y}_{t}^{10}(\tau )$, 
\begin{equation}
y_{t+h}(\tau )= c + \beta ^{\prime }W_{t}+\alpha ^{\prime
}F_{t}^{s}+\varepsilon _{t+h},\quad  \label{smallF}
\end{equation}%
where $F_{t}^{s}$ includes either 1, 2, or 3 latent factors corresponding to
the largest eigenvalues of the eigenvalue/eigenvector decomposition of a
small (standardized) yield dataset consisting of our 10-dimensional yield
dataset, and $W_{t}$ includes only one lag of the yield. This simple model
is included in order to facilitate direct comparison with the DNS models
given in equations (\ref{bet}) and (\ref{varbet}). \newline

\noindent {\Large Big Data Models}

\noindent \textbf{Diffusion Index Models:}

\noindent \textit{(Models in this section are summarized in Table 1,
include: DIF(4), DIF(5), DIF(6), VAR(1)+FB1, VAR(1)+FB2, VAR(SIC)+FB1,
VAR(SIC)+FB2, DIF(1)+FB1, DIF(2)+FB1, DIF(3)+FB1, DIF(1)+FB2,
DIF(2)+FB2, DIF(3)+FB2)}

We utilize the prediction model given in equation (\ref{smallF}), but with
latent factors, say $F_{t}^{b},$ estimated using PCA with a set of 103
macroeconomic variables (see below data description for a discussion of the
variables used). In particular, we estimate variants of the following factor
augmented forecasting model: 
\begin{equation*}
y_{t+h}(\tau )= c + \beta ^{\prime }W_{t}+\alpha ^{\prime
}F_{t}^{b}+\varepsilon _{t+h},
\end{equation*}%
where setting $\beta =0$ yields \textquotedblleft pure\textquotedblright\
diffusion index models, and $W_{t}$ is defined as above, yielding AR and VAR
variants of these models. Inclusion of the lagged yield in $W_{t}$ allows
for direct comparison of our diffusion index models with our pure
econometric AR and VAR models discussed at the beginning of this section.
Here, $F_{t}^{b}$ includes either 1 or 2 latent factors, and $\alpha $
and $\beta $ are conformably defined vectors of coefficients. For a related
discussion of so-called unspanned macroeconomic factors in the yield curve,
see Bauer and de los Rios (2012) and Coroneo, Giannone and Modugno (2016).

Additionally, we construct predictions using diffusion index models of the
following variety: 
\begin{equation*}
y_{t+h}(\tau )= c + \beta ^{\prime }W_{t}+\alpha _{2}^{\prime
}F_{t}^{b}+\alpha _{2}^{\prime }F_{t}^{s}+\varepsilon _{t+h}.\quad
\end{equation*}%
Note that although multiple yield lags were tried when specifying $W_{t}$,
\textquotedblleft MSFE-best\textquotedblright\ models always included only
the first lag of the yield(s). For this reason all empirical results
discussed in the sequel use one lag. \newline

\noindent \textbf{DNS Models with Diffusion Indexes:}

\noindent \textit{(Models in this section are summarized in Table 1, and
include: DNS(1)+FB1, DNS(2)+FB1, DNS(3)+FB1, DNS(4)+FB1, DNS(5)+FB1,
DNS(6)+FB1, DNS(1)+FB2, DNS(2)+FB2, DNS(3)+FB2, DNS(4)+FB2, DNS(5)+FB2,
DNS(6)+FB2)}

The DNS model discussed above is augmented to include
diffusion indexes. Namely, we considered DNS type predictions constructed
using: 
\begin{equation*}
\hat{\beta}_{i,t+h}^{f}=\widehat{c}_{i}+\widehat{\gamma }_{i}\widehat{\beta }%
_{i,t}+\widehat{\alpha }^{\prime }F_{t}^{b},\quad \text{for \ }i=1,2,3,
\end{equation*}%
where $F_{t}^{b}$ again includes either 1, 2 or 3 latent factors, and so is
a scalar or a 3x1 vector. All other terms are conformably defined. Analogous
to our above discussion of DNS models, we also construct predictions by
using the following VAR(1) variant of this model: 
\begin{equation*}
\hat{\beta}_{t+h}^{f}=\widehat{c}+\widehat{\Gamma }\widehat{\beta }_{t}+%
\widehat{\Xi }F_{t}^{b},
\end{equation*}%
where $\hat{\beta}_{t+h}^{f}=\left( \widehat{\beta }_{1,t+h}^{f},\widehat{%
\beta }_{2,t+h}^{f},\widehat{\beta }_{3,t+h}^{f}\right) ^{\prime }$, $%
\widehat{c}$ is 3x1 vector$,$ and $\widehat{\Gamma }=\left( \widehat{\gamma }%
_{1},\widehat{\gamma }_{2},\widehat{\gamma }_{3}\right) ,$ $\widehat{\gamma }%
_{j}$ is a 3x1 vector, for $j=1,2,3,$ and $\widehat{\Xi }$ is a 3x1 vector
(if $F_{t}^{b}$ is a scalar), or is a 3x2 matrix (if $F_{t}^{b}$ is a 2x1
vector).\newline

\noindent {\Large Forecast Combination}

In our prediction experiments, we also construct and analyze a select set of 
forecast combinations. The particular combinations are detailed in Table
7. Although the focus of this paper is not forecast combination, there are
two reasons why we include at least a small set of combinations. First, it
is well known that forecast combination is useful
in time series prediction. More importantly, inclusion of combinations in our empirical illustration
serves to stress that an important area for future research involves combination
of classical econometric and machine learning methods.
Just as shown in Kim and Swanson (2014), Carrasco and Rossi (2016), and
Hirano and Wright (2017), much can be gained via combination not only of
forecasts, but also of methodologies.\footnote{%
For a discussion of forecast combination using the types of factor augmented
regressions discussed in this paper, see Cheng and Hansen (2015).}

\subsection{Data}

Our term structure data are U.S. zero-coupon (end of month) yield curve data
reported by the Federal Reserve Board (see \textit{%
https://www.quandl.com/data/FED/SVENY-US-Treasury-Zero-Coupon-Yield-Curve }%
and Gurkaynak, Sack and Wright (2006))\textit{. }In particular, we utilize
monthly data for the period\textit{\ }January 1982 through July 2016, for 1
through 10 year maturities. Hence, we analyze a panel of dataset containing $%
N=10$ variables and $T=415$ monthly observations. All yields are
standardized to mean zero unit variance series before principle component
analysis.

Macro factors are constructed using a balanced panel of 103 macroeconomic
variables obtained from the FRED-MD dataset recently developed by the
Federal Reserve Bank of St. Louis. A detailed explanation on how the data
set is collected and adjusted is given in McCracken and Ng (2016). FRED-MD
is maintained by FRED, is updated on a monthly basis, and can be accessed at

\textit{https://research.stlouisfed.org/econ/mccracken/fred-databases/}. Our
version of this dataset contains observations for the period January 1982
through July 2016.

\subsection{Empirical Findings}

Tables 2A - 2D contain relative MSFEs for yield forecasts constructed using
the models listed in Table 1, for $h=1$, for 1, 2, 3, 5, and 10 year
maturities, and for 4 different forecasting periods, including:
1992:3-1999:12 (Subsample 1), 2000:1-2007:12 (Subsample 2), 2008:1-2016:7
(Subsample 3), and 1992:1-2016:7 (Subsample 4). The benchmark model used in
the construction of relative MSFEs is the AR(1) forecasting model. Tabulated
entries denoted in bold are the lowest (relative) point-MSFEs, for each maturity.
Starred entries indicate rejection of the ($DM_{P}$ test) null hypothesis of
no difference between the benchmark and the alternative model listed in
column 1 of the tables, in favor of the alternative model.\footnote{%
*** entries indicate rejection at the 1\% level, while ** and * denote
rejection at the 5\% and 10\% levels, respectively.} Tables 3A-D and 4A-D
collect analogous results, but for $h=3$ and $h=12$, respectively.
Additionally, the \textquotedblleft MSFE-best\textquotedblright\ models for
each bond maturity, each forecast horizon, and each subsample (i.e., the
models denoted in bold in Tables 2A-4D) are given in Table 5; and Table 6 is
an analogous table, but with two alternative subsamples (i.e., expansionary
and recessionary periods). Finally, the results of forecast combination
experiments utilizing all of the models are summarized in Tables 7 and 8A-C.

Turning to the results based on Tables 2A through 4D, a number of clearcut
conclusions emerge.

First, inspection of the results in Tables 2A-2D indicates that for
Subsamples 1 and 2, the MSFE-best model is usually a DNS model with added
\textquotedblleft big data\textquotedblright\ diffusion indexes. Namely,
DNS+FB models usually \textquotedblleft win\textquotedblright . In
particular, for forecast horizons of 1- and 3-steps ahead, this is true in
17 of 20 maturity/horizon permutations, across Subsamples 1 and 2.
Interestingly, in the most recent subsample (i.e., Subsample 3), DNS+FB type
models instead \textquotedblleft win\textquotedblright\ in only 2 of 10
cases, for forecast horizons of 1- and 3-steps ahead. Thus, the post
Great-recession period appears to have \textquotedblleft
confused\textquotedblright\ our models. Nevertheless, when results based on
the entire prediction period (i.e., Subsample 4) are examined, it is
noteworthy that DNS models with added \textquotedblleft big
data\textquotedblright\ diffusion indexes still \textquotedblleft
win\textquotedblright\ in 7 of 10 cases, for $h=1$ and $3$. For our longest
forecast horizon (i.e., $h=12$), the evidence in favor of using
\textquotedblleft big data\textquotedblright\ is not so clearcut, as
baseline DNS models without diffusion indexes and straw-man AR and VAR
models almost always \textquotedblleft win\textquotedblright . 

Second, even cursory examination of Tables 2A-4D indicates that models
listed as MSFE-best in Table 5 are almost always significantly better than
our benchmark AR(1) model, based upon application of the $DM_{P}$ test.

Third, the DNS type models that \textquotedblleft win\textquotedblright\ in
our experiments are usually the vector variety (i.e., DNS(4), DNS(5) and
DNS(6)). This suggests that the factors in the DNS model do not evolve
independently of one another. Thus, not only can the factors (i.e., the
\textquotedblleft betas\textquotedblright ) be better predicted by utilizing
\textquotedblleft big data\textquotedblright\ diffusion indexes, as
discussed above, but they can also be better predicted by modeling their
cross-correlation dynamics.

We now turn to a discussion of the results in Tables 5-8.

In Table 5, where point ``MSFE-best'' models are listed by subsample and maturity, a number of further conclusions emerge.
In this table, entries superscripted with {\textsuperscript{***}, \textsuperscript{**}, 
and \textsuperscript{*} in Table 5 denote rejections of the null hypothesis of equal predictive accuracy at 0.01, 0.05, and 0.10 significance levels, respectively, 	based on application of the Diebold-Mariano test discussed in Section 3; and indicate that the listed model is predictively superior to a 
``benchmark'' DNS($\tau$) model, based on MSFE loss. In particular, if the point ``MSFE-best'' model is 
DNS($\tau$)+$mod$, where $mod$ denotes another component of the model (for example, $mod$ may be FB1 or FB2, etc.) 
then the ``benchmark'' model is DNS($\tau$). If the point ``MSFE-best'' model is 
DNS(1), or if no DNS component appears in point ``MSFE-best'' model, then DNS(1) is the ``benchmark'' model.
Finally, for entries denoted ``DNS(1)'', no predictive accuracy test was carried out.
These test results are included to highlight the importance 	
of incorporating \textquotedblleft big data\textquotedblright\ in DNS type
prediction models. Turning to the results of these tests, note that for forecast horizons of 1- and 3-steps ahead, DNS($\tau$)+FB models significantly 	
outperform their DNS($\tau$) counterparts in almost all cases, across Subsamples 1 and 2. 	
In Subsample 3 (2008:1-2016:7), the evidence is more mixed, with ``less to choose'' between the alternative models in our experiments.	
Additionally, and as discussed above, our ``straw-man'' models perform well at the 12-step ahead forecast horizon.	

Needless to say, there are instances where AR type models outperform our more complex models. The reasons for this may be many.
For example, structural breaks may play an important role that is not captured by any of our specifications, 
leading to cases where the ``simplest'' approximations (e.g., AR and VAR models) dominate, from the perspective of predictive accuracy. 
Of course, this does not preclude the possibility that more complex models than ours may outperform (V)AR models in such cases. 
Additionally, note that (V)AR models perform better at longer horizons, which is not surprising, and is a well know stylized fact 
in empirical economics; again probably stemming from issues pertaining to the approximate nature of our models, and the ability 	of the most parsimonious models to dominate under increased uncertainty, due to model specification and parameter uncertainty issues.

In Table 6, we see that the evidence in favor of DNS+FB type models is both stronger and
weaker when our prediction periods are broken into two alternative
subsamples defined as \textquotedblleft expansionary\textquotedblright\ and
\textquotedblleft recessionary\textquotedblright , based upon application of
NBER dating. In particular, in recessionary times, DNS+FB models win in 13
of 15 maturity/horizon permutations, including maturities of 1, 3, 5, and 10
years and horizons of $h=1,3,$ and 12 months ahead. Thus, in
recessionary times our DNS+FB models even \textquotedblleft
win\textquotedblright\ for $h=12$, which was not the case based upon our
earlier analysis of Subsamples 1-4. On the other hand, in expansionary
times, DNS+FB models win in only 7 of 15 maturity/horizon permutations, and
none of these wins occur when $h=12$.

Finally, Table 7 lists a small number of different forecast
combinations that were utilized in order to construct alternative
prediction models to compare with those discussed above. The \textquotedblleft MSFE-best\textquotedblright\ combination
models are usually preferred to the AR(1) benchmark, based on application of
the $DM_{P}$ test, as might be expected, given our above discussion.
However, it is noteworthy that point MSFEs associated with the best
combination models are usually higher than point MSFEs associated with out
best individual models. Indeed, combination models fail to \textquotedblleft
win\textquotedblright\ in 15 of 20 cases, for $h=1$, Subsamples 1-4, and
across all 5 bond maturities (see Table 8A). For $h=3$, the case against
forecast combination is even stronger, with combination models failing to
\textquotedblleft win\textquotedblright\ in 18 of 20 cases, for Subsamples
1-4 and across all 5 bond maturities (see Table 8B). Similarly, for $h=12,$
combination models fail to \textquotedblleft win\textquotedblright\ in 17 of
20 cases (see Table 8C). Evidently, a richer set of combination models needs
to be entertained if the usual result that combination works is to be found.
Examination of this is left to future research.

\section{Concluding Remarks}

This paper discusses recent advances in the analysis of big data using latent factor type dimension reduction methods as well as various other machine learning and shrinkage approaches. It is suggested that much remains to be learned regarding the ways in which extant econometric methods can be combined with dimension reduction methods in order to achieve improvements in prediction. We show how readily standard econometric models can be augmented to include predictive error reducing information from big datasets, in an illustration in which the term structure of
interest rates is predicted. Finally, we address predictive accuracy testing in the context of big data, and outline new loss function free methods that may be useful for forecast accuracy and model selection assessment.

\pagebreak

\section{References}

\noindent Altavilla, C., R. Giacomini and G. Ragusa (2016), Anchoring the
Yield Curve Using Survey Expectations, Working Paper, University College
London.

\noindent Ang, A. and M. Piazzesi (2003): A No-Arbitrage Vector
Autoregression of Term Structure Dynamics With Macroeconomic and Latent
Variables, \textit{Journal of Monetary Economics} 50, 745--787.

\noindent Armah, N.A. and N.R. Swanson (2010a), Seeing Inside the Black Box:
Using Diffusion Index Methodology to Construct Factor Proxies in Largescale
Macroeconomic Time Series Environments, \textit{Econometric Reviews} 29,
476-510.\smallskip 

\noindent Armah, N.A. and N.R. Swanson (2010b), Diffusion Index Models and
Index Proxies: Recent Results and New Directions, \textit{European Journal
of Pure and Applied Mathematics} 3, 2010, 478-501.

\noindent Bai, J. (2003), Inferential Theory for Factor Models of Large
Dimensions, \textit{Econometrica} 71, 135-171.\smallskip

\noindent Bai, J. and S. Ng (2002), Determining the Number of Factors in
Approximate Factor Models, \textit{Econometrica} 70, 191-221.\smallskip

\noindent Bai, J. and S. Ng (2006a), Confidence Intervals for Diffusion
Index Forecasts and Inference for Factor-Augmented Regressions, \textit{%
Econometrica} 74, 1133-1150.\medskip

\noindent Bai, J. and S. Ng (2006b), Evaluating Latent and Observed Factors
in Macroeconomics and Finance, \textit{Journal of Econometrics} 113,
507-537.\medskip

\noindent Bai, J. and S. Ng (2007), Determining the Number of Primitive
Shocks in Factor Models,\ \textit{Journal of Business and Economic Statistics%
} 25, 52-60.\medskip

\noindent Bai, J. and S. Ng (2008), Forecasting Economic Time Series Using
Targeted Predictors, \textit{Journal of Econometrics} 146, 304-317.\medskip

\noindent Bai, J. and S. Ng (2009), Boosting Diffusion Indices,\ \textit{%
Journal of Applied Econometrics} 24, 607-629.\medskip

\noindent Banerjee, A., M. Marcellino and I. Marsten (2008), Forecasting
Macroeconomic Variables Using Diffusion Indexes in Short Samples with
Structural Changes, in D. Rapach and M. Wohar (eds.), \textbf{Forecasting in
the Presence of Structural Breaks and Model Uncertainty}\textit{,} Emerald
Group Publishing, London.\smallskip

\noindent Banerjee, A., M. Marcellino and I. Marsten (2014), Forecasting
with Factor-Augmented Error Correction Models, \textit{International Journal
of Forecasting} 30, 589-612.\medskip

\noindent Bauer, G. and A.D. de los Rios (2012), An International Dynamic
Term Structure Model with Economic Restrictions and Unspanned Risks, Working
Paper \#2012-5, Bank of Canada.

\noindent Boivin, J. and S. Ng (2005), Understanding and Comparing Factor
Based Macroeconomic Forecasts, \textit{International Journal of Central
Banking} 1, 117-152.\medskip

\noindent Breiman, L. (1995), Better Subset Regression Using the Nonnegative
Garrote, \textit{Technometrics }37, 373--384.

\noindent Breiman, L. (1996), Bagging Predictors, \textit{Machine Learning }%
24, 123--140.

\noindent Breitung, J. and S. Eickmeier (2011), Testing for Structural
Breaks in Dynamic Factor Models, \textit{Journal of Econometrics }163,
71-84.\smallskip

\noindent Carrasco, M. and B. Rossi (2016), In-Sample Inference and
Forecasting in Misspecified Factor Models, \textit{Journal of Business and
Economic Statistics} 34, 313-338.

\noindent Castle, J.A., M.P. Clements and D.F. Hendry (2013), Forecasting by
Factors, by Variables or Both?, \textit{Journal of Econometrics }177,
305-319.\smallskip

\noindent Chao, J.C., V. Corradi and \ N.R. Swanson (2001), An out of Sample
Test for Granger Causality, \textit{Macroeconomic Dynamics }5, 598-620.

\noindent Chen, L., J.J. Dolado and J. Gonzalo, (2014), Detecting Big
Structural Breaks in Large Factor Models, \textit{Journal of Econometrics }%
180, 30-48.\smallskip

\noindent Chen, Y.-P., H.-C. Huang and I.-P. Tu (2010), A New Approach for
Selecting the Number of Factors, \textit{Computational Statistics and Data
Analysis }54, 2990-2998.

\noindent Cheng, X. and B.E. Hansen (2015), Forecasting with Factor
Augmented Regression: A Frequentist Model Averaging Approach, \textit{%
Journal of Econometrics} 186, 280-293.

\noindent Cheng, M., N.R. Swanson and X. Yang (2017), Latent Common Return Volatility Factors: Capturing Elusive Predictive Accuracy Gains When Forecasting Volatility, Working Paper, Rutgers University.

\noindent Carrasco, M., V. Chernozhukov, S. Goncalves and E. Renault (2015), 
\textbf{High Dimensional Problems in Econometrics}, \textit{Special Issue}, 
\textit{Journal of Econometrics} 186, 277-476.

\noindent Christensen, J.H.E., F.X. Diebold, and G.D. Rudebusch (2011), 
The Affine Arbitrage Free Class of Nelson-Siegel Term Structure Models, 
\textit{Journal of Econometrics} 164, 4-20.

\noindent Christoffersen, P. (1998), Evaluating Interval Forecasts, \textit{%
International Economic Review} 39, 841-862.

\noindent Clark, T.E. and M.W. McCracken (2001), Tests of Equal Forecast
Accuracy and Encompassing for Nested Models, \textit{Journal of Econometrics}
105, 85-110.

\noindent Clark, T. E. and M.W. McCracken (2013), Advances in Forecast
Evaluation, in G. Elliott and A. Timmermann (eds.), \textbf{Handbook of
Economic Forecasting}, Volume 2, Elsevier, Amsterdam.

\noindent Clements, M.P. and D.F. Hendry (1993), On the Limitations of
Comparing Mean Square Forecast Errors,\textit{\ Journal of Forecasting} 12,
617-637.

\noindent Clements, M.P. and D.F. Hendry (1999), On Winning Forecasting
Competitions in Economics, \textit{Spanish Economic Review} 1, 123-160.

\noindent Clements, M.P. and D.F. Hendry (2001), Forecasting with Difference
and Trend Stationary Models, \textit{Econometrics Journal} 4, S1--S19.

\noindent Coroneo, L., D. Giannone and M. Modugno (2016), Unspanned
Macroeconomic Factors in the Yield Curve, \textit{Journal of Business and
Economic Statistics} 34, 472-485.

\noindent Corradi, V., S. Jin, and N.R. Swanson (2017), Robust Forecast
Comparison, \textit{Econometric Theory}, forthcoming.

\noindent Corradi, V. and N.R. Swanson (2002), A Consistent Test for
Nonlinear out of Sample Predictive Accuracy, \textit{Journal of Econometrics
110}, 353-381.

\noindent Corradi, V. and N.R. Swanson (2005), A Test for Comparing Multiple
Misspecified Conditional Interval Models, \textit{Econometric Theory} 21,
991-1016.

\noindent Corradi, V. and N.R. Swanson (2006a), Predictive Density and
Conditional Confidence Interval Accuracy Tests, \textit{Journal of
Econometrics} 135, 187-228.

\noindent Corradi, V. and N.R. Swanson (2006b), Predictive Density
Evaluation, in G. Elliot, C. W. J. Granger, and A. Timmermann, (eds.), 
\textbf{Handbook of Economic Forecasting}\textit{,} Volume 1, Elsevier,
Amsterdam.

\noindent Corradi, V. and N.R. Swanson (2007), Nonparametric Bootstrap
Procedures For Predictive Inference Based On Recursive Estimation Schemes, 
\textit{International Economic Review} 48, 67--109.

\noindent Corradi, V. and W. Distaso (2011), Multiple Forecast Evaluation,
in D. F. Hendry and M. P. Clements (eds.), \textbf{Oxford Handbook of
Economic Forecasting}, Oxford University Press, Oxford.

\noindent Corradi, V. and N.R. Swanson (2013), A Survey of Recent Advances
in Forecast Accuracy Comparison Testing, with an Extension to Stochastic
Dominance, in X. Chen and N.R. Swanson (eds.), \textbf{Causality,
Prediction, and Specification Analysis: Recent Advances and Future
Directions, Essays in honor of Halbert L. White, Jr.}, Springer.

\noindent Corradi, V. and N.R. Swanson (2014), Testing for Structural
Stability of Factor Augmented Forecasting Models, \textit{Journal of
Econometrics} 182, 2014, 100-118.

\noindent Corradi, V. , N.R. Swanson\ and C. Olivetti (2001), Predictive
Ability with Cointegrated Variables, \textit{Journal of Econometrics} 104,
315-358.

\noindent Diebold, F.X. and C. Li (2006), Forecasting the Term Structure of
Government Bond Yields, \textit{Journal of Econometrics }130, 337-364.

\noindent Diebold, F.X. and R.S. Mariano (1995), Comparing Predictive
Accuracy, \textit{Journal of Business and Economic Statistics} 13, 253-263.

\noindent Diebold, F.X. and G.D. Rudebusch (2013), \textbf{Yield Curve
Modeling and Forecasting: The Dynamic Nelson-Siegel Approach}, Princeton
University Press: Princeton.

\noindent Diebold, F. X., G. D. Rudebusch, and S. B. Aruoba (2006), The
Macroeconomy and the Yield Curve: A Dynamic Latent Factor Approach, \textit{%
Journal of Econometrics} 131, 309--338.

\noindent Diebold, F.X. and M. Shin (2015), Assessing Point Forecast
Accuracy by Stochastic Loss Distance, \textit{Economics Letters} 130, 37-38.

\noindent Ding, A. and J. Hwang (1999), Prediction Intervals, Factor
Analysis Models, and High-Dimensional Empirical Linear Prediction, \textit{%
Journal of the American Statistical Association} 94, 446-455.

\noindent Duffie, G. R. (2011): Information In (and Not In) the Term
Structure, \textit{Review of Financial Studies} 24, 2895--2934.

\noindent Dufour, J.-M. and D. Stevanovic (2013), Factor Augmented VARMA
Models: Identification, Estimation, Forecasting and Impulse Responses, 
\textit{Journal of Business and Economic Statistics} 31, 491-506.\medskip

\noindent Efron, B., T. Hastie, L. Johnstone and R. Tibshirani (2004), Least
Angle Regression, \textit{Annals of Statistics} 32, 407-499.

\noindent Elliott, G. and A. Timmermann (2004), Optimal Forecast
Combinations Under General Loss Functions and Forecast Error Distributions, 
\textit{Journal of Econometrics} 122, 47--79.

\noindent Fan, J., Y. Liao, and M. Mincheva (2011), High Dimensional
Covariance Matrix Estimation in Approximate Factor Models, \textit{Annals of
Statistics} 39, 3320-3356.

\noindent Fan, J., Y. Liao, and W. Wang (2016), Projected Principal
Components Analysis in Factor Models, \textit{Annals of Statistics} 44,
219-254.

\noindent Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2000), The
Generalized Dynamic Factor Model: Identification and Estimation, \textit{The
Review of Economics and Statistics} 82, 540-552.\medskip

\noindent Forni, M., M. Hallin, M. Lippi, and L. Reichlin (2005), The
Generalized Dynamic Factor Model, One Sided Estimation and Forecasting, 
\textit{Journal of the American Statistical Association} 100, 830--840.

\noindent Forni, M., M. Hallin, M. Lippi, L. Reichlin and P. Zaffaroni
(2015), Dynamic Factor Models with Infinite Dimensional Factor Spaces: One
Sided Representations, \textit{Journal of the Econometrics} 185, 359-371.

\noindent Freund, Y. and R.E. Schapire (1997), A Decision-Theoretic
Generalization of On-Line Learning and an Application to Boosting, \textit{%
Journal of Computer and System Sciences }55, 119--139.

\noindent Ghysels, E., J.B. Hill and K. Motegi (2017), Testing a Large Set
of Zero Restrictions in Regression Models, with an Application to Mixed
Frequency Granger Causality, Working Paper, University of North Carolina,
Chapel Hill.

\noindent Giacomini, R. and H. White (2006), Tests of Conditional Predictive
Ability, \textit{Econometrica} 74, 1545-1578.

\noindent Giacomini R. and B. Rossi (2009), Detecting and Predicting
Forecast Breakdowns, \textit{Review of Economic Studies }76,
669-705.\smallskip

\noindent Goncalves S. and B. Perron (2014), Bootstrapping Factor-Augmented
Regression Models, \textit{Journal of Econometrics }182, 156-173.\smallskip

\noindent Goncalves S. and H. White (2004), Maximum Likelihood and the
Bootstrap for Nonlinear Dynamic Models, \textit{Journal of Econometrics }%
119, 199-219.\medskip

\noindent Granger, C.W.J. (1999a), Outline of Forecast Theory using
Generalized Cost Functions, \textit{Spanish Economic Review} 1, 161-173

\noindent Granger, C.W.J. (1999b), \textbf{Empirical Modeling in Economics:
Specification and Evaluation}, Cambridge University Press, New York.

\noindent Granger, C.W.J. and \ M. H. Pesaran (2000), Economic and
Statistical Measures of Forecast Accuracy, \textit{Journal of Forecasting}
19, 537-560.

\noindent Gurkaynak, R.S., B. Sack, and J.H. Wright (2006), The U.S.
Treasury Yield Curve: 1961 to the Present, Finance and Economics Discussion
Series \#2006-28.

\noindent Han, X. and A. Inoue (2014), Tests for Parameter Instability in
Dynamic Factor Models, \textit{Econometric Theory} 31, 1117-1152.

\noindent Hansen, P.R. and A. Timmermann (2012), Choice of Sample Split in
Out-of-Sample Forecast Evaluation. Working Paper, UCSD, Rady School of
Management.\medskip

\noindent Hendry D.F. and M.P. Clements (2002), Pooling of Forecasts, 
\textit{Econometrics Journal }5, 1-26.\smallskip

\noindent Hendry D.F. and G. Mizon (2005), Forecasting in the Presence of
Structural Breaks and Policy Regime Shifts, in D.W.K. Andrews and J.H. Stock
(eds.) \textbf{Identification and Inference for Econometric Models: Essays
in Honour of Thomas Rothemberg}, Cambridge University Press.\smallskip

\noindent Hirano, K. and J.H. Wright (2017), Forecasting With Model
Uncertainty: Representations and Risk Reduction, \textit{Econometrica }85,
617-643.

\noindent Hornik, K., M. Stinchcombe, and H. White (1989), Multilayer
Feedforward Networks are Universal Approximators, \textit{Neural Networks}
2, 359-366.

\noindent Hyv\"{a}rinen, A. (1998), Independent Component Analysis in the
Presence of Gaussian Noise by Maximizing Joint Likelihood, \textit{%
Neurocomputing} 22, 49-67.

\noindent Hyv\"{a}rinen, A. (1999), Survey on Independent Component
Analysis, \textit{Neural Computing Surveys} 2, 94-128.

\noindent Hyv\"{a}rinen, A. and E. Oja (2000), Independent Component
Analysis: Algorithms and Applications, \textit{Neural Networks }13, 411-430.

\noindent Kim H.H. and N.R. Swanson (2014), Forecasting Financial and
Macroeconomic Variables Using Data Reduction Methods: New Empirical
Evidence, \textit{Journal of Econometrics} 178, 352-367.\smallskip

\noindent Kim H.H. and N.R. Swanson (2017), Mining Big Data Using
Parsimonious Factor, Machine Learning, Variable Selection, and Shrinkage
Methods, \textit{International Journal of Forecasting,} forthcoming.%
\smallskip

\noindent McCracken, M.W. (2000), Robust Out-of-Sample Inference, \textit{%
Journal of Econometrics} 99, 195-223.

\noindent McCracken, M.W. and S. Ng (2016), Fred-MD: A Monthly Database for
Macroeconomic Research, \textit{Journal of Business \& Economic Statistics}
34, 574-589.

\noindent Nelson, C.R. and A.F. Siegel (1987), Parsimonious Modeling of
Yield Curves, \textit{Journal of Business} 60, 473-489.

\noindent Onatski, A. (2015), Asymptotic Analysis of the Squared Estimation
Error in Misspecified Factor Models, \textit{Journal of Econometrics} 186,
388-406.

\noindent Rossi, B. and A. Inoue (2012), Out of Sample Forecast Tests Robust
to Window Size Choice, \textit{Journal of Business and Economic Statistics }%
30, 432-453.\smallskip

\noindent Schumacher, C. (2007), Forecasting German GDP Using Alternative
Factor Models Based on Large Datasets, \textit{Journal of Forecasting} 26,
271-302.

\noindent Schumacher, C. (2009), Factor Forecasting Using International
Targeted Predictors: The Case of German GDP, \textit{Economics Letters} 107,
95-98.

\noindent Spearman, C. (1904), General Intelligence Objectively Determined
and Measured, \textit{American Journal of Psychology} 15, 201-293.

\noindent Stock, J. and M.W. Watson (1998),\ Diffusion Indexes. \textit{%
Working Paper} 6702, National Bureau of Economic Research.\medskip

\noindent Stock, J.H. and M.W. Watson (2002a), Macroeconomic Forecasting
Using Diffusion Indexes, \textit{Journal of Business and Economic Statistics 
}20, 147-162.\smallskip

\noindent Stock, J.H. and M.W. Watson (2002b), Forecasting Using Principal
Components from a Large Number of Predictors, \textit{Journal of the
American Statistical Association }97, 1167-1179.\smallskip

\noindent Stock, J.H. and M.W. Watson (2004), Combination Forecasts of
Output Growth in a Seven-Countries Data-Set, \textit{Journal of Forecasting }%
23, 405-430.\smallskip

\noindent Stock, J.H. and M.W. Watson (2009), Forecasting in Dynamic Factor
Models Subject to Structural Instability, in J. Castle and N. Shephard
(eds.), \textbf{The Methodology and Practice of Econometrics: Festschrift in
Honour of D.F. Hendry}, Oxford University Press.\smallskip

\noindent Stock, J. H. and M.W. Watson (2012), Generalized Shrinkage Methods
for Forecasting Using Many Predictors, \textit{Journal of Business and
Economic Statistics} 30, 481-493.

\noindent Stone, J.V. (2004), \textbf{Independent Component Analysis}, MIT
Press, Boston.

\noindent Sullivan, R., A. Timmermann and H. White (1999), Data-snooping,
Technical Trading Rule Performance, and the Bootstrap, \textit{Journal of
Finance }54, 1647-1691.

\noindent Swanson, N.R. (2016), Comment on: In Sample Inference and
Forecasting in Misspecified Factor Models, \textit{Journal of Business and
Economic Statistics} 34, 348-353.

\noindent Tibshirani, R. (1996), Regression Shrinkage and Selection via the
Lasso, \textit{Journal of the Royal Statistical Society, Series B} 58,
267--288.

\noindent Timmermann, A. (2006), Forecast Combinations, in G. Elliot, C. W.
J. Granger, and A. Timmermann, (eds.), \textbf{Handbook of Economic
Forecasting}\textit{,} Elsevier, Amsterdam.\noindent

\noindent Tong, L., R.-W. Liu, V.C. Soon, and Y.-F. Huan (1991),
Indeterminacy and Identifiability of Blind Identification, \textit{IEEE
Transactions on Circuits and Systems }38, 499-509.

\noindent West, \ K. D. (1996), Asymptotic Inference about Predictive
Ability, \textit{Econometrica} 64, 1067-1084.

\noindent West, K. D. and M. W. McCracken (1998), Regression Based Tests of
Predictive Ability, \textit{International Economic Review} 39, 817-840.

\noindent West, K. D. (2006), Forecast Evaluation, in G. Elliot, C. W. J.
Granger, and A. Timmermann, (eds.), \textbf{Handbook of Economic Forecasting}%
\textit{,} Elsevier, Amsterdam.

\noindent White, H. (2000), A Reality Check for Data Snooping, \textit{%
Econometrica} 68, 1097-1126.

\noindent Yuan, M. and Y. Lin (2007), On the Non-Negative Garrotte
Estimator, \textit{Journal of the Royal Statistical Society} 69, 143-161.

\noindent Zou, H. (2006), The Adaptive Lasso and Its Oracle Properties, 
\textit{Journal of the American Statistical Association }101, 1418-1429.

\noindent Zou, H. and T. Hastie (2005), Regularization and Variable
Selection via the Elastic Net, \textit{Journal Of The Royal Statistical
Society Series B }67, 301-320.

\noindent Zou, H., T. Hastie, and R. Tibshirani (2006), Sparse Principal
Component Analysis, \textit{Journal of Computational and Graphical Statistics%
} 15, 262-286.

\newpage

\begin{center}
Table 1: Models Used in Forecast Experiments\textnormal{\superscript{*}}

\vspace{0.3cm} 
\begin{adjustbox}{totalheight=0.85\textheight-2\baselineskip}
\begin{tabular}{ll}
\hline \hline
\multicolumn{1}{c}{Model}&		\multicolumn{1}{c}{Description} 	\\\hline
AR(1)         &     Autoregressive model with one lag\\
VAR(1)        &     Five-dimensional vector autoregressive model with one lag\\
VAR(1)+FB1    &     VAR(1) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
VAR(1)+FB2    &     VAR(1) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
AR(SIC)       &     Autoregressive model with lag(s) selected by the Schwarz information criterion\\
VAR(SIC)	  &     Five-dimensional vector autoregressive model with lag(s) selected by the Schwarz information criterion\\
VAR(SIC)+FB1  &     VAR(SIC) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
VAR(SIC)+FB2  &     VAR(SIC) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(1)        &     Dynamic Nelson-Siegel (DNS) model with underlying AR(1) factor specifications fitted with ten-dimensional yields: maturity $\tau = 12, 24, 36, 48, 60, 72, 84, 96, 108, 120$ months\\
DNS(2)        &     DNS model with underlying AR(1) factor specifications fitted with six-dimensional yields: maturity $\tau = 12, 24, 36, 60, 84, 120$ months\\
DNS(3)        &     DNS model with underlying AR(1) factor specifications fitted with four-dimensional yields: maturity $\tau = 12, 36, 60, 120$ months\\
DNS(4)        &     DNS model with underlying VAR(1) factor specifications fitted with ten-dimensional yields: maturity $\tau = 12, 24, 36, 48, 60, 72, 84, 96, 108, 120$ months\\
DNS(5)        &     DNS model with underlying VAR(1) factor specifications fitted with six-dimensional yields: maturity $\tau = 12, 24, 36, 60, 84, 120$ months\\
DNS(6)        &     DNS model with underlying VAR(1) factor specifications fitted with four-dimensional yields: maturity $\tau = 12, 36, 60, 120$ months\\
DNS(1)+FB1    &     DNS(1) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(2)+FB1    &     DNS(2) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(3)+FB1    &     DNS(3) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(4)+FB1    &     DNS(4) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(5)+FB1    &     DNS(5) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(6)+FB1    &     DNS(6) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DNS(1)+FB2    &     DNS(1) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(2)+FB2    &     DNS(2) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(3)+FB2    &     DNS(3) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(4)+FB2    &     DNS(4) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(5)+FB2    &     DNS(5) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(6)+FB2    &     DNS(6) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DNS(1)+MAC    &     DNS(1) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DNS(2)+MAC    &     DNS(2) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DNS(3)+MAC    &     DNS(3) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DNS(4)+MAC    &     DNS(4) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DNS(5)+MAC    &     DNS(5) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DNS(6)+MAC    &     DNS(6) model with three key macroeconomic variables added: manufacturing capacity utilization, the federal funds rate, and annual price inflation\\
DIF(1)        &     Diffusion index model with one principle component estimator based on all ten-dimensional yields\\
DIF(2)        &     Diffusion index model with two principle component estimators based on all ten-dimensional yields\\
DIF(3)        &     Diffusion index model with three principle component estimators based on all ten-dimensional yields\\
DIF(4)        &     Diffusion index model with one principle component estimator based on all 103 macroeconomic variables \\
DIF(5)        &     Diffusion index model with two principle component estimators based on all 103 macroeconomic variables\\
DIF(6)        &     Diffusion index model with three principle component estimators based on all 103 macroeconomic variables\\
DIF(1)+FB1    &     DIF(1) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DIF(2)+FB1    &     DIF(2) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DIF(3)+FB1    &     DIF(3) model with one principle component added, principle component analysis based on all 103 macroeconomic variables\\
DIF(1)+FB2    &     DIF(1) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DIF(2)+FB2    &     DIF(2) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\
DIF(3)+FB2    &     DIF(3) model with two principle components added, principle component analysis based on all 103 macroeconomic variables\\\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{1\columnwidth}

\hspace{0.1in}

\scriptsize
\textnormal{\superscript{*}} Notes: This table summarizes the models utilized in all forecasting experiments.
\end{minipage}

\newpage

Table 2A: 1-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
1: 1992:3-1999:12)\textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{*}}
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	1.099	&	1.108	&	1.103	&	1.098	&	1.141	\\
VAR(1)+FB1	&	0.819\textnormal{\superscript{**}}	&	0.868\textnormal{\superscript{*}}	&	0.893\textnormal{\superscript{*}}	&	0.927	&	1.045	\\
VAR(1)+FB2	&	0.844	&	0.874	&	0.897	&	0.940	&	1.106	\\
AR(SIC)	&	0.864\textnormal{\superscript{**}}	&	0.942\textnormal{\superscript{*}}	&	0.958	&	0.974	&	\bfseries 0.972\textnormal{\superscript{**}}	\\
VAR(SIC)	&	1.099	&	1.108	&	1.103	&	1.098	&	1.141	\\
VAR(SIC)+FB1	&	0.819\textnormal{\superscript{**}}	&	0.868\textnormal{\superscript{*}}	&	0.893\textnormal{\superscript{*}}	&	0.927	&	1.045	\\
VAR(SIC)+FB2	&	0.844	&	0.874	&	0.897	&	0.940	&	1.106	\\
DNS(1)	&	1.032	&	1.097	&	1.061	&	1.039	&	1.067	\\
DNS(2)	&	1.036	&	1.088	&	1.053	&	1.046	&	1.064	\\
DNS(3)	&	1.040	&	1.123	&	1.066	&	1.045	&	1.037	\\
DNS(4)	&	1.088	&	1.160	&	1.104	&	1.070	&	1.102	\\
DNS(5)	&	1.095	&	1.147	&	1.095	&	1.081	&	1.098	\\
DNS(6)	&	1.094	&	1.190	&	1.107	&	1.065	&	1.071	\\
DNS(1)+FB1	&	0.900	&	0.862\textnormal{\superscript{*}}	&	0.895	&	0.981	&	0.981	\\
DNS(2)+FB1	&	0.891	&	0.865\textnormal{\superscript{*}}	&	0.903	&	1.000	&	0.980	\\
DNS(3)+FB1	&	0.876	&	0.868\textnormal{\superscript{*}}	&	0.896	&	1.006	&	0.990	\\
DNS(4)+FB1	&	0.784\textnormal{\superscript{**}}	&	0.861\textnormal{\superscript{**}}	&	0.870\textnormal{\superscript{**}}	&	0.922	&	0.990	\\
DNS(5)+FB1	&	0.785\textnormal{\superscript{**}}	&	0.854\textnormal{\superscript{**}}	&	0.867\textnormal{\superscript{**}}	&	0.934	&	0.987	\\
DNS(6)+FB1	&	0.775\textnormal{\superscript{***}}	&	0.882\textnormal{\superscript{**}}	&	0.872\textnormal{\superscript{**}}	&	0.930	&	0.985	\\
DNS(1)+FB2	&	0.960	&	0.908	&	0.948	&	1.053	&	1.053	\\
DNS(2)+FB2	&	0.948	&	0.911	&	0.957	&	1.074	&	1.051	\\
DNS(3)+FB2	&	0.933	&	0.911	&	0.948	&	1.081	&	1.073	\\
DNS(4)+FB2	&	0.789\textnormal{\superscript{**}}	&	0.844\textnormal{\superscript{**}}	&	0.858\textnormal{\superscript{**}}	&	0.920	&	0.988	\\
DNS(5)+FB2	&	0.790\textnormal{\superscript{**}}	&	\bfseries 0.840\textnormal{\superscript{**}}	&	\bfseries 0.857\textnormal{\superscript{**}}	&	0.934	&	0.985	\\
DNS(6)+FB2	&	\bfseries 0.775\textnormal{\superscript{**}}	&	0.863\textnormal{\superscript{**}}	&	0.860\textnormal{\superscript{**}}	&	0.929	&	0.987	\\
DNS(1)+MAC	&	1.028	&	1.099	&	1.073	&	1.056	&	1.095	\\
DNS(2)+MAC	&	1.029	&	1.089	&	1.065	&	1.063	&	1.091	\\
DNS(3)+MAC	&	1.032	&	1.123	&	1.079	&	1.062	&	1.063	\\
DNS(4)+MAC	&	1.132	&	1.147	&	1.129	&	1.154	&	1.191	\\
DNS(5)+MAC	&	1.130	&	1.140	&	1.125	&	1.164	&	1.184	\\
DNS(6)+MAC	&	1.119	&	1.165	&	1.130	&	1.161	&	1.188	\\
DIF(1)	&	3.048	&	2.655	&	1.926	&	\bfseries 0.919\textnormal{\superscript{**}}	&	2.245	\\
DIF(2)	&	1.274	&	1.067	&	1.038	&	1.029	&	1.199	\\
DIF(3)	&	0.973	&	1.046	&	1.044	&	1.049	&	1.128	\\
DIF(4)	&	2.238	&	2.303	&	2.337	&	2.382	&	2.438	\\
DIF(5)	&	2.253	&	2.338	&	2.386	&	2.455	&	2.588	\\
DIF(6)	&	2.236	&	2.320	&	2.359	&	2.410	&	2.514	\\
DIF(1)+FB1	&	2.208	&	2.182	&	1.717	&	0.950	&	2.239	\\
DIF(2)+FB1	&	1.340	&	1.074	&	1.026	&	1.039	&	1.254	\\
DIF(3)+FB1	&	0.958	&	1.006	&	1.021	&	1.060	&	1.164	\\
DIF(1)+FB2	&	2.002	&	1.933	&	1.489	&	0.969	&	2.065	\\
DIF(2)+FB2	&	1.269	&	1.052	&	1.016	&	1.029	&	1.247	\\
DIF(3)+FB2	&	0.947	&	1.007	&	1.022	&	1.057	&	1.177	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: Table 2A reports the mean squared forecast error (MSFE) relative to that from the benchmark AR(1) model based on 1-step-ahead forecasts of 
monthly U.S. Treasury bond yields of various maturities. The models, as listed in column 1, are summarized in Table 1. Entries in bold denote models with lowest 
mean square forecast error (MSFE) for a given bond maturity. 
Entries superscripted with \textsuperscript{***}, \textsuperscript{**}, 
and \textsuperscript{*} denote rejections of the null of equal predictive accuracy at 0.01, 0.05, and 0.10 significance levels, respectively, 
based on application of the Diebold-Mariano test discussed in Section 3; and indicate that the listed model is predictively superior to the 
AR(1) benchmark, based on MSFE loss. For complete details, refer to Section 4.
\end{minipage}

\newpage

Table 2B: 1-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
2: 2000:1-2007:12) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.970	&	1.029	&	1.032	&	1.045	&	1.110	\\
VAR(1)+FB1	&	0.733\textnormal{\superscript{**}}	&	0.858\textnormal{\superscript{*}}	&	0.906	&	0.971	&	1.084	\\
VAR(1)+FB2	&	0.810	&	0.899	&	0.936	&	1.003	&	1.157	\\
AR(SIC)	&	0.939	&	1.033	&	1.033	&	1.035	&	1.015	\\
VAR(SIC)	&	0.970	&	1.029	&	1.032	&	1.045	&	1.110	\\
VAR(SIC)+FB1	&	0.733\textnormal{\superscript{**}}	&	0.858\textnormal{\superscript{*}}	&	0.906	&	0.971	&	1.084	\\
VAR(SIC)+FB2	&	0.810	&	0.899	&	0.936	&	1.003	&	1.157	\\
DNS(1)	&	1.211	&	1.015	&	1.000	&	1.094	&	0.959\textnormal{\superscript{*}}	\\
DNS(2)	&	1.182	&	1.016	&	1.012	&	1.121	&	0.958\textnormal{\superscript{**}}	\\
DNS(3)	&	1.150	&	1.015	&	0.998	&	1.126	&	0.983	\\
DNS(4)	&	1.017	&	1.067	&	1.031	&	1.082	&	1.026	\\
DNS(5)	&	1.014	&	1.058	&	1.034	&	1.110	&	1.027	\\
DNS(6)	&	1.021	&	1.099	&	1.037	&	1.099	&	1.032	\\
DNS(1)+FB1	&	0.780\textnormal{\superscript{*}}	&	0.851	&	0.860	&	0.947	&	0.947	\\
DNS(2)+FB1	&	0.773\textnormal{\superscript{*}}	&	0.842	&	0.859\textnormal{\superscript{*}}	&	0.966	&	0.944	\\
DNS(3)+FB1	&	0.770\textnormal{\superscript{*}}	&	0.873	&	0.863	&	0.966	&	0.944	\\
DNS(4)+FB1	&	0.708\textnormal{\superscript{***}}	&	0.853\textnormal{\superscript{**}}	&	0.866\textnormal{\superscript{**}}	&	0.962	&	0.959	\\
DNS(5)+FB1	&	0.703\textnormal{\superscript{***}}	&	0.840\textnormal{\superscript{**}}	&	0.865\textnormal{\superscript{**}}	&	0.987	&	0.960	\\
DNS(6)+FB1	&	0.713\textnormal{\superscript{***}}	&	0.884\textnormal{\superscript{*}}	&	0.872\textnormal{\superscript{**}}	&	0.979	&	0.965	\\
DNS(1)+FB2	&	0.717\textnormal{\superscript{**}}	&	0.741\textnormal{\superscript{**}}	&	\bfseries 0.763\textnormal{\superscript{**}}	&	\bfseries 0.887	&	0.855\textnormal{\superscript{**}}	\\
DNS(2)+FB2	&	0.707\textnormal{\superscript{**}}	& \bfseries 	0.734\textnormal{\superscript{**}}	&	0.766\textnormal{\superscript{**}}	&	0.912	&	\bfseries 0.854\textnormal{\superscript{**}}	\\
DNS(3)+FB2	&	\bfseries 0.697\textnormal{\superscript{**}}	&	0.756\textnormal{\superscript{**}}	&	0.765\textnormal{\superscript{**}}	&	0.915	&	0.877\textnormal{\superscript{**}}	\\
DNS(4)+FB2	&	0.727\textnormal{\superscript{***}}	&	0.793\textnormal{\superscript{***}}	&	0.824\textnormal{\superscript{***}}	&	0.961	&	0.933	\\
DNS(5)+FB2	&	0.721\textnormal{\superscript{***}}	&	0.791\textnormal{\superscript{***}}	&	0.832\textnormal{\superscript{**}}	&	0.991	&	0.935	\\
DNS(6)+FB2	&	0.703\textnormal{\superscript{***}}	&	0.810\textnormal{\superscript{***}}	&	0.824\textnormal{\superscript{***}}	&	0.983	&	0.960	\\
DNS(1)+MAC	&	1.065	&	0.982	&	1.002	&	1.099	&	0.979	\\
DNS(2)+MAC	&	1.037	&	0.983	&	1.011	&	1.125	&	0.977	\\
DNS(3)+MAC	&	1.000	&	0.983	&	1.000	&	1.129	&	0.997	\\
DNS(4)+MAC	&	0.972	&	1.040	&	1.056	&	1.165	&	1.064	\\
DNS(5)+MAC	&	0.960	&	1.037	&	1.065	&	1.197	&	1.065	\\
DNS(6)+MAC	&	0.949	&	1.057	&	1.056	&	1.190	&	1.097	\\
DIF(1)	&	2.474	&	2.046	&	1.688	&	1.062	&	1.788	\\
DIF(2)	&	1.288	&	1.112	&	1.104	&	1.061	&	1.214	\\
DIF(3)	&	1.029	&	1.128	&	1.114	&	1.073	&	1.121	\\
DIF(4)	&	1.566	&	1.733	&	1.830	&	1.930	&	1.961	\\
DIF(5)	&	1.349	&	1.688	&	1.805	&	1.884	&	1.937	\\
DIF(6)	&	1.389	&	1.697	&	1.804	&	1.868	&	1.919	\\
DIF(1)+FB1	&	1.575	&	1.633	&	1.468	&	1.045	&	1.794	\\
DIF(2)+FB1	&	1.093	&	1.001	&	1.027	&	1.038	&	1.227	\\
DIF(3)+FB1	&	0.892	&	1.021	&	1.049	&	1.053	&	1.115	\\
DIF(1)+FB2	&	1.435	&	1.673	&	1.521	&	1.039	&	1.667	\\
DIF(2)+FB2	&	1.117	&	1.024	&	1.039	&	1.046	&	1.184	\\
DIF(3)+FB2	&	0.875	&	1.023	&	1.058	&	1.059	&	1.122	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 2C: 1-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
3: 2008:1-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{*}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	\bfseries 1.000	&	\bfseries 1.000	&	1.000	&	1.000	\\
VAR(1)	&	1.177	&	1.242	&	1.222	&	1.180	&	1.158	\\
VAR(1)+FB1	&	1.249	&	1.311	&	1.293	&	1.281	&	1.297	\\
VAR(1)+FB2	&	1.369	&	1.451	&	1.406	&	1.345	&	1.320	\\
AR(SIC)	&	\bfseries 0.920	&	1.028	&	1.005	&	\bfseries 0.999	&	1.009	\\
VAR(SIC)	&	1.177	&	1.242	&	1.222	&	1.180	&	1.158	\\
VAR(SIC)+FB1	&	1.249	&	1.311	&	1.293	&	1.281	&	1.297	\\
VAR(SIC)+FB2	&	1.369	&	1.451	&	1.406	&	1.345	&	1.320	\\
DNS(1)	&	2.108	&	1.044	&	1.137	&	1.444	&	0.932\textnormal{\superscript{*}}	\\
DNS(2)	&	1.966	&	1.085	&	1.223	&	1.542	&	\bfseries 0.928\textnormal{\superscript{*}}	\\
DNS(3)	&	1.710	&	1.003	&	1.098	&	1.521	&	0.980	\\
DNS(4)	&	1.396	&	1.156	&	1.141	&	1.387	&	1.042	\\
DNS(5)	&	1.317	&	1.127	&	1.167	&	1.459	&	1.030	\\
DNS(6)	&	1.231	&	1.220	&	1.137	&	1.450	&	1.071	\\
DNS(1)+FB1	&	2.132	&	1.536	&	1.390	&	1.511	&	1.078	\\
DNS(2)+FB1	&	2.030	&	1.522	&	1.421	&	1.583	&	1.072	\\
DNS(3)+FB1	&	1.914	&	1.577	&	1.379	&	1.582	&	1.110	\\
DNS(4)+FB1	&	1.420	&	1.315	&	1.212	&	1.387	&	1.132	\\
DNS(5)+FB1	&	1.373	&	1.275	&	1.223	&	1.452	&	1.123	\\
DNS(6)+FB1	&	1.306	&	1.403	&	1.213	&	1.437	&	1.138	\\
DNS(1)+FB2	&	2.259	&	1.661	&	1.469	&	1.523	&	1.075	\\
DNS(2)+FB2	&	2.149	&	1.645	&	1.497	&	1.591	&	1.068	\\
DNS(3)+FB2	&	2.044	&	1.707	&	1.462	&	1.595	&	1.106	\\
DNS(4)+FB2	&	1.553	&	1.467	&	1.327	&	1.454	&	1.177	\\
DNS(5)+FB2	&	1.503	&	1.423	&	1.332	&	1.513	&	1.166	\\
DNS(6)+FB2	&	1.442	&	1.552	&	1.325	&	1.501	&	1.180	\\
DNS(1)+MAC	&	1.720	&	1.051	&	1.094	&	1.331	&	0.943	\\
DNS(2)+MAC	&	1.604	&	1.064	&	1.149	&	1.413	&	0.939	\\
DNS(3)+MAC	&	1.429	&	1.060	&	1.078	&	1.406	&	0.966	\\
DNS(4)+MAC	&	1.316	&	1.137	&	1.141	&	1.382	&	1.056	\\
DNS(5)+MAC	&	1.228	&	1.108	&	1.162	&	1.447	&	1.041	\\
DNS(6)+MAC	&	1.144	&	1.199	&	1.132	&	1.437	&	1.077	\\
DIF(1)	&	2.521	&	2.326	&	2.076	&	1.245	&	1.621	\\
DIF(2)	&	1.581	&	1.336	&	1.315	&	1.215	&	1.196	\\
DIF(3)	&	1.058	&	1.392	&	1.388	&	1.259	&	1.257	\\
DIF(4)	&	4.145	&	3.409	&	2.884	&	2.440	&	2.219	\\
DIF(5)	&	4.718	&	3.707	&	2.960	&	2.305	&	1.980	\\
DIF(6)	&	4.699	&	3.805	&	3.151	&	2.573	&	2.192	\\
DIF(1)+FB1	&	4.053	&	3.207	&	2.411	&	1.280	&	1.575	\\
DIF(2)+FB1	&	2.127	&	1.637	&	1.439	&	1.270	&	1.226	\\
DIF(3)+FB1	&	1.367	&	1.666	&	1.532	&	1.326	&	1.320	\\
DIF(1)+FB2	&	4.438	&	3.232	&	2.257	&	1.179	&	1.452	\\
DIF(2)+FB2	&	2.100	&	1.615	&	1.403	&	1.182	&	1.106	\\
DIF(3)+FB2	&	1.341	&	1.623	&	1.448	&	1.190	&	1.147	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 2D: 1-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
4: 1992:3-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&\bfseries 1.000	&	1.000	\\
VAR(1)	&	1.063	&	1.103	&	1.101	&	1.102	&	1.139	\\
VAR(1)+FB1	&	0.874\textnormal{\superscript{*}}	&	0.955	&	0.990	&	1.046	&	1.165	\\
VAR(1)+FB2	&	0.940	&	1.003	&	1.029	&	1.081	&	1.213	\\
AR(SIC)	&	0.906\textnormal{\superscript{*}}	&	0.998	&	0.999	&	1.004	&	1.001	\\
VAR(SIC)	&	1.063	&	1.103	&	1.101	&	1.102	&	1.139	\\
VAR(SIC)+FB1	&	0.874\textnormal{\superscript{*}}	&	0.955	&	0.990	&	1.046	&	1.165	\\
VAR(SIC)+FB2	&	0.940	&	1.003	&	1.029	&	1.081	&	1.213	\\
DNS(1)	&	1.331	&	1.052	&	1.053	&	1.177	&	0.976	\\
DNS(2)	&	1.291	&	1.058	&	1.075	&	1.218	&\bfseries	0.973	\\
DNS(3)	&	1.226	&	1.053	&	1.046	&	1.213	&	0.996	\\
DNS(4)	&	1.123	&	1.120	&	1.083	&	1.166	&	1.053	\\
DNS(5)	&	1.108	&	1.106	&	1.086	&	1.201	&	1.047	\\
DNS(6)	&	1.093	&	1.158	&	1.085	&	1.189	&	1.059	\\
DNS(1)+FB1	&	1.109	&	0.996	&	0.994	&	1.122	&	1.012	\\
DNS(2)+FB1	&	1.081	&	0.991	&	1.003	&	1.155	&	1.008	\\
DNS(3)+FB1	&	1.050	&	1.016	&	0.993	&	1.157	&	1.027	\\
DNS(4)+FB1	&	0.886\textnormal{\superscript{*}}	&	0.951	&\bfseries	0.946	&	1.071	&	1.041	\\
DNS(5)+FB1	&	0.874\textnormal{\superscript{*}}	&\bfseries	0.935	&	0.947	&	1.104	&	1.037	\\
DNS(6)+FB1	&\bfseries	0.861\textnormal{\superscript{**}}	&	0.990	&	0.950	&	1.095	&	1.045	\\
DNS(1)+FB2	&	1.132	&	0.993	&	0.992	&	1.127	&	1.001	\\
DNS(2)+FB2	&	1.100	&	0.988	&	1.003	&	1.162	&	0.998	\\
DNS(3)+FB2	&	1.069	&	1.010	&	0.991	&	1.167	&	1.027	\\
DNS(4)+FB2	&	0.924	&	0.951	&	0.951	&	1.090	&	1.052	\\
DNS(5)+FB2	&	0.911	&	0.939	&	0.955	&	1.123	&	1.047	\\
DNS(6)+FB2	&	0.885	&	0.982	&	0.951	&	1.115	&	1.061	\\
DNS(1)+MAC	&	1.188	&	1.040	&	1.049	&	1.152	&	0.995	\\
DNS(2)+MAC	&	1.153	&	1.040	&	1.063	&	1.187	&	0.991	\\
DNS(3)+MAC	&	1.102	&	1.052	&	1.047	&	1.187	&	1.001	\\
DNS(4)+MAC	&	1.105	&	1.101	&	1.102	&	1.224	&	1.094	\\
DNS(5)+MAC	&	1.081	&	1.091	&	1.109	&	1.258	&	1.086	\\
DNS(6)+MAC	&	1.055	&	1.127	&	1.100	&	1.252	&	1.112	\\
DIF(1)	&	2.702	&	2.334	&	1.863	&	1.067	&	1.838	\\
DIF(2)	&	1.344	&	1.141	&	1.128	&	1.095	&	1.203	\\
DIF(3)	&	1.014	&	1.151	&	1.151	&	1.119	&	1.181	\\
DIF(4)	&	2.361	&	2.293	&	2.256	&	2.229	&	2.198	\\
DIF(5)	&	2.397	&	2.349	&	2.281	&	2.197	&	2.128	\\
DIF(6)	&	2.404	&	2.366	&	2.314	&	2.253	&	2.193	\\
DIF(1)+FB1	&	2.334	&	2.165	&	1.774	&	1.081	&	1.818	\\
DIF(2)+FB1	&	1.403	&	1.159	&	1.120	&	1.105	&	1.234	\\
DIF(3)+FB1	&	1.016	&	1.147	&	1.149	&	1.134	&	1.215	\\
DIF(1)+FB2	&	2.279	&	2.092	&	1.677	&	1.056	&	1.681	\\
DIF(2)+FB2	&	1.381	&	1.156	&	1.114	&	1.080	&	1.167	\\
DIF(3)+FB2	&	1.000	&	1.140	&	1.134	&	1.096	&	1.147	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 3A: 3-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
1: 1992:7-1999:12) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{*}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSS[table-space-text-post = \textnormal{\superscript{**}}]SS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000			\\
VAR(1)	&	1.014	&	1.062	&	1.074	&	1.063	&	1.035			\\
VAR(1)+FB1	&	0.981	&	1.037	&	1.055	&	1.050	&	1.030		\\
VAR(1)+FB2	&	1.070	&	1.119	&	1.134	&	1.130	&	1.114		\\
AR(SIC)	&	0.885\textnormal{\superscript{**}}	&	0.963	&	0.974	&	0.969	&	\bfseries 0.933\textnormal{\superscript{**}}		 \\
VAR(SIC)	&	1.014	&	1.062	&	1.074	&	1.063	&	1.035		\\
VAR(SIC)+FB1	&	0.981	&	1.037	&	1.055	&	1.050	&	1.030	\\
VAR(SIC)+FB2	&	1.070	&	1.119	&	1.134	&	1.130	&	1.114	\\
DNS(1)	&	1.061	&	1.079	&	1.065	&	1.047	&	1.029			\\
DNS(2)	&	1.067	&	1.082	&	1.069	&	1.054	&	1.031			\\
DNS(3)	&	1.065	&	1.080	&	1.062	&	1.046	&	1.028			\\
DNS(4)	&	0.990	&	1.075	&	1.063	&	1.024	&	1.003			\\
DNS(5)	&	0.996	&	1.071	&	1.058	&	1.024	&	1.002			\\
DNS(6)	&	1.000	&	1.086	&	1.063	&	1.017	&	0.989			\\
DNS(1)+FB1	&	0.922	&	0.914	&	\bfseries 0.954	&	1.013	&	1.000		\\
DNS(2)+FB1	&	0.923	&	0.924	&	0.964	&	1.024	&	1.004		\\
DNS(3)+FB1	&	0.918	&	\bfseries 0.914	&	0.955	&	1.025	&	1.022		\\
DNS(4)+FB1	&	\bfseries 0.855\textnormal{\superscript{**}}	&	0.961	&	0.973	&	0.966	&	0.959  	  \\
DNS(5)+FB1	&	0.861\textnormal{\superscript{**}}	&	0.957	&	0.969	&	0.966	&	0.956	\\
DNS(6)+FB1	&	0.860\textnormal{\superscript{**}}	&	0.968	&	0.972	&	\bfseries 0.961	&	0.951	\\
DNS(1)+FB2	&	1.033	&	1.019	&	1.039	&	1.063	&	1.008	\\
DNS(2)+FB2	&	1.032	&	1.027	&	1.048	&	1.074	&	1.013	\\
DNS(3)+FB2	&	1.034	&	1.024	&	1.045	&	1.080	&	1.035	\\
DNS(4)+FB2	&	0.920\textnormal{\superscript{*}}	&	1.016	&	1.025	&	1.014	&	0.998	\\
DNS(5)+FB2	&	0.930	&	1.017	&	1.025	&	1.019	&	0.999	\\
DNS(6)+FB2	&	0.919\textnormal{\superscript{*}}	&	1.018	&	1.021	&	1.008	&	0.990	\\
DNS(1)+MAC	&	1.056	&	1.099	&	1.091	&	1.063	&	1.025	\\
DNS(2)+MAC	&	1.063	&	1.102	&	1.093	&	1.068	&	1.028	\\
DNS(3)+MAC	&	1.060	&	1.102	&	1.089	&	1.061	&	1.020	\\
DNS(4)+MAC	&	0.942	&	1.043	&	1.052	&	1.040	&	1.031	\\
DNS(5)+MAC	&	0.945	&	1.037	&	1.045	&	1.036	&	1.026	\\	
DNS(6)+MAC	&	0.950	&	1.053	&	1.054	&	1.038	&	1.025	\\
DIF(1)	&	1.678	&	1.558	&	1.285	&	0.987	&	1.381	\\
DIF(2)	&	1.272	&	1.252	&	1.227	&	1.207	&	1.248	\\
DIF(3)	&	1.209	&	1.241	&	1.218	&	1.181	&	1.195	\\
DIF(4)	&	1.217	&	1.278	&	1.312	&	1.355	&	1.430	\\
DIF(5)	&	1.465	&	1.533	&	1.563	&	1.568	&	1.541	\\
DIF(6)	&	1.489	&	1.558	&	1.587	&	1.587	&	1.538	\\
DIF(1)+FB1	&	1.272	&	1.340	&	1.196	&	0.998	&	1.406	\\
DIF(2)+FB1	&	1.216	&	1.199	&	1.178	&	1.176	&	1.240	\\
DIF(3)+FB1	&	1.050	&	1.100	&	1.111	&	1.124	&	1.184	\\
DIF(1)+FB2	&	1.395	&	1.513	&	1.388	&	1.200	&	1.408	\\
DIF(2)+FB2	&	1.316	&	1.305	&	1.276	&	1.244	&	1.266	\\
DIF(3)+FB2	&	1.140	&	1.203	&	1.209	&	1.199	&	1.229	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 3B: 3-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
2: 2000:1-2007:12) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.832\textnormal{\superscript{***}}	&	0.880\textnormal{\superscript{***}}	&	0.885\textnormal{\superscript{**}}	&	0.909\textnormal{\superscript{*}}	&	1.013	\\
VAR(1)+FB1	&	0.837\textnormal{\superscript{***}}	&	0.880\textnormal{\superscript{***}}	&	0.882\textnormal{\superscript{**}}	&	0.905\textnormal{\superscript{**}}	&	1.009	\\
VAR(1)+FB2	&	0.839\textnormal{\superscript{***}}	&	0.889\textnormal{\superscript{**}}	&	0.895\textnormal{\superscript{**}}	&	0.924\textnormal{\superscript{*}}	&	1.043	\\
AR(SIC)	&	0.819\textnormal{\superscript{***}}	&	0.877\textnormal{\superscript{**}}	&	0.873\textnormal{\superscript{**}}	&	0.881\textnormal{\superscript{**}}	&	0.937	\\
VAR(SIC)	&	0.832\textnormal{\superscript{***}}	&	0.880\textnormal{\superscript{***}}	&	0.885\textnormal{\superscript{**}}	&	0.909\textnormal{\superscript{*}}	&	1.013	\\
VAR(SIC)+FB1	&	0.837\textnormal{\superscript{***}}	&	0.880\textnormal{\superscript{***}}	&	0.882\textnormal{\superscript{**}}	&	0.905\textnormal{\superscript{**}}	&	1.009	\\
VAR(SIC)+FB2	&	0.839\textnormal{\superscript{***}}	&	0.889\textnormal{\superscript{**}}	&	0.895\textnormal{\superscript{**}}	&	0.924\textnormal{\superscript{*}}	&	1.043	\\
DNS(1)	&	1.250	&	1.068	&	1.009	&	1.051	&	0.913\textnormal{\superscript{***}}	\\
DNS(2)	&	1.233	&	1.069	&	1.019	&	1.066	&	0.913\textnormal{\superscript{***}}	\\
DNS(3)	&	1.232	&	1.063	&	1.009	&	1.078	&	0.977	\\
DNS(4)	&	0.905\textnormal{\superscript{***}}	&	0.919\textnormal{\superscript{**}}	&	0.895\textnormal{\superscript{**}}	&	0.915\textnormal{\superscript{**}}	&	0.929	\\
DNS(5)	&	0.900\textnormal{\superscript{***}}	&	0.916\textnormal{\superscript{**}}	&	0.897\textnormal{\superscript{**}}	&	0.926\textnormal{\superscript{**}}	&	0.929	\\
DNS(6)	&	0.915\textnormal{\superscript{***}}	&	0.930\textnormal{\superscript{*}}	&	0.897\textnormal{\superscript{**}}	&	0.920\textnormal{\superscript{**}}	&	0.942	\\
DNS(1)+FB1	&	0.676\textnormal{\superscript{**}}	&	0.705\textnormal{\superscript{**}}	&	0.724\textnormal{\superscript{**}}	&	\bfseries 0.827\textnormal{\superscript{**}}	&	0.846\textnormal{\superscript{**}}	\\
DNS(2)+FB1	&	0.674\textnormal{\superscript{**}}	&	0.706\textnormal{\superscript{**}}	&	0.730\textnormal{\superscript{**}}	&	0.840\textnormal{\superscript{*}}	&	\bfseries 0.846\textnormal{\superscript{**}}	\\
DNS(3)+FB1	&	\bfseries 0.672\textnormal{\superscript{**}}	&	\bfseries 0.703\textnormal{\superscript{**}}	&	\bfseries 0.720\textnormal{\superscript{**}}	&	0.842\textnormal{\superscript{*}}	&	0.891\textnormal{\superscript{*}}	\\
DNS(4)+FB1	&	0.830\textnormal{\superscript{***}}	&	0.857\textnormal{\superscript{***}}	&	0.846\textnormal{\superscript{***}}	&	0.884\textnormal{\superscript{**}}	&	0.909\textnormal{\superscript{*}}	\\
DNS(5)+FB1	&	0.830\textnormal{\superscript{***}}	&	0.857\textnormal{\superscript{***}}	&	0.851\textnormal{\superscript{***}}	&	0.898\textnormal{\superscript{**}}	&	0.909\textnormal{\superscript{*}}	\\
DNS(6)+FB1	&	0.833\textnormal{\superscript{***}}	&	0.863\textnormal{\superscript{**}}	&	0.845\textnormal{\superscript{***}}	&	0.889\textnormal{\superscript{**}}	&	0.925\textnormal{\superscript{*}}	\\
DNS(1)+FB2	&	0.794\textnormal{\superscript{*}}	&	0.755\textnormal{\superscript{**}}	&	0.773\textnormal{\superscript{**}}	&	0.898	&	0.921	\\
DNS(2)+FB2	&	0.784\textnormal{\superscript{*}}	&	0.754\textnormal{\superscript{**}}	&	0.779\textnormal{\superscript{**}}	&	0.912	&	0.920	\\
DNS(3)+FB2	&	0.793	&	0.757\textnormal{\superscript{**}}	&	0.777\textnormal{\superscript{**}}	&	0.929	&	0.998	\\
DNS(4)+FB2	&	0.833\textnormal{\superscript{***}}	&	0.860\textnormal{\superscript{***}}	&	0.849\textnormal{\superscript{***}}	&	0.887\textnormal{\superscript{**}}	&	0.915\textnormal{\superscript{*}}	\\
DNS(5)+FB2	&	0.833\textnormal{\superscript{***}}	&	0.860\textnormal{\superscript{***}}	&	0.854\textnormal{\superscript{***}}	&	0.901\textnormal{\superscript{**}}	&	0.916\textnormal{\superscript{*}}	\\
DNS(6)+FB2	&	0.836\textnormal{\superscript{***}}	&	0.866\textnormal{\superscript{**}}	&	0.848\textnormal{\superscript{***}}	&	0.892\textnormal{\superscript{**}}	&	0.931\textnormal{\superscript{*}}	\\
DNS(1)+MAC	&	1.073	&	1.026	&	1.026	&	1.097	&	0.963\textnormal{\superscript{*}}	\\
DNS(2)+MAC	&	1.055	&	1.028	&	1.036	&	1.111	&	0.962\textnormal{\superscript{*}}	\\
DNS(3)+MAC	&	1.049	&	1.018	&	1.025	&	1.122	&	1.020	\\
DNS(4)+MAC	&	0.853\textnormal{\superscript{***}}	&	0.897\textnormal{\superscript{*}}	&	0.900\textnormal{\superscript{*}}	&	0.952	&	0.972	\\
DNS(5)+MAC	&	0.849\textnormal{\superscript{***}}	&	0.896\textnormal{\superscript{*}}	&	0.903\textnormal{\superscript{*}}	&	0.964	&	0.970	\\
DNS(6)+MAC	&	0.854\textnormal{\superscript{***}}	&	0.901\textnormal{\superscript{*}}	&	0.898\textnormal{\superscript{*}}	&	0.959	&	0.996	\\
DIF(1)	&	1.641	&	1.460	&	1.331	&	1.146	&	1.286	\\
DIF(2)	&	1.234	&	1.217	&	1.191	&	1.188	&	1.354	\\
DIF(3)	&	1.186	&	1.280	&	1.261	&	1.241	&	1.340	\\
DIF(4)	&	0.946	&	1.084	&	1.188	&	1.330	&	1.358	\\
DIF(5)	&	0.970	&	1.143	&	1.210	&	1.305	&	1.488	\\
DIF(6)	&	1.007	&	1.182	&	1.247	&	1.331	&	1.511	\\
DIF(1)+FB1	&	0.943	&	1.057	&	1.096	&	1.196	&	1.428	\\
DIF(2)+FB1	&	0.915	&	1.018	&	1.082	&	1.184	&	1.462	\\
DIF(3)+FB1	&	0.911	&	1.081	&	1.149	&	1.227	&	1.413	\\
DIF(1)+FB2	&	1.090	&	1.273	&	1.266	&	1.189	&	1.728	\\
DIF(2)+FB2	&	0.913	&	1.059	&	1.124	&	1.196	&	1.414	\\
DIF(3)+FB2	&	0.905	&	1.098	&	1.171	&	1.242	&	1.432	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 3C: 3-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
3: 2008:1-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.975	&	0.998	&	0.974	&	0.927\textnormal{\superscript{*}}	&	0.926\textnormal{\superscript{*}}	\\
VAR(1)+FB1	&	0.937	&	0.976	&	0.961	&	0.924	&	0.935	\\
VAR(1)+FB2	&	0.942	&	0.978	&	0.963	&	0.926	&	0.936	\\
AR(SIC)	&	0.975	&	0.971	&	\bfseries 0.946\textnormal{\superscript{**}}	&	\bfseries 0.921\textnormal{\superscript{**}}	&	0.917\textnormal{\superscript{**}}	\\
VAR(SIC)	&	0.975	&	0.998	&	0.974	&	0.927\textnormal{\superscript{*}}	&	0.926\textnormal{\superscript{*}}	\\
VAR(SIC)+FB1	&	0.937	&	0.976	&	0.961	&	0.924	&	0.935	\\
VAR(SIC)+FB2	&	0.942	&	0.978	&	0.963	&	0.926	&	0.936	\\
DNS(1)	&	1.825	&	1.278	&	1.304	&	1.407	&	0.971	\\
DNS(2)	&	1.762	&	1.310	&	1.357	&	1.453	&	0.968	\\
DNS(3)	&	1.652	&	1.203	&	1.260	&	1.431	&	1.022	\\
DNS(4)	&	1.046	&	0.974	&	0.990	&	1.050	&	0.907\textnormal{\superscript{*}}	\\
DNS(5)	&	1.025	&	0.973	&	1.001	&	1.071	&	0.899\textnormal{\superscript{*}}	\\
DNS(6)	&	0.997	&	0.979	&	0.987	&	1.070	&	0.919\textnormal{\superscript{*}}	\\
DNS(1)+FB1	&	2.186	&	1.848	&	1.660	&	1.545	&	1.093	\\
DNS(2)+FB1	&	2.154	&	1.846	&	1.675	&	1.571	&	1.089	\\
DNS(3)+FB1	&	2.118	&	1.823	&	1.633	&	1.563	&	1.136	\\
DNS(4)+FB1	&	0.957	&	0.929	&	0.948	&	1.014	&	0.914	\\
DNS(5)+FB1	&	0.939	& \bfseries	0.927	&	0.957	&	1.035	&	0.907	\\
DNS(6)+FB1	& \bfseries	0.915	&	0.939	&	0.947	&	1.032	&	0.920\textnormal{\superscript{*}}	\\
DNS(1)+FB2	&	2.215	&	1.871	&	1.643	&	1.478	&	1.041	\\
DNS(2)+FB2	&	2.184	&	1.863	&	1.651	&	1.499	&	1.036	\\
DNS(3)+FB2	&	2.158	&	1.853	&	1.619	&	1.493	&	1.074	\\
DNS(4)+FB2	&	0.984	&	0.948	&	0.964	&	1.027	&	0.923	\\
DNS(5)+FB2	&	0.966	&	0.945	&	0.972	&	1.047	&	0.916	\\
DNS(6)+FB2	&	0.943	&	0.960	&	0.964	&	1.045	&	0.930	\\
DNS(1)+MAC	&	1.864	&	1.393	&	1.372	&	1.423	&	1.006	\\
DNS(2)+MAC	&	1.811	&	1.417	&	1.416	&	1.467	&	1.003	\\
DNS(3)+MAC	&	1.721	&	1.337	&	1.339	&	1.451	&	1.056	\\
DNS(4)+MAC	&	1.039	&	0.977	&	0.992	&	1.045	&	0.903\textnormal{\superscript{*}}	\\
DNS(5)+MAC	&	1.017	&	0.978	&	1.004	&	1.065	&	\bfseries 0.895\textnormal{\superscript{*}}	\\
DNS(6)+MAC	&	0.988	&	0.981	&	0.987	&	1.061	&	0.911\textnormal{\superscript{**}}	\\
DIF(1)	&	1.367	&	1.366	&	1.315	&	1.124	&	1.230	\\
DIF(2)	&	1.514	&	1.527	&	1.428	&	1.249	&	1.170	\\
DIF(3)	&	1.483	&	1.602	&	1.535	&	1.369	&	1.219	\\
DIF(4)	&	2.757	&	2.332	&	2.023	&	1.677	&	1.430	\\
DIF(5)	&	2.918	&	2.444	&	2.107	&	1.728	&	1.429	\\
DIF(6)	&	3.131	&	2.713	&	2.456	&	2.145	&	1.729	\\
DIF(1)+FB1	&	2.756	&	2.260	&	1.817	&	1.282	&	1.270	\\
DIF(2)+FB1	&	2.146	&	1.940	&	1.674	&	1.376	&	1.230	\\
DIF(3)+FB1	&	1.867	&	1.948	&	1.741	&	1.460	&	1.272	\\
DIF(1)+FB2	&	2.826	&	2.300	&	1.852	&	1.328	&	1.252	\\
DIF(2)+FB2	&	2.111	&	1.912	&	1.662	&	1.377	&	1.244	\\
DIF(3)+FB2	&	1.810	&	1.895	&	1.702	&	1.437	&	1.256	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 3D: 3-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
4: 1992:7-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
		Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
		Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.927\textnormal{\superscript{**}}	&	0.974	&	0.977	&	0.972	&	0.987	\\
VAR(1)+FB1	&	0.909\textnormal{\superscript{***}}	&	0.960	&	0.966	&	0.965	&	0.988	\\
VAR(1)+FB2	&	0.941\textnormal{\superscript{**}}	&	0.994	&	1.001	&	1.002	&	1.027	\\
AR(SIC)	&	0.878\textnormal{\superscript{***}}	&	0.930\textnormal{\superscript{**}}	&	0.928\textnormal{\superscript{***}}	&\bfseries	0.925\textnormal{\superscript{***}}	&	0.928\textnormal{\superscript{***}}	\\
VAR(SIC)	&	0.927\textnormal{\superscript{**}}	&	0.974	&	0.977	&	0.972	&	0.987	\\
VAR(SIC)+FB1	&	0.909\textnormal{\superscript{***}}	&	0.960	&	0.966	&	0.965	&	0.988	\\
VAR(SIC)+FB2	&	0.941\textnormal{\superscript{**}}	&	0.994	&	1.001	&	1.002	&	1.027	\\
DNS(1)	&	1.320	&	1.119	&	1.101	&	1.150	&	0.976	\\
DNS(2)	&	1.300	&	1.129	&	1.118	&	1.171	&	0.976	\\
DNS(3)	&	1.273	&	1.101	&	1.089	&	1.166	&	1.012	\\
DNS(4)	&	0.967\textnormal{\superscript{*}}	&	0.989	&	0.981	&	0.994	&	0.946\textnormal{\superscript{*}}	\\
DNS(5)	&	0.962\textnormal{\superscript{*}}	&	0.986	&	0.982	&	1.004	&	0.943\textnormal{\superscript{*}}	\\
DNS(6)	&	0.963\textnormal{\superscript{*}}	&	0.999	&	0.981	&	0.999	&	0.949\textnormal{\superscript{**}}	\\
DNS(1)+FB1	&	1.111	&	1.041	&	1.034	&	1.100	&	0.995	\\
DNS(2)+FB1	&	1.103	&	1.045	&	1.044	&	1.116	&	0.995	\\
DNS(3)+FB1	&	1.092	&	1.035	&	1.027	&	1.114	&	1.031	\\
DNS(4)+FB1	&	0.868\textnormal{\superscript{***}}	&	0.912\textnormal{\superscript{**}}	&	0.918\textnormal{\superscript{**}}	&	0.952\textnormal{\superscript{**}}	&	0.928\textnormal{\superscript{**}}	\\
DNS(5)+FB1	&	0.866\textnormal{\superscript{***}}	&\bfseries	0.910\textnormal{\superscript{***}}	&	0.920\textnormal{\superscript{***}}	&	0.962\textnormal{\superscript{*}}	&\bfseries	0.925\textnormal{\superscript{**}}	\\
DNS(6)+FB1	&\bfseries	0.861\textnormal{\superscript{***}}	&	0.919\textnormal{\superscript{**}}	&\bfseries	0.917\textnormal{\superscript{**}}	&	0.956\textnormal{\superscript{**}}	&	0.932\textnormal{\superscript{**}}	\\
DNS(1)+FB2	&	1.206	&	1.106	&	1.081	&	1.124	&	0.997	\\
DNS(2)+FB2	&	1.194	&	1.106	&	1.089	&	1.139	&	0.997	\\
DNS(3)+FB2	&	1.193	&	1.104	&	1.079	&	1.145	&	1.040	\\
DNS(4)+FB2	&	0.898\textnormal{\superscript{***}}	&	0.938\textnormal{\superscript{*}}	&	0.943\textnormal{\superscript{**}}	&	0.974	&	0.947\textnormal{\superscript{*}}	\\
DNS(5)+FB2	&	0.897\textnormal{\superscript{***}}	&	0.938\textnormal{\superscript{**}}	&	0.947\textnormal{\superscript{**}}	&	0.987	&	0.945\textnormal{\superscript{*}}	\\
DNS(6)+FB2	&	0.889\textnormal{\superscript{***}}	&	0.944\textnormal{\superscript{*}}	&	0.940\textnormal{\superscript{**}}	&	0.979	&	0.951\textnormal{\superscript{**}}	\\
DNS(1)+MAC	&	1.251	&	1.136	&	1.133	&	1.177	&	1.001	\\
DNS(2)+MAC	&	1.234	&	1.144	&	1.148	&	1.196	&	1.001	\\
DNS(3)+MAC	&	1.209	&	1.121	&	1.124	&	1.192	&	1.034	\\
DNS(4)+MAC	&	0.927\textnormal{\superscript{**}}	&	0.969	&	0.979	&	1.011	&	0.966	\\
DNS(5)+MAC	&	0.921\textnormal{\superscript{**}}	&	0.967	&	0.981	&	1.020	&	0.961	\\
DNS(6)+MAC	&	0.918\textnormal{\superscript{**}}	&	0.976	&	0.978	&	1.017	&	0.974	\\
DIF(1)	&	1.589	&	1.475	&	1.310	&	1.081	&	1.298	\\
DIF(2)	&	1.312	&	1.300	&	1.261	&	1.212	&	1.246	\\
DIF(3)	&	1.263	&	1.339	&	1.310	&	1.255	&	1.243	\\
DIF(4)	&	1.460	&	1.438	&	1.435	&	1.438	&	1.411	\\
DIF(5)	&	1.591	&	1.582	&	1.557	&	1.523	&	1.484	\\
DIF(6)	&	1.665	&	1.668	&	1.664	&	1.658	&	1.604	\\
DIF(1)+FB1	&	1.477	&	1.434	&	1.306	&	1.147	&	1.359	\\
DIF(2)+FB1	&	1.304	&	1.294	&	1.260	&	1.235	&	1.295	\\
DIF(3)+FB1	&	1.181	&	1.284	&	1.276	&	1.254	&	1.279	\\
DIF(1)+FB2	&	1.598	&	1.594	&	1.452	&	1.233	&	1.433	\\
DIF(2)+FB2	&	1.329	&	1.343	&	1.310	&	1.265	&	1.297	\\
DIF(3)+FB2	&	1.195	&	1.318	&	1.312	&	1.281	&	1.293	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize

\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 4A: 12-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
1: 1994:1-1999:12) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 1,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	1.312	&	1.302	&	1.280	&	1.207	&	1.049	\\
VAR(1)+FB1	&	1.299	&	1.290	&	1.267	&	1.194	&	1.036	\\
VAR(1)+FB2	&	1.292	&	1.282	&	1.260	&	1.189	&	1.031	\\
AR(SIC)	&	1.208	&	1.217	&	1.206	&	1.132	&	0.967	\\
VAR(SIC)	&	1.312	&	1.302	&	1.280	&	1.207	&	1.049	\\
VAR(SIC)+FB1	&	1.299	&	1.290	&	1.267	&	1.194	&	1.036	\\
VAR(SIC)+FB2	&	1.292	&	1.282	&	1.260	&	1.189	&	1.031	\\
DNS(1)	&	0.635\textnormal{\superscript{***}}	&	0.682\textnormal{\superscript{***}}	&	0.730\textnormal{\superscript{***}}	&	0.846\textnormal{\superscript{**}}	&	\bfseries 0.954	\\
DNS(2)	&	0.640\textnormal{\superscript{***}}	&	0.688\textnormal{\superscript{***}}	&	0.737\textnormal{\superscript{***}}	&	0.853\textnormal{\superscript{**}}	&	0.956	\\
DNS(3)	&	\bfseries 0.624\textnormal{\superscript{***}}	&	\bfseries 0.669\textnormal{\superscript{***}}	&	\bfseries 0.718\textnormal{\superscript{***}}	&	\bfseries 0.845\textnormal{\superscript{**}}	&	0.973	\\
DNS(4)	&	1.276	&	1.298	&	1.259	&	1.165	&	1.022	\\
DNS(5)	&	1.283	&	1.298	&	1.258	&	1.168	&	1.025	\\
DNS(6)	&	1.284	&	1.301	&	1.256	&	1.157	&	1.013	\\
DNS(1)+FB1	&	0.905	&	0.889	&	0.952	&	1.097	&	1.159	\\
DNS(2)+FB1	&	0.902	&	0.895	&	0.960	&	1.104	&	1.157	\\
DNS(3)+FB1	&	0.897	&	0.882	&	0.950	&	1.113	&	1.200	\\
DNS(4)+FB1	&	1.225	&	1.265	&	1.237	&	1.157	&	1.024	\\
DNS(5)+FB1	&	1.232	&	1.265	&	1.236	&	1.158	&	1.027	\\
DNS(6)+FB1	&	1.234	&	1.270	&	1.236	&	1.150	&	1.017	\\
DNS(1)+FB2	&	1.055	&	0.980	&	1.049	&	1.226	&	1.295	\\
DNS(2)+FB2	&	1.047	&	0.984	&	1.056	&	1.233	&	1.293	\\
DNS(3)+FB2	&	1.059	&	0.984	&	1.060	&	1.256	&	1.351	\\
DNS(4)+FB2	&	1.210	&	1.252	&	1.227	&	1.152	&	1.020	\\
DNS(5)+FB2	&	1.216	&	1.250	&	1.225	&	1.153	&	1.022	\\
DNS(6)+FB2	&	1.220	&	1.257	&	1.226	&	1.147	&	1.014	\\
DNS(1)+MAC	&	0.685\textnormal{\superscript{**}}	&	0.729\textnormal{\superscript{**}}	&	0.776\textnormal{\superscript{**}}	&	0.889	&	0.977	\\
DNS(2)+MAC	&	0.689\textnormal{\superscript{**}}	&	0.734\textnormal{\superscript{**}}	&	0.782\textnormal{\superscript{**}}	&	0.895	&	0.978	\\
DNS(3)+MAC	&	0.672\textnormal{\superscript{**}}	&	0.716\textnormal{\superscript{**}}	&	0.765\textnormal{\superscript{**}}	&	0.889	&	0.998	\\
DNS(4)+MAC	&	1.228	&	1.275	&	1.253	&	1.181	&	1.050	\\
DNS(5)+MAC	&	1.233	&	1.273	&	1.250	&	1.180	&	1.050	\\
DNS(6)+MAC	&	1.237	&	1.280	&	1.253	&	1.176	&	1.045	\\
DIF(1)	&	0.984	&	0.925\textnormal{\superscript{*}}	&	0.838\textnormal{\superscript{***}}	&	1.124	&	1.829	\\
DIF(2)	&	1.328	&	1.346	&	1.493	&	1.748	&	1.905	\\
DIF(3)	&	1.254	&	1.224	&	1.348	&	1.586	&	1.812	\\
DIF(4)	&	1.122	&	1.156	&	1.184	&	1.225	&	1.292	\\
DIF(5)	&	1.619	&	1.610	&	1.647	&	1.689	&	1.690	\\
DIF(6)	&	1.718	&	1.695	&	1.723	&	1.747	&	1.712	\\
DIF(1)+FB1	&	1.340	&	1.283	&	1.158	&	1.371	&	2.125	\\
DIF(2)+FB1	&	1.622	&	1.749	&	1.871	&	2.054	&	2.170	\\
DIF(3)+FB1	&	1.459	&	1.542	&	1.653	&	1.850	&	2.084	\\
DIF(1)+FB2	&	1.487	&	1.503	&	1.548	&	1.960	&	2.324	\\
DIF(2)+FB2	&	1.883	&	1.908	&	2.002	&	2.166	&	2.262	\\
DIF(3)+FB2	&	1.593	&	1.637	&	1.752	&	1.961	&	2.176	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 4B: 12-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
2: 2000:1-2007:12) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.567\textnormal{\superscript{***}}	&	\bfseries 0.475\textnormal{\superscript{***}}	&	\bfseries 0.432\textnormal{\superscript{***}}	&	\bfseries 0.431\textnormal{\superscript{***}}	&	0.557\textnormal{\superscript{***}}	\\
VAR(1)+FB1	&	0.583\textnormal{\superscript{***}}	&	0.488\textnormal{\superscript{***}}	&	0.444\textnormal{\superscript{***}}	&	0.441\textnormal{\superscript{***}}	&	0.561\textnormal{\superscript{***}}	\\
VAR(1)+FB2	&	0.587\textnormal{\superscript{***}}	&	0.493\textnormal{\superscript{***}}	&	0.451\textnormal{\superscript{***}}	&	0.452\textnormal{\superscript{***}}	&	0.584\textnormal{\superscript{***}}	\\
AR(SIC)	&	0.574\textnormal{\superscript{***}}	&	0.485\textnormal{\superscript{***}}	&	0.444\textnormal{\superscript{***}}	&	0.445\textnormal{\superscript{***}}	&	\bfseries 0.547\textnormal{\superscript{***}}	\\
VAR(SIC)	&	0.567\textnormal{\superscript{***}}	&	0.475\textnormal{\superscript{***}}	&	0.432\textnormal{\superscript{***}}	&	0.431\textnormal{\superscript{***}}	&	0.557\textnormal{\superscript{***}}	\\
VAR(SIC)+FB1	&	0.583\textnormal{\superscript{***}}	&	0.488\textnormal{\superscript{***}}	&	0.444\textnormal{\superscript{***}}	&	0.441\textnormal{\superscript{***}}	&	0.561\textnormal{\superscript{***}}	\\
VAR(SIC)+FB2	&	0.587\textnormal{\superscript{***}}	&	0.493\textnormal{\superscript{***}}	&	0.451\textnormal{\superscript{***}}	&	0.452\textnormal{\superscript{***}}	&	0.584\textnormal{\superscript{***}}	\\
DNS(1)	&	0.708\textnormal{\superscript{***}}	&	0.602\textnormal{\superscript{***}}	&	0.571\textnormal{\superscript{***}}	&	0.631\textnormal{\superscript{***}}	&	0.770\textnormal{\superscript{***}}	\\
DNS(2)	&	0.707\textnormal{\superscript{***}}	&	0.605\textnormal{\superscript{***}}	&	0.576\textnormal{\superscript{***}}	&	0.638\textnormal{\superscript{***}}	&	0.771\textnormal{\superscript{***}}	\\
DNS(3)	&	0.702\textnormal{\superscript{***}}	&	0.599\textnormal{\superscript{***}}	&	0.570\textnormal{\superscript{***}}	&	0.639\textnormal{\superscript{***}}	&	0.811\textnormal{\superscript{***}}	\\
DNS(4)	&	0.593\textnormal{\superscript{***}}	&	0.507\textnormal{\superscript{***}}	&	0.460\textnormal{\superscript{***}}	&	0.454\textnormal{\superscript{***}}	&	0.564\textnormal{\superscript{***}}	\\
DNS(5)	&	0.593\textnormal{\superscript{***}}	&	0.506\textnormal{\superscript{***}}	&	0.459\textnormal{\superscript{***}}	&	0.456\textnormal{\superscript{***}}	&	0.564\textnormal{\superscript{***}}	\\
DNS(6)	&	0.599\textnormal{\superscript{***}}	&	0.510\textnormal{\superscript{***}}	&	0.460\textnormal{\superscript{***}}	&	0.452\textnormal{\superscript{***}}	&	0.563\textnormal{\superscript{***}}	\\
DNS(1)+FB1	&	0.548\textnormal{\superscript{***}}	&	0.507\textnormal{\superscript{***}}	&	0.520\textnormal{\superscript{***}}	&	0.658\textnormal{\superscript{***}}	&	0.999	\\
DNS(2)+FB1	&	0.547\textnormal{\superscript{***}}	&	0.508\textnormal{\superscript{***}}	&	0.523\textnormal{\superscript{***}}	&	0.662\textnormal{\superscript{***}}	&	0.996	\\
DNS(3)+FB1	&	\bfseries 0.543\textnormal{\superscript{***}}	&	0.502\textnormal{\superscript{***}}	&	0.516\textnormal{\superscript{***}}	&	0.661\textnormal{\superscript{***}}	&	1.036	\\
DNS(4)+FB1	&	0.595\textnormal{\superscript{***}}	&	0.509\textnormal{\superscript{***}}	&	0.461\textnormal{\superscript{***}}	&	0.455\textnormal{\superscript{***}}	&	0.564\textnormal{\superscript{***}}	\\
DNS(5)+FB1	&	0.595\textnormal{\superscript{***}}	&	0.508\textnormal{\superscript{***}}	&	0.461\textnormal{\superscript{***}}	&	0.457\textnormal{\superscript{***}}	&	0.565\textnormal{\superscript{***}}	\\
DNS(6)+FB1	&	0.598\textnormal{\superscript{***}}	&	0.511\textnormal{\superscript{***}}	&	0.461\textnormal{\superscript{***}}	&	0.452\textnormal{\superscript{***}}	&	0.562\textnormal{\superscript{***}}	\\
DNS(1)+FB2	&	1.023	&	1.049	&	1.143	&	1.530	&	2.724	\\
DNS(2)+FB2	&	1.023	&	1.055	&	1.152	&	1.541	&	2.725	\\
DNS(3)+FB2	&	1.010	&	1.036	&	1.130	&	1.521	&	2.739	\\
DNS(4)+FB2	&	0.594\textnormal{\superscript{***}}	&	0.510\textnormal{\superscript{***}}	&	0.464\textnormal{\superscript{***}}	&	0.460\textnormal{\superscript{***}}	&	0.574\textnormal{\superscript{***}}	\\
DNS(5)+FB2	&	0.594\textnormal{\superscript{***}}	&	0.510\textnormal{\superscript{***}}	&	0.464\textnormal{\superscript{***}}	&	0.462\textnormal{\superscript{***}}	&	0.575\textnormal{\superscript{***}}	\\
DNS(6)+FB2	&	0.597\textnormal{\superscript{***}}	&	0.511\textnormal{\superscript{***}}	&	0.463\textnormal{\superscript{***}}	&	0.456\textnormal{\superscript{***}}	&	0.571\textnormal{\superscript{***}}	\\
DNS(1)+MAC	&	0.671\textnormal{\superscript{***}}	&	0.609\textnormal{\superscript{***}}	&	0.604\textnormal{\superscript{***}}	&	0.700\textnormal{\superscript{***}}	&	0.884\textnormal{\superscript{***}}	\\
DNS(2)+MAC	&	0.669\textnormal{\superscript{***}}	&	0.611\textnormal{\superscript{***}}	&	0.609\textnormal{\superscript{***}}	&	0.706\textnormal{\superscript{***}}	&	0.884\textnormal{\superscript{***}}	\\
DNS(3)+MAC	&	0.665\textnormal{\superscript{***}}	&	0.606\textnormal{\superscript{***}}	&	0.604\textnormal{\superscript{***}}	&	0.708\textnormal{\superscript{***}}	&	0.922\textnormal{\superscript{**}}	\\
DNS(4)+MAC	&	0.579\textnormal{\superscript{***}}	&	0.495\textnormal{\superscript{***}}	&	0.451\textnormal{\superscript{***}}	&	0.449\textnormal{\superscript{***}}	&	0.567\textnormal{\superscript{***}}	\\
DNS(5)+MAC	&	0.580\textnormal{\superscript{***}}	&	0.495\textnormal{\superscript{***}}	&	0.450\textnormal{\superscript{***}}	&	0.451\textnormal{\superscript{***}}	&	0.566\textnormal{\superscript{***}}	\\
DNS(6)+MAC	&	0.583\textnormal{\superscript{***}}	&	0.497\textnormal{\superscript{***}}	&	0.450\textnormal{\superscript{***}}	&	0.447\textnormal{\superscript{***}}	&	0.568\textnormal{\superscript{***}}	\\
DIF(1)	&	1.244	&	1.157	&	1.124	&	1.142	&	1.537	\\
DIF(2)	&	1.895	&	1.456	&	1.321	&	1.372	&	1.801	\\
DIF(3)	&	2.284	&	1.721	&	1.565	&	1.569	&	1.959	\\
DIF(4)	&	0.842\textnormal{\superscript{***}}	&	0.966	&	1.112	&	1.448	&	1.910	\\
DIF(5)	&	1.019	&	1.109	&	1.243	&	1.765	&	3.530	\\
DIF(6)	&	1.037	&	1.146	&	1.299	&	1.857	&	3.729	\\
DIF(1)+FB1	&	1.001	&	1.056	&	1.129	&	1.539	&	2.403	\\
DIF(2)+FB1	&	1.544	&	1.364	&	1.412	&	1.711	&	2.703	\\
DIF(3)+FB1	&	1.798	&	1.664	&	1.720	&	2.028	&	2.985	\\
DIF(1)+FB2	&	1.032	&	1.138	&	1.272	&	1.842	&	3.941	\\
DIF(2)+FB2	&	1.850	&	1.804	&	1.980	&	2.589	&	4.305	\\
DIF(3)+FB2	&	2.098	&	2.053	&	2.226	&	2.832	&	4.471	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 4C: 12-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
3: 2008:1-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.737\textnormal{\superscript{***}}	&	0.716\textnormal{\superscript{***}}	&	0.695\textnormal{\superscript{***}}	&	\bfseries 0.663\textnormal{\superscript{***}}	&	0.709\textnormal{\superscript{***}}	\\
VAR(1)+FB1	&	0.778\textnormal{\superscript{***}}	&	0.756\textnormal{\superscript{***}}	&	0.731\textnormal{\superscript{***}}	&	0.686\textnormal{\superscript{***}}	&	0.707\textnormal{\superscript{***}}	\\
VAR(1)+FB2	&	0.796\textnormal{\superscript{***}}	&	0.773\textnormal{\superscript{***}}	&	0.747\textnormal{\superscript{***}}	&	0.697\textnormal{\superscript{***}}	&	0.713\textnormal{\superscript{***}}	\\
AR(SIC)	&	\bfseries 0.729\textnormal{\superscript{***}}	&	0.714\textnormal{\superscript{***}}	&	\bfseries 0.687\textnormal{\superscript{***}}	&	0.674\textnormal{\superscript{***}}	&	0.737\textnormal{\superscript{***}}	\\
VAR(SIC)	&	0.737\textnormal{\superscript{***}}	&	0.716\textnormal{\superscript{***}}	&	0.695\textnormal{\superscript{***}}	&	0.663\textnormal{\superscript{***}}	&	0.709\textnormal{\superscript{***}}	\\
VAR(SIC)+FB1	&	0.778\textnormal{\superscript{***}}	&	0.756\textnormal{\superscript{***}}	&	0.731\textnormal{\superscript{***}}	&	0.686\textnormal{\superscript{***}}	&	0.707\textnormal{\superscript{***}}	\\
VAR(SIC)+FB2	&	0.796\textnormal{\superscript{***}}	&	0.773\textnormal{\superscript{***}}	&	0.747\textnormal{\superscript{***}}	&	0.697\textnormal{\superscript{***}}	&	0.713\textnormal{\superscript{***}}	\\
DNS(1)	&	1.468	&	1.389	&	1.483	&	1.559	&	1.118	\\
DNS(2)	&	1.451	&	1.412	&	1.520	&	1.591	&	1.118	\\
DNS(3)	&	1.423	&	1.356	&	1.467	&	1.583	&	1.179	\\
DNS(4)	&	0.765\textnormal{\superscript{***}}	&	0.689\textnormal{\superscript{***}}	&	0.697\textnormal{\superscript{***}}	&	0.737\textnormal{\superscript{***}}	&	0.687\textnormal{\superscript{***}}	\\
DNS(5)	&	0.759\textnormal{\superscript{***}}	&	0.694\textnormal{\superscript{***}}	&	0.708\textnormal{\superscript{***}}	&	0.748\textnormal{\superscript{***}}	&	\bfseries 0.683\textnormal{\superscript{***}}	\\
DNS(6)	&	0.747\textnormal{\superscript{***}}	&	0.679\textnormal{\superscript{***}}	&	0.692\textnormal{\superscript{***}}	&	0.747\textnormal{\superscript{***}}	&	0.706\textnormal{\superscript{***}}	\\
DNS(1)+FB1	&	1.744	&	1.554	&	1.511	&	1.534	&	1.209	\\
DNS(2)+FB1	&	1.734	&	1.557	&	1.524	&	1.552	&	1.206	\\
DNS(3)+FB1	&	1.736	&	1.542	&	1.502	&	1.554	&	1.274	\\
DNS(4)+FB1	&	0.813\textnormal{\superscript{***}}	&	0.730\textnormal{\superscript{***}}	&	0.737\textnormal{\superscript{***}}	&	0.771\textnormal{\superscript{***}}	&	0.703\textnormal{\superscript{***}}	\\
DNS(5)+FB1	&	0.804\textnormal{\superscript{***}}	&	0.735\textnormal{\superscript{***}}	&	0.746\textnormal{\superscript{***}}	&	0.781\textnormal{\superscript{***}}	&	0.699\textnormal{\superscript{***}}	\\
DNS(6)+FB1	&	0.791\textnormal{\superscript{***}}	&	0.718\textnormal{\superscript{***}}	&	0.730\textnormal{\superscript{***}}	&	0.780\textnormal{\superscript{***}}	&	0.723\textnormal{\superscript{***}}	\\
DNS(1)+FB2	&	1.895	&	1.779	&	1.792	&	1.842	&	1.427	\\
DNS(2)+FB2	&	1.887	&	1.785	&	1.809	&	1.863	&	1.425	\\
DNS(3)+FB2	&	1.877	&	1.757	&	1.771	&	1.851	&	1.486	\\
DNS(4)+FB2	&	0.827\textnormal{\superscript{***}}	&	0.742\textnormal{\superscript{***}}	&	0.747\textnormal{\superscript{***}}	&	0.777\textnormal{\superscript{***}}	&	0.703\textnormal{\superscript{***}}	\\
DNS(5)+FB2	&	0.816\textnormal{\superscript{***}}	&	0.745\textnormal{\superscript{***}}	&	0.755\textnormal{\superscript{***}}	&	0.786\textnormal{\superscript{***}}	&	0.699\textnormal{\superscript{***}}	\\
DNS(6)+FB2	&	0.804\textnormal{\superscript{***}}	&	0.730\textnormal{\superscript{***}}	&	0.740\textnormal{\superscript{***}}	&	0.786\textnormal{\superscript{***}}	&	0.723\textnormal{\superscript{***}}	\\
DNS(1)+MAC	&	1.579	&	1.583	&	1.729	&	1.824	&	1.274	\\
DNS(2)+MAC	&	1.555	&	1.600	&	1.761	&	1.850	&	1.269	\\
DNS(3)+MAC	&	1.543	&	1.557	&	1.723	&	1.865	&	1.353	\\
DNS(4)+MAC	&	0.758\textnormal{\superscript{***}}	&	0.685\textnormal{\superscript{***}}	&	0.695\textnormal{\superscript{***}}	&	0.739\textnormal{\superscript{***}}	&	0.694\textnormal{\superscript{***}}	\\
DNS(5)+MAC	&	0.751\textnormal{\superscript{***}}	&	0.690\textnormal{\superscript{***}}	&	0.705\textnormal{\superscript{***}}	&	0.749\textnormal{\superscript{***}}	&	0.691\textnormal{\superscript{***}}	\\
DNS(6)+MAC	&	0.739\textnormal{\superscript{***}}	&	\bfseries 0.675\textnormal{\superscript{***}}	&	0.689\textnormal{\superscript{***}}	&	0.747\textnormal{\superscript{***}}	&	0.711\textnormal{\superscript{***}}	\\
DIF(1)	&	1.190	&	1.280	&	1.277	&	1.357	&	1.155	\\
DIF(2)	&	2.405	&	2.453	&	2.228	&	1.789	&	1.155	\\
DIF(3)	&	2.787	&	2.711	&	2.641	&	2.260	&	1.472	\\
DIF(4)	&	1.978	&	1.719	&	1.539	&	1.291	&	1.086	\\
DIF(5)	&	1.859	&	1.677	&	1.611	&	1.564	&	1.457	\\
DIF(6)	&	2.332	&	2.278	&	2.257	&	2.110	&	1.711	\\
DIF(1)+FB1	&	2.036	&	1.789	&	1.563	&	1.316	&	1.167	\\
DIF(2)+FB1	&	2.280	&	2.164	&	1.982	&	1.663	&	1.171	\\
DIF(3)+FB1	&	2.490	&	2.487	&	2.410	&	2.139	&	1.569	\\
DIF(1)+FB2	&	1.886	&	1.726	&	1.614	&	1.618	&	1.462	\\
DIF(2)+FB2	&	2.289	&	2.228	&	2.123	&	1.907	&	1.430	\\
DIF(3)+FB2	&	2.503	&	2.530	&	2.505	&	2.297	&	1.737	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 4D: 12-Step-Ahead Relative MSFEs of All Forecasting Models (Subsample
4: 1994:1-2016:7) \textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 2,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{***}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{lSSSSS}
\hline\hline
Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
AR(1)	&	1.000	&	1.000	&	1.000	&	1.000	&	1.000	\\
VAR(1)	&	0.706\textnormal{\superscript{***}}	&	0.660\textnormal{\superscript{***}}	&	0.643\textnormal{\superscript{***}}	&	0.665\textnormal{\superscript{***}}	&	0.767\textnormal{\superscript{***}}	\\
VAR(1)+FB1	&	0.724\textnormal{\superscript{***}}	&	0.674\textnormal{\superscript{***}}	&	0.655\textnormal{\superscript{***}}	&	0.673\textnormal{\superscript{***}}	&	0.763\textnormal{\superscript{***}}	\\
VAR(1)+FB2	&	0.730\textnormal{\superscript{***}}	&	0.680\textnormal{\superscript{***}} &	0.661\textnormal{\superscript{***}}	&	0.680\textnormal{\superscript{***}}	&	0.771\textnormal{\superscript{***}}	\\
AR(SIC)	&\bfseries	0.695\textnormal{\superscript{***}}	&\bfseries	0.652\textnormal{\superscript{***}}	&\bfseries	0.635\textnormal{\superscript{***}}	&\bfseries	0.657\textnormal{\superscript{***}}	&\bfseries	0.748\textnormal{\superscript{***}}	\\
VAR(SIC)	&	0.706\textnormal{\superscript{***}}	&	0.660\textnormal{\superscript{***}}	&	0.643\textnormal{\superscript{***}}	&	0.665\textnormal{\superscript{***}}	&	0.767\textnormal{\superscript{***}}	\\
VAR(SIC)+FB1	&	0.724\textnormal{\superscript{***}}	&	0.674\textnormal{\superscript{***}}	&	0.655\textnormal{\superscript{***}}	&	0.673\textnormal{\superscript{***}}	&	0.763\textnormal{\superscript{***}}	\\
VAR(SIC)+FB2	&	0.730\textnormal{\superscript{***}} &	0.680\textnormal{\superscript{***}}	&	0.661\textnormal{\superscript{***}}	&	0.680\textnormal{\superscript{***}}	&	0.771\textnormal{\superscript{***}}	\\
DNS(1)	&	0.879\textnormal{\superscript{**}}	&	0.784\textnormal{\superscript{***}}	&	0.794\textnormal{\superscript{***}}	&	0.910\textnormal{\superscript{**}}	&	0.953	\\
DNS(2)	&	0.875\textnormal{\superscript{**}}	&	0.792\textnormal{\superscript{***}}	&	0.806\textnormal{\superscript{***}}	&	0.923\textnormal{\superscript{*}}	&	0.954\textnormal{\superscript{*}}	\\
DNS(3)	&	0.863\textnormal{\superscript{***}}	&	0.773\textnormal{\superscript{***}}	&	0.788\textnormal{\superscript{***}}	&	0.919\textnormal{\superscript{*}}	&	0.994	\\
DNS(4)	&	0.725\textnormal{\superscript{***}}	&	0.673\textnormal{\superscript{***}}	&	0.656\textnormal{\superscript{***}}	&	0.686\textnormal{\superscript{***}}	&	0.753\textnormal{\superscript{***}}	\\
DNS(5)	&	0.724\textnormal{\superscript{***}}	&	0.674\textnormal{\superscript{***}}	&	0.658\textnormal{\superscript{***}}	&	0.690\textnormal{\superscript{***}}	&	0.753\textnormal{\superscript{***}}	\\
DNS(6)	&	0.725\textnormal{\superscript{***}}	&	0.674\textnormal{\superscript{***}}	&	0.655\textnormal{\superscript{***}}	&	0.685\textnormal{\superscript{***}}	&	0.756\textnormal{\superscript{***}}	\\
DNS(1)+FB1	&	0.880	&	0.794\textnormal{\superscript{**}}	&	0.810\textnormal{\superscript{**}}	&	0.975	&	1.125	\\
DNS(2)+FB1	&	0.876	&	0.796\textnormal{\superscript{**}}	&	0.816\textnormal{\superscript{**}}	&	0.983	&	1.122	\\
DNS(3)+FB1	&	0.873	&	0.787\textnormal{\superscript{**}}	&	0.805\textnormal{\superscript{**}}	&	0.985	&	1.173	\\
DNS(4)+FB1	&	0.730\textnormal{\superscript{***}}	&	0.678\textnormal{\superscript{***}}	&	0.661\textnormal{\superscript{***}}	&	0.692\textnormal{\superscript{***}}	&	0.759\textnormal{\superscript{***}}	\\
DNS(5)+FB1	&	0.729\textnormal{\superscript{***}}	&	0.679\textnormal{\superscript{***}}	&	0.663\textnormal{\superscript{***}}	&	0.696\textnormal{\superscript{***}}	&	0.759\textnormal{\superscript{***}}	\\
DNS(6)+FB1	&	0.729\textnormal{\superscript{***}}	&	0.677\textnormal{\superscript{***}}	&	0.659\textnormal{\superscript{***}}	&	0.692\textnormal{\superscript{***}}	&	0.764\textnormal{\superscript{***}}	\\
DNS(1)+FB2	&	1.234	&	1.195	&	1.264	&	1.539	&	1.807	\\
DNS(2)+FB2	&	1.232	&	1.201	&	1.274	&	1.551	&	1.806	\\
DNS(3)+FB2	&	1.222	&	1.183	&	1.254	&	1.543	&	1.850	\\
DNS(4)+FB2	&	0.731\textnormal{\superscript{***}}	&	0.679\textnormal{\superscript{***}}	&	0.663\textnormal{\superscript{***}}	&	0.695\textnormal{\superscript{***}}	&	0.761\textnormal{\superscript{***}}	\\
DNS(5)+FB2	&	0.730\textnormal{\superscript{***}}	&	0.680\textnormal{\superscript{***}}	&	0.665\textnormal{\superscript{***}} &	0.699\textnormal{\superscript{***}}	&	0.760\textnormal{\superscript{***}}	\\
DNS(6)+FB2	&	0.729\textnormal{\superscript{***}}	&	0.678\textnormal{\superscript{***}}	&	0.661\textnormal{\superscript{***}}	&	0.695\textnormal{\superscript{***}}	&	0.766\textnormal{\superscript{***}}	\\
DNS(1)+MAC	&	0.889\textnormal{\superscript{*}}	&	0.838\textnormal{\superscript{**}}	&	0.875\textnormal{\superscript{**}}	&	1.022	&	1.054	\\
DNS(2)+MAC	&	0.882\textnormal{\superscript{**}}	&	0.844\textnormal{\superscript{**}}	&	0.886\textnormal{\superscript{*}}	&	1.032	&	1.052	\\
DNS(3)+MAC	&	0.875\textnormal{\superscript{**}}	&	0.828\textnormal{\superscript{***}}	&	0.872\textnormal{\superscript{**}}	&	1.036	&	1.101	\\
DNS(4)+MAC	&	0.708\textnormal{\superscript{***}}	&	0.662\textnormal{\superscript{***}}	&	0.649\textnormal{\superscript{***}}	&	0.687\textnormal{\superscript{***}}	&	0.765\textnormal{\superscript{***}}	\\
DNS(5)+MAC	&	0.707\textnormal{\superscript{***}}	&	0.662\textnormal{\superscript{***}}	&	0.651\textnormal{\superscript{***}}	&	0.690\textnormal{\superscript{***}}	&	0.764\textnormal{\superscript{***}}	\\
DNS(6)+MAC	&	0.707\textnormal{\superscript{***}}	&	0.661\textnormal{\superscript{***}}	&	0.647\textnormal{\superscript{***}}	&	0.687\textnormal{\superscript{***}}	&	0.770\textnormal{\superscript{***}}	\\
DIF(1)	&	1.197	&	1.146	&	1.104	&	1.191	&	1.492	\\
DIF(2)	&	1.941	&	1.653	&	1.546	&	1.561	&	1.602	\\
DIF(3)	&	2.267	&	1.854	&	1.755	&	1.744	&	1.738	\\
DIF(4)	&	1.149	&	1.159	&	1.216	&	1.358	&	1.419	\\
DIF(5)	&	1.299	&	1.312	&	1.395	&	1.698	&	2.204	\\
DIF(6)	&	1.435	&	1.478	&	1.581	&	1.895	&	2.367	\\
DIF(1)+FB1	&	1.292	&	1.250	&	1.227	&	1.445	&	1.871	\\
DIF(2)+FB1	&	1.729	&	1.598	&	1.617	&	1.777	&	1.984	\\
DIF(3)+FB1	&	1.917	&	1.821	&	1.855	&	2.015	&	2.192	\\
DIF(1)+FB2	&	1.295	&	1.323	&	1.395	&	1.813	&	2.540	\\
DIF(2)+FB2	&	1.959	&	1.912	&	2.014	&	2.324	&	2.627	\\
DIF(3)+FB2	&	2.127	&	2.088	&	2.199	&	2.502	&	2.764	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A.
\end{minipage}

\newpage

Table 5: Top 3 Forecast Models with Lowest MSFE\textnormal{\superscript{*}}

\vspace{0.3cm}

\renewcommand{\arraystretch}{1}

\begin{adjustbox}{width=0.95\columnwidth}
\begin{tabular}{ccp{3cm}p{3cm}p{3cm}p{3cm}p{3cm}}
\hline \hline
& \multicolumn{1}{l}{Maturity} & \multicolumn{1}{l}{3 Months} & \multicolumn{1}{l}{1 Year} & \multicolumn{1}{l}{3 Years} & \multicolumn{1}{l}{5 Years} & \multicolumn{1}{l}{10 Years} 	\\\midrule
\multicolumn{1}{c}{Forecast Sample} & \multicolumn{1}{c}{Horizon} &  &  &  &  	\\\midrule
	&		&	\bfseries	DNS(6)+FB2\textnormal{\superscript{***}}	&	\bfseries	DNS(5)+FB2\textnormal{\superscript{***}}	&	\bfseries	DNS(5)+FB2\textnormal{\superscript{***}}	&	\bfseries	DIF(1)\textnormal{\superscript{**}}	&	\bfseries	AR(SIC)\textnormal{\superscript{**}}	\\
	&	\multicolumn{1}{c}{1 Step}	&	DNS(6)+FB1\textnormal{\superscript{***}}	&	DNS(4)+FB2\textnormal{\superscript{***}}	&	DNS(4)+FB2\textnormal{\superscript{***}}	&	DNS(4)+FB2\textnormal{\superscript{**}}	&	DNS(2)+FB1\textnormal{\superscript{*}}	\\
	&		&	DNS(4)+FB1\textnormal{\superscript{***}}	&	DNS(5)+FB1\textnormal{\superscript{***}}	&	DNS(6)+FB2\textnormal{\superscript{***}}	&	DNS(4)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB1\textnormal{\superscript{*}}	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(4)+FB1\textnormal{\superscript{***}}	&	\bfseries	DNS(3)+FB1\textnormal{\superscript{**}}	&	\bfseries	DNS(1)+FB1\textnormal{\superscript{*}}	&	\bfseries	DNS(6)+FB1\textnormal{\superscript{**}}	&	\bfseries	AR(SIC)\textnormal{\superscript{**}}	\\
	1992:3-1999:12	&	\multicolumn{1}{c}{3 Step}	&	DNS(6)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB1\textnormal{\superscript{**}}	&	DNS(3)+FB1\textnormal{\superscript{*}}	&	DNS(5)+FB1\textnormal{\superscript{**}}	&	DNS(6)+FB1\textnormal{\superscript{*}}	\\
	\small{`1st Subsample}	&		&	DNS(5)+FB1\textnormal{\superscript{***}}	&	DNS(2)+FB1\textnormal{\superscript{**}}	&	DNS(2)+FB1\textnormal{\superscript{*}}	&	DNS(4)+FB1\textnormal{\superscript{**}}	&	DNS(5)+FB1\textnormal{\superscript{**}}	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(3)\textnormal{\superscript{***}}	&	\bfseries	DNS(3)\textnormal{\superscript{***}}	&	\bfseries	DNS(3)\textnormal{\superscript{***}}	&	\bfseries	DNS(3)	&	\bfseries	DNS(1)	\\
	&	\multicolumn{1}{c}{12 Step}	&	DNS(1)	&	DNS(1)	&	DNS(1)	&	DNS(1)	&	DNS(2)	\\
	&		&	DNS(2)	&	DNS(2)	&	DNS(2)	&	DNS(2)	&	AR(SIC)	\\\midrule
	&		&	\bfseries	DNS(3)+FB2\textnormal{\superscript{***}}	&	\bfseries	DNS(2)+FB2\textnormal{\superscript{**}}	&	\bfseries	DNS(1)+FB2\textnormal{\superscript{**}}	&	\bfseries	DNS(1)+FB2\textnormal{\superscript{***}}	&	\bfseries	DNS(2)+FB2\textnormal{\superscript{**}}	\\
	&	\multicolumn{1}{c}{1 Step}	&	DNS(5)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB2\textnormal{\superscript{**}}	&	DNS(3)+FB2\textnormal{\superscript{**}}	&	DNS(2)+FB2\textnormal{\superscript{***}}	&	DNS(1)+FB2\textnormal{\superscript{**}}	\\
	&		&	DNS(6)+FB2\textnormal{\superscript{***}}	&	DNS(3)+FB2\textnormal{\superscript{**}}	&	DNS(2)+FB2\textnormal{\superscript{***}}	&	DNS(3)+FB2\textnormal{\superscript{***}}	&	DNS(3)+FB2\textnormal{\superscript{**}}	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(3)+FB1\textnormal{\superscript{***}}	&	\bfseries	DNS(3)+FB1\textnormal{\superscript{***}}	&	\bfseries	DNS(3)+FB1\textnormal{\superscript{***}}	&	\bfseries	DNS(1)+FB1\textnormal{\superscript{***}}	&	\bfseries	DNS(2)+FB1	\\
	2000:1-2007:12	&	\multicolumn{1}{c}{3 Step}	&	DNS(2)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB1\textnormal{\superscript{***}}	&	DNS(2)+FB1\textnormal{\superscript{***}}	&	DNS(1)+FB1	\\
	\small{`2nd Subsample}	&		&	DNS(1)+FB1\textnormal{\superscript{***}}	&	DNS(2)+FB1\textnormal{\superscript{***}}	&	DNS(2)+FB1\textnormal{\superscript{***}}	&	DNS(3)+FB1\textnormal{\superscript{***}}	&	DNS(3)+FB1\textnormal{\superscript{*}}	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(3)+FB1\textnormal{\superscript{**}}	&	\bfseries	VAR(1)\textnormal{\superscript{**}}	&	\bfseries	VAR(1)\textnormal{\superscript{***}}	&	\bfseries	VAR(1)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	\\
	&	\multicolumn{1}{c}{12 Step}	&	DNS(2)+FB1\textnormal{\superscript{**}}	&	VAR(SIC)\textnormal{\superscript{**}}	&	VAR(SIC)\textnormal{\superscript{***}}	&	VAR(SIC)\textnormal{\superscript{***}}	&	VAR(SIC)\textnormal{\superscript{***}}	\\
	&		&	DNS(1)+FB1\textnormal{\superscript{**}}	&	AR(SIC)\textnormal{\superscript{**}}	&	VAR(SIC)+FB1\textnormal{\superscript{***}}	&	VAR(SIC)+FB1\textnormal{\superscript{***}}	&	VAR(1)\textnormal{\superscript{***}}	\\\midrule
	&		&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(1)	&	\bfseries	AR(1)\textnormal{\superscript{**}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	DNS(2)	\\
	&	\multicolumn{1}{c}{1 Step}	&	AR(1)\textnormal{\superscript{***}}	&	DNS(3)\textnormal{\superscript{*}}	&	AR(SIC)\textnormal{\superscript{**}}	&	AR(1)\textnormal{\superscript{***}}	&	DNS(1)	\\
	&		&	DIF(3)\textnormal{\superscript{***}}	&	AR(SIC)	&	DNS(3)+MAC	&	DIF(1)+FB2\textnormal{\superscript{*}}	&	DNS(2)+MAC	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(6)+FB1\textnormal{\superscript{*}}	&	\bfseries	DNS(5)+FB1	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	DNS(5)+MAC	\\
	2008:1-2016:7	&	\multicolumn{1}{c}{3 Step}	&	VAR(1)+FB1\textnormal{\superscript{***}}	&	DNS(4)+FB1	&	DNS(6)+FB1	&	VAR(1)+FB1\textnormal{\superscript{***}}	&	DNS(5)\textnormal{\superscript{*}}	\\
	\small{`3rd Subsample}	&		&	VAR(SIC)+FB1\textnormal{\superscript{***}}	&	DNS(6)+FB1	&	DNS(4)+FB1	&	VAR(SIC)+FB1\textnormal{\superscript{***}}	&	DNS(4)+MAC	\\\cmidrule{2-7}
	&		&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	DNS(6)+MAC	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	VAR(1)\textnormal{\superscript{***}}	&	\bfseries	DNS(5)\textnormal{\superscript{***}}	\\
	&	\multicolumn{1}{c}{12 Step}	&	VAR(1)\textnormal{\superscript{***}}	&	DNS(6)\textnormal{\superscript{***}}	&	DNS(6)+MAC	&	VAR(SIC)\textnormal{\superscript{***}}	&	DNS(4)\textnormal{\superscript{***}}	\\
	&		&	VAR(SIC)\textnormal{\superscript{***}}	&	DNS(4)+MAC	&	DNS(6)\textnormal{\superscript{***}}	&	AR(SIC)\textnormal{\superscript{***}}	&	DNS(5)+MAC	\\\midrule
	&		&	\bfseries	DNS(6)+FB1	&	\bfseries	DNS(5)+FB1	&	\bfseries	DNS(4)+FB1	&	\bfseries	AR(1)\textnormal{\superscript{***}}	&	\bfseries	DNS(2)	\\
	&	\multicolumn{1}{c}{1 Step}	&	VAR(SIC)+FB1\textnormal{\superscript{**}}	&	DNS(5)+FB2	&	DNS(5)+FB1	&	AR(SIC)\textnormal{\superscript{***}}	&	DNS(1)	\\
	&		&	VAR(1)+FB1\textnormal{\superscript{**}}	&	DNS(4)+FB1	&	DNS(6)+FB1	&	VAR(SIC)+FB1	&	DNS(2)+MAC	\\\cmidrule{2-7}
	&		&	\bfseries	DNS(6)+FB1\textnormal{\superscript{*}}	&	\bfseries	DNS(5)+FB1	&	\bfseries	DNS(6)+FB1	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	DNS(5)+FB1	\\
	1992:3-2016:7	&	\multicolumn{1}{c}{3 Step}	&	DNS(5)+FB1\textnormal{\superscript{*}}	&	DNS(4)+FB1	&	DNS(4)+FB1	&	DNS(4)+FB1	&	AR(SIC)	\\
	\small{`Whole Sample}	&		&	DNS(4)+FB1\textnormal{\superscript{*}}	&	DNS(6)+FB1	&	DNS(5)+FB1	&	DNS(6)+FB1	&	DNS(4)+FB1	\\\cmidrule{2-7}
	&		&\bfseries		AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	&	\bfseries	AR(SIC)\textnormal{\superscript{***}}	\\
	&	\multicolumn{1}{c}{12 Step}	&	VAR(1)\textnormal{\superscript{***}}	&	VAR(1)\textnormal{\superscript{***}}	&	VAR(1)\textnormal{\superscript{***}}	&	VAR(1)\textnormal{\superscript{***}}	&	DNS(5)\textnormal{\superscript{***}}	\\
	&		&	VAR(SIC)\textnormal{\superscript{***}}	&	VAR(SIC)\textnormal{\superscript{***}}	&	VAR(SIC)\textnormal{\superscript{***}}	&	VAR(SIC)\textnormal{\superscript{***}}	&	DNS(4)\textnormal{\superscript{***}}	\\
\hline\hline
\end{tabular}
\end{adjustbox}	

\begin{minipage}{1.05\columnwidth}

\hspace{0.1in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A. This table reports the top three performing forecast models (based on MSFE) from lowest-MSFE to highest-MSFE, 
for all subsamples, horizons, and maturities, summarizing the results of Tables 2A-4D. Entries in bold denote models with lowest MSFE for a given maturity. 
Entries superscripted with \textsuperscript{***}, \textsuperscript{**}, 
and \textsuperscript{*} denote rejections of the null hypothesis of equal predictive accuracy at 0.01, 0.05, and 0.10 significance levels, respectively, 
based on application of the Diebold-Mariano test discussed in Section 3; and indicate that the listed model is predictively superior to a 
``benchmark'' DNS($\tau$) model, based on MSFE loss. In particular, if the point ``MSFE-best'' model is 
DNS($\tau$)+$mod$, where $mod$ denotes another component of the model (for example, $mod$ may be FB1 or FB2, etc.) 
then the ``benchmark'' model is DNS($\tau$). If the point ``MSFE-best'' model is 
DNS(1), or if no DNS component appears in point ``MSFE-best'' model, then DNS(1) is the ``benchmark'' model.
Finally, for entries denoted ``DNS(1)'', no predictive accuracy test was carried out. These test results are included to highlight the importance 
of incorporating \textquotedblleft big data\textquotedblright\ in DNS type prediction models.
For complete details, refer to Section 4.
\end{minipage}

\newpage

Table 6: Top 3 Forecast Models with Lowest MSFE in Expansionary and
Recessionary Periods\textnormal{\superscript{*}}

\vspace{0.3cm}

\renewcommand{\arraystretch}{1}

\begin{adjustbox}{width=0.95\columnwidth}
\begin{tabular}{ccp{3cm}p{3cm}p{3cm}p{3cm}p{3cm}}
\hline \hline
& \multicolumn{1}{l}{Maturity} & \multicolumn{1}{l}{3 Months} & \multicolumn{1}{l}{1 Year} & \multicolumn{1}{l}{3 Years} & \multicolumn{1}{l}{5 Years} & \multicolumn{1}{l}{10 Years} 	\\\midrule
\multicolumn{1}{c}{Forecast Sample} & \multicolumn{1}{c}{Horizon} &  &  &  &  	\\\midrule
	&						&\bfseries	DNS(4)+FB1	&\bfseries	VAR(SIC)+FB1	&\bfseries	VAR(1)	&\bfseries	DNS(3)+FB2	&\bfseries	DIF(2)+FB2	\\
	&	\multicolumn{1}{c}{1 Step}	&	DNS(5)+FB1	&	VAR(1)+FB1	&	VAR(SIC)	&	DNS(2)+FB2	&	DNS(3)	\\
	&								&	VAR(SIC)+FB1	&	DNS(2)+MAC	&	DNS(1)+MAC	&	VAR(SIC)	&	DNS(2)	\\\cmidrule{2-7}
	&						&\bfseries	DNS(6)+FB1	&\bfseries	DNS(6)+FB1	&	\bfseries DNS(6)+FB1	&\bfseries	DNS(2)+FB1	&\bfseries	DNS(3)+FB1	\\
Recession&	\multicolumn{1}{c}{3 Step}	&	DNS(6)+MAC	&	DNS(4)+FB1	&	DNS(4)+FB1	&	DNS(3)+FB1	&	DNS(2)	\\
	&					&	DNS(1)+FB1	&	DNS(6)+MAC	&	DNS(5)+FB1	&	DNS(1)+FB1	&	DNS(1)	\\\cmidrule{2-7}
	&					&\bfseries	DNS(3)+FB1	&\bfseries	DNS(3)+FB1	&\bfseries	DNS(3)+FB1	&\bfseries	DNS(1)+FB1	&\bfseries	VAR(1)	\\
	&	\multicolumn{1}{c}{12 Step}	&	DNS(2)+FB1	&	DNS(1)+FB1	&	DNS(1)+FB1	&	DNS(2)+FB1	&	VAR(SIC)	\\
	&				&	DNS(1)+FB1	&	DNS(2)+FB1	&	DNS(2)+FB1	&	DNS(3)+FB1	&	DNS(5)	\\\midrule
	&		&\bfseries	DNS(6)+FB2	&\bfseries	DNS(5)+FB2	&\bfseries	DNS(6)+FB2	&\bfseries	AR(1)	&\bfseries	DNS(2)+FB2	\\
	&	\multicolumn{1}{c}{1 Step}	&	DNS(6)+FB1	&	DNS(4)+FB2	&	DNS(4)+FB2	&	AR(SIC)	&	DNS(1)+FB2	\\
	&		&	VAR(1)+FB1	&	DNS(3)+FB2	&	DNS(5)+FB2	&	DIF(1)	&	DNS(2)+FB1	\\\cmidrule{2-7}
	&		&\bfseries	DNS(5)+FB1	&\bfseries	DNS(5)+FB1	&\bfseries	AR(SIC)	&\bfseries	AR(SIC)	&	\bfseries DNS(5)+FB1	\\
Expansion	&	\multicolumn{1}{c}{3 Step}	&	DNS(6)+FB1	&	DNS(4)+FB1	&	DNS(5)+FB1	&	DNS(4)+FB1	&	DNS(4)+FB1	\\
	&		&	DNS(4)+FB1	&	AR(SIC)	&	DNS(4)+FB1	&	DNS(6)+FB1	&	AR(SIC)	\\\cmidrule{2-7}
	&		&\bfseries	DNS(4)+MAC	&\bfseries	AR(SIC)	&\bfseries	AR(SIC)	&\bfseries	AR(SIC)	&	\bfseries AR(SIC)	\\
	&	\multicolumn{1}{c}{12 Step}	&	DNS(5)+MAC	&	DNS(5)+MAC	&	DNS(5)+MAC	&	VAR(1)+FB1	&	DNS(6)	\\
	&		&	DNS(6)+MAC	&	DNS(4)+MAC	&	DNS(6)+MAC	&	VAR(SIC)+FB1	&	DNS(4)	\\
\hline\hline
\end{tabular}
\end{adjustbox}	

\begin{minipage}{1.05\columnwidth}

\hspace{0.1in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 5. Recessions and expansion are defined according to NBER business cycle dates.
\end{minipage}

\newpage

Table 7: Forecast Combination Models Used in Forecast Experiments\textnormal{\superscript{*}}

\vspace{0.3cm} 
\begin{adjustbox}{width=1\columnwidth}
\begin{tabular}{ll}
\hline \hline
\multicolumn{1}{c}{Model}&		\multicolumn{1}{c}{Description} 	\\\hline
All   &     Average of all forty four forecast models\\
FB    &     Average of twenty five models that contain principle component(s), principle component analysis based on all 103 macroeconomic variables\\
FS    &     Average of nineteen non-FB type models\\
Econometrics   &     Average of all eight AR and VAR type models\\
DNS    &     Average of all twenty two DNS type models\\
DI    &     Average of twelve diffusion index type models\\\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{1\columnwidth}

\hspace{0.1in}

\scriptsize
\textnormal{\superscript{*}} Notes: This table summarizes the combination models utilized in all forecast experiments.
\end{minipage}

\newpage

Table 8A: 1-Step-Ahead Relative MSFEs of Forecast Combination Models\textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 1,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{**}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{llSSSSS}
\hline\hline
&Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
&Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
		&	All	&	0.922	&	0.976	&	0.971	&	0.999	&	1.066	\\
		&	FB	&	0.906	&	0.949	&	0.958	&	1.008	&	1.101	\\
	1992:3-1999:12	&	FS	&	1.003	&	1.062	&	1.030	&	1.020	&	1.063	\\
	\small{`Subsample 1'}	&	Econometrics	&	\bfseries 0.842\textnormal{\superscript{**}}	&	\bfseries 0.893\textnormal{\superscript{**}}	&	\bfseries 0.912\textnormal{\superscript{**}}	&	\bfseries 0.930\textnormal{\superscript{*}}	&	\bfseries 0.993	\\
		&	DNS	&	0.861\textnormal{\superscript{**}}	&	0.928\textnormal{\superscript{**}}	&	0.932\textnormal{\superscript{**}}	&	0.982	&	0.998	\\
		&	DIF	&	1.293	&	1.292	&	1.196	&	1.142	&	1.468	\\\midrule

		&	All	&	0.740\textnormal{\superscript{***}}	&	0.866\textnormal{\superscript{***}}	&	0.903\textnormal{\superscript{**}}	&	0.981	&	0.967	\\
		&	FB	&	\bfseries 0.652\textnormal{\superscript{***}}	&	\bfseries 0.812\textnormal{\superscript{**}}	&	\bfseries 0.864\textnormal{\superscript{**}}	&	0.949	&	0.964	\\
	2000:1-2007:12	&	FS	&	0.933	&	0.991	&	0.996	&	1.052	&	0.999	\\
	\small{`Subsample 2'}	&	Econometrics	&	0.769\textnormal{\superscript{***}}	&	0.871\textnormal{\superscript{**}}	&	0.899\textnormal{\superscript{*}}	&	 \bfseries 0.933	&	1.005	\\
		&	DNS	&	0.751\textnormal{\superscript{***}}	&	0.838\textnormal{\superscript{***}}	&	0.866\textnormal{\superscript{***}}	&	0.997	&	\bfseries  0.931\textnormal{\superscript{**}}	\\
		&	DIF	&	0.926	&	1.076	&	1.091	&	1.047	&	1.201	\\\midrule

		&	All	&	1.227	&	1.174	&	1.147	&	1.239	&	1.081	\\
		&	FB	&	1.654	&	1.527	&	1.359	&	1.313	&	1.155	\\
	2008:1-2016:7	&	FS	&	1.135	&	\bfseries 1.010	&	\bfseries 1.067	&	1.235	&	\bfseries 1.013	\\
	\small{`Subsample 3'}	&	Econometrics	&	\bfseries  0.969	&	1.079	&	1.092	&	\bfseries 1.098	&	1.125	\\
		&	DNS	&	1.365	&	1.129	&	1.134	&	1.419	&	1.030	\\
		&	DIF	&	1.841	&	1.702	&	1.466	&	1.216	&	1.335	\\\midrule

		&	All	&	0.911	&	0.971	&	0.984	&	1.061	&	1.042	\\
		&	FB	&	0.958	&	1.011	&	1.011	&	1.074	&	1.082	\\
	1992:3-2016:7	&	FS	&	1.002	&	1.022	&	1.025	&	1.094	&	1.022	\\
	\small{`Subsample 4'}	&	Econometrics	&	\bfseries 0.839\textnormal{\superscript{***}}	&	\bfseries 0.922\textnormal{\superscript{*}}	&	\bfseries 0.947	&	\bfseries 0.980	&	1.053	\\
		&	DNS	&	0.922	&	0.932\textnormal{\superscript{*}}	&	0.951	&	1.114	&	\bfseries 0.991	\\
		&	DIF	&	1.257	&	1.286	&	1.215	&	1.127	&	1.329	\\\midrule

		&	All	&	0.814	&	0.991	&	0.968	&	0.920	&	0.996	\\
		&	FB	&	1.052	&	1.287	&	1.190	&	0.997	&	1.063	\\
	Recession	&	FS	&	0.887\textnormal{\superscript{*}}	&	0.907	&	0.943\textnormal{\superscript{*}}	&	0.999	&	\bfseries 0.956	\\
		&	Econometrics	&	\bfseries 0.692\textnormal{\superscript{**}}	&	\bfseries 0.805\textnormal{\superscript{*}}	&	\bfseries 0.841	& 0.899	&	1.061	\\
		&	DNS	&	0.707\textnormal{\superscript{**}}	&	0.910	&	0.877	&	\bfseries 0.893\textnormal{\superscript{**}}	& 1.052	\\
		&	DIF	&	1.506	&	1.517	&	1.404	&	1.109	&	0.984	\\\midrule

		&	All	&	0.948	&	0.966	&	0.987	&	1.089	&	1.054	\\
		&	FB	&	0.923	&	0.938\textnormal{\superscript{*}}	&	0.972	&	1.089	&	1.087	\\
	Normal	&	FS	&	1.045	&	1.052	&	1.042	&	1.113	&	1.040	\\
		&	Econometrics	&	\bfseries 0.894\textnormal{\superscript{*}}	&	0.953	&	0.971	&	\bfseries 0.995	&	1.051	\\
		&	DNS	&	1.002	&	\bfseries 0.938\textnormal{\superscript{**}}	&	\bfseries 0.967	&	1.157	&	\bfseries 0.975	\\
		&	DIF	&	1.163	&	1.225	&	1.174	&	1.131	&	1.419	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 2A. Forecast combination models are listed in Table 7.
\end{minipage}

\newpage

Table 8B: 3-Step-Ahead Relative MSFEs of Forecast Combination Models\textnormal{\superscript{*}}

\vspace{0.3cm} 
\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 1,
            table-figures-decimal = 3,
            table-space-text-post = \textnormal{\superscript{**}},
} 
\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{llSSSSS}
\hline\hline
&Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
&Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
		&	All	&	0.943	&	1.011	&	1.021	&	1.024	&	1.027	\\
		&	FB	&	0.941	&	1.013	&	1.032	&	1.046	&	1.057	\\
	1992:7-1999:12	&	FS	&	0.984	&	1.037	&	1.029	&	1.010	&	1.003	\\
	\small{`Subsample 1'}	&	Econometrics	&	0.989	&	1.041	&	1.055	&	1.050	&	1.028	\\
		&	DNS	&	\bfseries 0.881\textnormal{\superscript{**}}	&	\bfseries 0.953	&	\bfseries 0.970	&	\bfseries 0.982	&	\bfseries 0.968	\\
		&	DIF	&	1.168	&	1.228	&	1.194	&	1.174	&	1.278	\\\midrule

		&	All	&	0.779\textnormal{\superscript{***}}	&	0.842\textnormal{\superscript{***}}	&	0.864\textnormal{\superscript{***}}	&	0.935\textnormal{\superscript{*}}	&	0.959\textnormal{\superscript{**}}	\\
		&	FB	&	\bfseries 0.701\textnormal{\superscript{***}}	&	\bfseries 0.789\textnormal{\superscript{***}}	&	0.828\textnormal{\superscript{***}}	&	0.916\textnormal{\superscript{*}}	&	0.971	\\
	2000:1-2007:12	&	FS	&	0.911\textnormal{\superscript{**}}	&	0.930\textnormal{\superscript{**}}	&	0.926\textnormal{\superscript{**}}	&	0.970	&	0.960\textnormal{\superscript{**}}	\\
	\small{`Subsample 2'}	&	Econometrics	&	0.844\textnormal{\superscript{***}}	&	0.886\textnormal{\superscript{***}}	&	0.888\textnormal{\superscript{***}}	&	0.907\textnormal{\superscript{**}}	&	0.996	\\
		&	DNS	&	0.770\textnormal{\superscript{***}}	&	0.790\textnormal{\superscript{***}}	&	\bfseries 0.802\textnormal{\superscript{***}}	&	\bfseries 0.887\textnormal{\superscript{**}}	&	\bfseries 0.873\textnormal{\superscript{***}}	\\
		&	DIF	&	0.881\textnormal{\superscript{*}}	&	1.030	&	1.076	&	1.142	&	1.286	\\\midrule

		&	All	&	1.049	&	1.067	&	1.088	&	1.125	&	0.995	\\
		&	FB	&	1.284	&	1.280	&	1.230	&	1.187	&	1.040	\\
	2008:1-2016:7	&	FS	&	1.141	&	1.054	&	1.078	&	1.122	&	0.954	\\
	\small{`Subsample 3'}	&	Econometrics	&	\bfseries 0.934	&	\bfseries 0.960	&	\bfseries 0.945	&	\bfseries 0.914\textnormal{\superscript{**}}	&	\bfseries 0.923\textnormal{\superscript{**}}	\\
		&	DNS	&	1.105	&	1.004	&	1.052	&	1.174	&	0.942	\\
		&	DIF	&	1.614	&	1.597	&	1.472	&	1.314	&	1.262	\\\midrule

		&	All	&	0.897\textnormal{\superscript{**}}	&	0.955	&	0.976	&	1.022	&	0.997	\\
		&	FB	&	0.918	&	0.983	&	1.001	&	1.041	&	1.027	\\
	1992:7-2016:7	&	FS	&	0.989	&	0.998	&	1.001	&	1.028	&	0.973\textnormal{\superscript{**}}	\\
	\small{`Subsample 4'}	&	Econometrics	&	0.914\textnormal{\superscript{***}}	&	0.960\textnormal{\superscript{*}}	&	0.964\textnormal{\superscript{*}}	&	\bfseries 0.962\textnormal{\superscript{*}}	&	0.979	\\
		&	DNS	&	\bfseries 0.885\textnormal{\superscript{**}}	&	\bfseries 0.899\textnormal{\superscript{**}}	&	\bfseries 0.925\textnormal{\superscript{**}}	&	1.004	&	\bfseries 0.933\textnormal{\superscript{***}}	\\
		&	DIF	&	1.149	&	1.232	&	1.215	&	1.203	&	1.274	\\\midrule

		&	All	&	0.794\textnormal{\superscript{**}}	&	0.826\textnormal{\superscript{*}}	&	0.831\textnormal{\superscript{*}}	&	0.873\textnormal{\superscript{**}}	&	0.973	\\
		&	FB	&	0.856	&	0.897	&	0.875	&	0.880	&	1.009	\\
	Recession	&	FS	&	0.946\textnormal{\superscript{*}}	&	0.910\textnormal{\superscript{**}}	&	0.915\textnormal{\superscript{**}}	&	0.962	&	0.972	\\
		&	Econometrics	&	0.868\textnormal{\superscript{***}}	&	0.874\textnormal{\superscript{***}}	&	0.863\textnormal{\superscript{***}}	&	0.863\textnormal{\superscript{**}}	&	1.001	\\
		&	DNS	&	\bfseries 0.759\textnormal{\superscript{***}}	&	\bfseries 0.763\textnormal{\superscript{**}}	&	\bfseries 0.770\textnormal{\superscript{**}}	&	\bfseries 0.828\textnormal{\superscript{***}}	&	\bfseries 0.935	\\
		&	DIF	&	1.105	&	1.089	&	1.053	&	1.061	&	1.216	\\\midrule

		&	All	&	0.962	&	1.016	&	1.032	&	1.060	&	1.000	\\
		&	FB	&	0.957	&	1.024	&	1.048	&	1.082	&	1.030	\\
	Normal	&	FS	&	1.016	&	1.040	&	1.034	&	1.045	&	0.973\textnormal{\superscript{*}}	\\
		&	Econometrics	&	\bfseries 0.943\textnormal{\superscript{**}}	&	1.001	&	1.003	&	\bfseries 0.988	&	0.976	\\
		&	DNS	&	0.965	&	\bfseries 0.963	&	\bfseries 0.984	&	1.049	&	\bfseries 0.933\textnormal{\superscript{***}}	\\
		&	DIF	&	1.177	&	1.299	&	1.277	&	1.239	&	1.283	\\
	\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 8A.
\end{minipage}

\newpage

Table 8C: 12-Step-Ahead Relative MSFEs of Forecast Combination Models$^{*} $

\vspace{0.3cm}

\sisetup{
            detect-all,
            table-number-alignment = center,
            table-figures-integer = 1,
            table-figures-decimal = 3,
            table-space-text-post = {\superscript{*}},
}

\begin{adjustbox}{totalheight=0.9\textheight-2\baselineskip}
\begin{tabular}{llSSSSS}
\hline\hline
	&Model	&		&		&	\multicolumn{1}{c}{rMSFE}	&		&		\\\hline
	&Maturity&	{1 year}	&	{2 year}	& {3 years}	& {5 years}	& {10 years}	\\\hline
			&	All	&	0.852	&	0.923	&	0.966	&	1.044	&	1.071	\\
			&	FB	&	0.912	&	0.983	&	1.034	&	1.126	&	1.164	\\
	1994:1-1999:12	&	FS	&	0.889\textnormal{\superscript{*}}	&	0.933	&	0.951	&	0.988	&	0.990	\\
	\small{`Subsample 1'}	&	Econometrics	&	1.241	&	1.235	&	1.215	&	1.147	&	0.999	\\
		&	DNS	&	\bfseries 0.773\textnormal{\superscript{**}}	&	\bfseries 0.844\textnormal{\superscript{**}}	&	\bfseries 0.880\textnormal{\superscript{*}}	&	\bfseries 0.931	&	\bfseries 0.920\textnormal{\superscript{**}}	\\
		&	DIF	&	1.204	&	1.281	&	1.363	&	1.607	&	1.888	\\\midrule
                            
		&	All	&	0.673\textnormal{\superscript{***}}	&	0.619\textnormal{\superscript{***}}	&	0.616\textnormal{\superscript{***}}	&	0.717\textnormal{\superscript{***}}	&	1.006	\\
		&	FB	&	0.706\textnormal{\superscript{***}}	&	0.676\textnormal{\superscript{***}}	&	0.699\textnormal{\superscript{***}}	&	0.868\textnormal{\superscript{***}}	&	1.341	\\
	2000:1-2007:12	&	FS	&	0.641\textnormal{\superscript{***}}	&	0.559\textnormal{\superscript{***}}	&	0.527\textnormal{\superscript{***}}	&	0.558\textnormal{\superscript{***}}	&	0.686\textnormal{\superscript{***}}	\\
	\small{`Subsample 2'}	&	Econometrics	&	0.591\textnormal{\superscript{***}}	&	0.509\textnormal{\superscript{***}}	&	\bfseries 0.470\textnormal{\superscript{***}}	&	\bfseries 0.467\textnormal{\superscript{***}}	&	\bfseries 0.577\textnormal{\superscript{***}}	\\
		&	DNS	&	\bfseries 0.548\textnormal{\superscript{***}}	&	\bfseries 0.486\textnormal{\superscript{***}}	&	0.470\textnormal{\superscript{***}}	&	0.535\textnormal{\superscript{***}}	&	0.712\textnormal{\superscript{***}}	\\
		&	DIF	&	1.348	&	1.309	&	1.378	&	1.719	&	2.708	\\\midrule

		&	All	&	0.809\textnormal{\superscript{***}}	&	0.868\textnormal{\superscript{**}}	&	0.965	&	1.081	&	0.950\textnormal{\superscript{*}}	\\
		&	FB	&	0.986	&	1.022	&	1.084	&	1.158	&	1.008	\\
	2008:1-2016:7	&	FS	&	0.979	&	0.975	&	1.025	&	1.071	&	0.884\textnormal{\superscript{***}}	\\
	\small{`Subsample 3'}	&	Econometrics	&	0.768\textnormal{\superscript{***}}	&	\bfseries 0.753\textnormal{\superscript{***}}	&	\bfseries 0.733\textnormal{\superscript{***}}	&	\bfseries 0.704\textnormal{\superscript{***}}	&	\bfseries 0.738\textnormal{\superscript{***}}	\\
		&	DNS	&	\bfseries 0.746\textnormal{\superscript{***}}	&	0.764\textnormal{\superscript{***}}	&	0.881\textnormal{\superscript{**}}	&	1.056	&	0.894\textnormal{\superscript{***}}	\\
		&	DIF	&	1.567	&	1.598	&	1.599	&	1.575	&	1.317	\\\midrule

		&	All	&	0.729\textnormal{\superscript{***}}	&	0.722\textnormal{\superscript{***}}	&	0.754\textnormal{\superscript{***}}	&	0.882\textnormal{\superscript{***}}	&	1.006	\\
		&	FB	&	0.800\textnormal{\superscript{***}}	&	0.800\textnormal{\superscript{***}}	&	0.842\textnormal{\superscript{***}}	&	0.998	&	1.166	\\
	1994:1-2016:7	&	FS	&	0.754\textnormal{\superscript{***}}	&	0.709\textnormal{\superscript{***}}	&	0.711\textnormal{\superscript{***}}	&	0.783\textnormal{\superscript{***}}	&	0.853\textnormal{\superscript{***}}	\\
	\small{`Subsample 4'}	&	Econometrics	&	0.720\textnormal{\superscript{***}}	&	0.678\textnormal{\superscript{***}}	&	\bfseries 0.662\textnormal{\superscript{***}}	&	\bfseries 0.680\textnormal{\superscript{***}}	&	\bfseries 0.768\textnormal{\superscript{***}}	\\
		&	DNS	&	\bfseries 0.625\textnormal{\superscript{***}}	&	\bfseries 0.603\textnormal{\superscript{***}}	&	0.633\textnormal{\superscript{***}}	&	0.754\textnormal{\superscript{***}}	&	0.843\textnormal{\superscript{***}}	\\
		&	DIF	&	1.381	&	1.367	&	1.422	&	1.658	&	1.949	\\\midrule

		&	All	&	1.033	&	1.016	&	1.023	&	1.072	&	1.063	\\
		&	FB	&	\bfseries 0.948	&	0.963	&	0.999	&	1.103	&	1.254	\\
	Recession	&	FS	&	1.173	&	1.103	&	1.067	&	1.037	&	0.842\textnormal{\superscript{***}}	\\
		&	Econometrics	&	1.191	&	1.087	&	0.995	&	\bfseries 0.854\textnormal{\superscript{***}}	&	\bfseries 0.631\textnormal{\superscript{***}}	\\
		&	DNS	&	1.024	&	\bfseries 0.950	&	\bfseries 0.945	&	0.971	&	0.806\textnormal{\superscript{***}}	\\
		&	DIF	&	0.972	&	1.117	&	1.222	&	1.476	&	2.163	\\\midrule

		&	All	&	0.630\textnormal{\superscript{***}}	&	0.639\textnormal{\superscript{***}}	&	0.687\textnormal{\superscript{***}}	&	0.843\textnormal{\superscript{***}}	&	1.000	\\
		&	FB	&	0.752\textnormal{\superscript{***}}	&	0.754\textnormal{\superscript{***}}	&	0.803\textnormal{\superscript{***}}	&	0.977	&	1.157	\\
	Normal	&	FS	&	0.618\textnormal{\superscript{***}}	&	0.598\textnormal{\superscript{***}}	&	0.622\textnormal{\superscript{***}}	&	0.731\textnormal{\superscript{***}}	&	0.854\textnormal{\superscript{***}}	\\
		&	Econometrics	&	0.567\textnormal{\superscript{***}}	& 0.564\textnormal{\superscript{***}}	&	0.578\textnormal{\superscript{***}}	& \bfseries 0.645\textnormal{\superscript{***}}	&	\bfseries 0.782\textnormal{\superscript{***}}	\\
		&	DNS	&	\bfseries 0.495\textnormal{\superscript{***}}	&	\bfseries 0.506\textnormal{\superscript{***}}	&	\bfseries 0.554\textnormal{\superscript{***}}	&	 0.710\textnormal{\superscript{***}}	&	0.847\textnormal{\superscript{***}}	\\
		&	DIF	&	1.514	&	1.437	&	1.472	&	1.695	&	1.927	\\
\hline\hline
\end{tabular}
\end{adjustbox}

\begin{minipage}{0.95\columnwidth}

\hspace{0.3in}

\scriptsize
\textnormal{\superscript{*}} Notes: See notes to Table 8A.
\end{minipage}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Figures
\begin{landscape}
	
	\begin{center}
		
		\vspace*{\fill}
		
		Figure 1: FRED MD Dataset for Sample Period 1992:1 - 2016:7$^{\ast}$
		
		\vspace{0.3cm}
		
		\makebox[\textwidth][c]{%  
			\includegraphics[width=1.25\linewidth]{Figure1_noTtl.eps}  
		}
		
		\begin{minipage}{1\columnwidth}
			\footnotesize
			$^{(*)}$ Notes: The Figure displays all macroeconomic variables from the FRED-MD dataset and the 1 year zero-yield from the GSW dataset. All series transformed to ensure stationary and standardized to zero mean and unit variance. 
		\end{minipage}
		
		\vspace*{\fill}
		
		\newpage
		
		\vspace*{\fill}
		
		Figure 2: Yields and Diffusion Indexes for Sample Period 1992:1 - 2016:7$^{\ast}$
		
		\vspace{0.3cm}
		
		\makebox[\textwidth][c]{%  
			\includegraphics[width=1.25\linewidth]{Figure2_noTtl.eps}  
		}
		
		\begin{minipage}{1\columnwidth}
			\footnotesize
			$^{(*)}$ Notes: This Figure displays all ten zero-yields from the GSW dataset and three principle components (diffusion indexes) in PCA. 
		\end{minipage}
		
		\vspace*{\fill}
		
		\newpage
		
		\vspace*{\fill}
		
		Figure 3: Yields and Dynamic Nelson Siegel Factors for Sample Period 1992:1 - 2016:7$^{\ast}$
		
		\vspace{0.3cm}
		
		\makebox[\textwidth][c]{%  
			\includegraphics[width=1.25\linewidth]{Figure3_noTtl.eps}  
g		}
		
		\begin{minipage}{1\columnwidth}
			\footnotesize
			$^{(*)}$ Notes: This Figure displays all ten zero-yields from the GSW dataset and three Nelson Siegel latent factors (level, slope, and curvature).  
		\end{minipage}
		
		\vspace*{\fill}
		
		\newpage
		
		\vspace*{\fill}
		
		Figure 4: Yields, Diffusion Indexes and Dynamic Nelson Siegel Factors for Sample Period 1992:1 - 2016:7$^{\ast}$
		
		\makebox[\textwidth][c]{%  
			\includegraphics[width=1.25\linewidth]{Figure4_noTtl.eps}  
		}
		
		\begin{minipage}{1\columnwidth}
			\footnotesize
			$^{(*)}$ Notes: The Figure displays all ten zero-yields from the GSW dataset, three principle components (diffusion indexes) in PCA, and three Nelson Siegel latent factors (level, slope, and curvature). 
		\end{minipage}
		\vspace*{\fill}
		
	\end{center}
	
\end{landscape}

\end{center}

\end{document}
