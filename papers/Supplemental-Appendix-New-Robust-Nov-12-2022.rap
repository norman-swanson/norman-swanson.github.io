%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                     %
%              Scientific Word   Wrap/Unwrap  Version 2.5             %
%              Scientific Word   Wrap/Unwrap  Version 3.0             %
%                                                                     %
% If you are separating the files in this message by hand, you will   %
% need to identify the file type and place it in the appropriate      %
% directory.  The possible types are: Document, DocAssoc, Other,      %
% Macro, Style, Graphic, PastedPict, and PlotPict. Extract files      %
% tagged as Document, DocAssoc, or Other into your TeX source file    %
% directory.  Macro files go into your TeX macros directory. Style    %
% files are used by Scientific Word and do not need to be extracted.  %
% Graphic, PastedPict, and PlotPict files should be placed in a       %
% graphics directory.                                                 %
%                                                                     %
% Graphic files need to be converted from the text format (this is    %
% done for e-mail compatability) to the original 8-bit binary format. %
%                                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                                                                     %
% Files included:                                                     %
%                                                                     %
% "/document/Supplemental-Appendix-New-Robust-Nov-12-2022.tex", Document, 85086, 11/12/2022, 18:22:28, ""%
%                                                                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%% Start /document/Supplemental-Appendix-New-Robust-Nov-12-2022.tex %%%%%

%2multibyte Version: 5.50.0.2960 CodePage: 936

\documentclass[final,notitlepage]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{caption}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{Codepage=936}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wed May 15 17:28:15 2002}
%TCIDATA{LastRevised=Saturday, November 12, 2022 13:22:28}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=LaTeX article (bright).cst}
%TCIDATA{PageSetup=65,65,72,72,0}
%TCIDATA{AllPages=
%H=36
%F=29,\PARA{038<p type="texpara" tag="Body Text" > \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  }
%}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{tcilatex}
\textwidth=16.0cm
\oddsidemargin=0cm \evensidemargin=0cm
\topmargin=-20pt
\numberwithin{equation}{section}
\baselineskip=100pt
\textheight=21cm
\def\baselinestretch{1.2}
\begin{document}

\title{}

\begin{center}
{\LARGE \vspace{1in} Supplemental Appendix: Robust Forecast Superiority
Testing with an Application to Assessing Pools of Expert Forecasters} *

{\Large Valentina Corradi}$^{1}${\Large , Sainan Jin}$^{2},$ {\Large and
Norman R. Swanson}$^{3}\medskip $

$^{1}$University of Surrey, $^{2}$Tsinghua University, and $^{3}$Rutgers
University

\bigskip

November 2022

\bigskip

\bigskip
\end{center}

\noindent (i) Appendix SA1: Forecast Superiority Testing under Convex Loss,
CL

\bigskip

\noindent (ii) Appendix SA2: Forecast Superiority Tests in the Presence of
Recursive Estimation Error

\bigskip

\noindent (iii) Appendix SA3: The JCS Test

\bigskip

\noindent (iv) Appendix SA4: Additional Monte Carlo Experiments

\bigskip

\noindent (v) Appendix SA5: Additional Empirical Results

\bigskip

\bigskip

\setcounter{page}{0} \thispagestyle{empty}

{\small \newpage }

\newpage

\section{Appendix SA1: Forecast Superiority Tests under Convex Loss Functions%
}

In this Supplemental Section we state the counterpart of Lemmas 1 and 2, and
Theorems 1 and 2 for Convex Loss (CL) forecast superiority testing. Let $%
[a]_{+}=a1\left\{ a\geq 0\right\} ,$ define for $x\in \mathcal{X}^{+}$%
\begin{eqnarray*}
\widehat{\sigma }_{j,n}^{2,C+}(x) &=&\frac{1}{n}\sum_{t=1}^{n}(\widehat{\eta 
}_{j,t}(x)-\widehat{\eta }_{1,t}(x))^{2} \\
&&+2\frac{1}{n}\sum_{\tau =1}^{l_{n}}\sum_{t=\tau +1}^{n}w_{\tau }(\widehat{%
\eta }_{j,t}(x)-\widehat{\eta }_{1,t}(x))(\widehat{\eta }_{j,t-\tau }(x)-%
\widehat{\eta }_{1,t-\tau }(x)),
\end{eqnarray*}%
where%
\begin{equation*}
\widehat{\eta }_{j,t}(x)=\left[ \left( e_{j,t}-x\right) \right] _{+}-\frac{1%
}{n}\sum_{t=1}^{n}\left[ \left( e_{j,t}-x\right) \right] _{+},
\end{equation*}%
we have

\noindent \textbf{Lemma S1}\textit{: Let Assumptions A1-A3 hold. Then, if as 
}$n\rightarrow \infty ,$\textit{\ }$\frac{l_{n}}{n^{\delta }}\rightarrow c$, 
$with$ $0<c<\infty $ $and$\textit{\ }$0<\delta <\frac{1}{2},$\textit{\ with }%
$\delta $\textit{\ defined as in Assumption A1:}

\begin{equation*}
\sup_{x\in \mathcal{X}^{+}}\left\vert \widehat{\sigma }_{j,n}^{2,C+}(x)-%
\sigma _{j}^{2,C+}(x)\right\vert =o_{p}\left( 1\right) ,
\end{equation*}%
\textit{with }$\sigma _{j}^{2C+}(x)=avar\left( \sqrt{n}C_{j,n}^{+}(x)\right)
.$

\noindent Now let $\eta _{j,t}^{\ast }(x)=\left[ e_{j,t}^{\ast }-x\right]
_{+}-\frac{1}{n}\sum_{t=1}^{n}1\left[ e_{j,t}^{\ast }-x\right] _{+}$ and
define

\begin{equation*}
\widehat{\sigma }_{j,n}^{2\ast C+}(x)=\frac{1}{b_{n}}\sum_{k=1}^{b_{n}}%
\left( \frac{1}{l_{n}^{1/2}}\sum_{i=1}^{l_{n}}\left( \eta
_{j,(k-1)l_{n}+i}^{\ast }(x)-\eta _{1,(k-1)l_{n}+i}^{\ast }(x)\right)
\right) ^{2},
\end{equation*}

\noindent We have

\noindent \textbf{Lemma S2: }\textit{Let Assumptions A1-A3 hold. Then, if as 
}$n\rightarrow \infty ,$\textit{\ }$\frac{l_{n}}{n^{\delta }}\rightarrow c$, 
$with$ $0<c<\infty $ $and$\textit{\ }$0<\delta <\frac{1}{2},$\textit{\ with }%
$\delta $ \textit{defined as in Assumption A1:}

\begin{equation*}
\sup_{x\in \mathcal{X}^{+}}\left\vert \widehat{\sigma }_{j,n}^{\ast C+}(x)-%
\widehat{\sigma }_{j,n}^{C+}(x)\right\vert =o_{p}^{\ast }\left( 1\right) ,
\end{equation*}%
\textit{where }$o_{p}^{\ast }(1)$\textit{\ denotes convergence to zero
according to the bootstrap law, }$P^{\ast },$\textit{\ conditional on the
sample.}

\noindent Finally, define 
\begin{eqnarray}
C_{j,n}(x) &=&\int_{-\infty }^{x}\left( \widehat{F}_{1,n}(t)-\widehat{F}%
_{j,n}(t)\right) dt1(x<0)-\int_{x}^{\infty }\left( \widehat{F}_{j,n}(t)-%
\widehat{F}_{1,n}(t)\right) dt1(x\geq 0)  \notag \\
&=&\frac{1}{n}\sum_{t=1}^{n}\left( \left[ \left( e_{1,t}-x\right) sgn(x)%
\right] _{+}-\left[ \left( e_{j,t}-x\right) sgn(x)\right] _{+}\right) ,
\label{Cj}
\end{eqnarray}%
where $sgn(x)=1$ if $x\geq 0$ and $sgn(x)=-1$ if $x<0,$ and%
\begin{equation*}
S_{n}^{C+}=\underset{x\in \mathcal{X}^{+}}{\int }\underset{j=2}{\sum^{k}}%
\left( \max \left\{ 0,\sqrt{n}\frac{C_{j,n}(x)}{\overline{\sigma }%
_{j,n}^{C+}(x)}\right\} \right) ^{2}dQ(x)\text{ }
\end{equation*}%
where $\overline{\sigma }_{j,n}^{2C}(x)=\widehat{\sigma }_{j,n}^{2C}(x)+%
\varepsilon .$

\noindent As shown in Proposition 2.3 in Jin, Corradi and Swanson (2017;
JCS), the null hypothesis is CL forecast superiority and its negation write
as%
\begin{eqnarray*}
H_{0}^{C} &=&H_{0}^{C-}\cap H_{0}^{C+} \\
&:&\left( \int_{-\infty }^{x}(F_{1}(t)-F_{j}(t))dt\leq 0,\text{ for }%
j=2,...,k\text{, and for all }x\in \mathcal{X}^{-}\right) \\
&&\cap \left( \int_{x}^{\infty }(F_{j}(t)-F_{1}(t))dt\leq 0,\text{ for }%
j=2,...,k,\text{ and for all }x\in \mathcal{X}^{+}\right)
\end{eqnarray*}%
versus%
\begin{eqnarray*}
H_{A}^{C} &=&H_{A}^{C-}\cup H_{A}^{C+} \\
&:&\left( \int_{-\infty }^{x}(F_{1}(t)-F_{j}(t))dt>0,\text{ for some }%
j=2,...,k,\text{ and for some }x\in \mathcal{X}^{-}\right) \\
&&\cup \left( \max_{j=2,...,k}\int_{x}^{\infty }(F_{j}(t)-F_{1}(t))dt>0,%
\text{ for some }j=2,...,k,\text{ and for some }x\in \mathcal{X}^{+}\right) .
\end{eqnarray*}%
We now find lower and upper bound for $S_{n}^{C+}$ under $H_{0}^{C+}.$Let%
\begin{equation*}
v^{C+}(.)=(v_{2}^{C+}(.),...,v_{k}^{C+}(.))^{\prime },
\end{equation*}%
be a ($k-1)-$dimensional zero mean Gaussian process with covariance kernel
equal to \textrm{acov}$\left( \sqrt{n}C^{+}(x),\sqrt{n}C^{+}(x^{\prime
})\right) $. Also, let%
\begin{equation*}
h_{j,A,n}^{C+}(x)=\sigma _{j}^{C+}(x)^{-1}\sqrt{n}C_{j}^{+}(x)
\end{equation*}%
\begin{equation*}
h_{j,B}^{C+}(x)=\sigma _{j}^{2,C+}(x)^{-1}\left( \sigma
_{j}^{C+}(x)+\varepsilon \right) ^{2}
\end{equation*}%
and $h_{B}^{C+}(x)=\left( h_{2,B}^{C+}(x),...,h_{k,B}^{C+}(x)\right) $%
\begin{equation*}
S_{n}^{\dag C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max \left\{ 0,%
\frac{v_{j}^{C+}(x)+h_{j,A,n}^{C+}(x)}{\sqrt{h_{j,B}^{C+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
and

\begin{equation*}
S_{\infty }^{\dag C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max
\left\{ 0,\frac{v_{j}^{C+}(x)+h_{j,A,\infty }^{C+}(x)}{\sqrt{h_{j,B}^{C+}(x)}%
}\right\} \right) ^{2}\mathrm{d}Q(x),
\end{equation*}
We have the CL counterpart of Theorem 1.

\noindent \textbf{Theorem S1: }\textit{Let Assumptions A1-A4 hold. Then,
under }$H_{0}^{C+},$\textit{\ there exists }$\delta >0$ \textit{and }$a^{C+}$%
\textit{\ such that}%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( S_{n}^{C+}>a^{C+}\right) -P\left( S_{n}^{\dag C+}+\delta
>a^{C+}\right) \right] \leq 0
\end{equation*}%
\textit{and}%
\begin{equation*}
\lim \inf_{n\rightarrow \infty }\inf_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( S_{n}^{C+}>a^{C+}\right) -P\left( S_{n}^{\dag C+}-\delta
>a^{C+}\right) \right] \geq 0.
\end{equation*}%
We now construct asymptotically valid bootstrap critical values. Let%
\begin{equation*}
C_{j,n}^{\ast +}(x)=\frac{1}{n}\sum_{t=1}^{n}\left( \left[ e_{j,t}^{\ast }-x%
\right] _{+}-\frac{1}{n}\sum_{t=1}^{n}\left[ e_{1,t}^{\ast }-x\right]
_{+}\right)
\end{equation*}%
and $v_{n}^{\ast C+}(x)=\left( v_{2,n}^{\ast C+}(x),...,v_{k,n}^{\ast
C+}(x)\right) $ with%
\begin{equation*}
v_{j,n}^{\ast C+}(x)=\frac{\sqrt{n}\left( C_{j,n}^{\ast
+}(x)-C_{j,n}^{+}(x)\right) }{\widehat{\sigma }_{j,n}^{C+}(x)}.
\end{equation*}%
\noindent Then, define:%
\begin{equation*}
\xi _{j,n}^{C+}(x)=\kappa _{n}^{-1}n^{1/2}\frac{C_{j,n}^{+}(x)}{\overline{%
\sigma }_{j,n}^{C+}(x)},
\end{equation*}%
and%
\begin{equation*}
\phi _{j,n}^{G+}(x)=c_{n}1\left\{ \xi _{j,n}^{C+}(x)<-1\right\} ,
\end{equation*}%
and $\phi _{n}^{C+}=\left( \phi _{2,n}^{C+},...,\phi _{k,n}^{C+}\right) .$
Now, let%
\begin{equation}
S_{n}^{\ast C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left( \left\{ 0,%
\frac{v_{j,n}^{\ast C+}(x)-\phi _{j,n}^{C+}(x)}{\sqrt{h_{j,B}^{\ast C+}(x)}}%
\right\} \right) ^{2}\mathrm{d}Q(x),  \label{Sn*}
\end{equation}%
where 
\begin{equation}
h_{j,B}^{\ast C+}(x)=\overline{\sigma }_{j,n}^{2,\ast C+}(x)/\widehat{\sigma 
}_{j,n}^{2,C+}(x),  \label{hB*}
\end{equation}%
Then, construct:%
\begin{equation*}
S_{n}^{\ast C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left( \left\{ 0,%
\frac{v_{j,n}^{\ast C+}(x)-\phi _{j,n}^{C+}(x)}{\sqrt{h_{B,jj}^{\ast C+}(x)}}%
\right\} \right) ^{2}\mathrm{d}Q(x).
\end{equation*}%
and let $c_{0,n,1-\alpha }^{\ast C+}\left( \phi
_{n}^{C+},h_{B,n}^{C+}\right) =\lim_{B\rightarrow \infty }c_{n,B,1-\alpha
+\eta }^{\ast C+}\left( \phi _{n}^{C+},h_{B,n}^{\ast C+}\right) +\eta .$ We
have the CL counterpart of Theorem 2.

\noindent \textbf{Theorem S2: }\textit{Let Assumptions A1-A4 hold, and let }$%
l_{n}\rightarrow \infty $\textit{\ and }$l_{n}n^{\frac{1}{3}-\varepsilon
}\rightarrow 0$\textit{\ as }$n\rightarrow \infty .$ \textit{Under }$%
H_{0}^{C+},:$

\textit{\noindent (i) if as }$n\rightarrow \infty ,$\textit{\ }$\kappa
_{n}\rightarrow \infty $\textit{\ and }$c_{n}/\kappa _{n}\rightarrow 0,$%
\textit{\ then}%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}P\left(
S_{n}^{C+}\geq c_{0,n,1-\alpha }^{\ast C+}\left( \phi
_{n}^{C+},h_{B,n}^{\ast C+}\right) \right) \leq \alpha ;
\end{equation*}%
\textit{\noindent (ii) Let }$\mathcal{B}^{C+}$ \textit{be defined
analogously to} $\mathcal{B}^{G+}$ \textit{Eq.(3.12) in main text}. \textit{%
If as }$n\rightarrow \infty ,$\textit{\ }$\kappa _{n}\rightarrow \infty ,$%
\textit{\ }$c_{n}\rightarrow \infty $\textit{, }$\sqrt{n}/\kappa
_{n}\rightarrow \infty ,$\textit{\ and }$Q\left( \mathcal{B}^{C+}\right) >0,$%
\textit{\ then}%
\begin{equation*}
\lim_{\eta \rightarrow 0}\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{%
P}_{0}^{C+}}P\left( S_{n}^{C+}\geq c_{0,n,1-\alpha }^{\ast C+}\left( \phi
_{n}^{C+},h_{B,n}^{\ast C+}\right) \right) =\alpha .
\end{equation*}%
Finally, the CL counterparts Theorem 3 and Theorem 4 follow
straightforwardly, and are not stated for brevity.

\noindent The proofs for Theorems 1 and 2 do not depend on whether we
construct GL or CL forecast superiority tests. Hence, below we only provide
the proofs for Lemma S1 and Lemma S2.

\noindent \textbf{Proof of Lemma S1:}

\noindent By noting that,%
\begin{eqnarray*}
&&\left[ e_{j,t}-s_{j}\right] _{+}-\left[ e_{j,t}-x\right] _{+} \\
&=&(x-s_{j})1\{e_{t}\geq x\}+(x-s_{j})\left( 1\{e_{t}\geq x\}-1\{e_{t}\geq
s_{j}\}\right) \\
&&+\left( e_{t}-x\right) \left( 1\{e_{t}\geq s_{j}\}-1\{e_{t}\geq x\}\right)
,
\end{eqnarray*}%
the statement follows by the same argument as that used the proof of Lemma 1.

\noindent \textbf{Proof of Lemma S2:}

\noindent By the same argument as the proof of Lemma 2, replacing $\widehat{u%
}_{j,t}^{\ast }$ with $\widehat{\eta }_{j,t}^{\ast }.$

\newpage

\section{Appendix SA2: Forecast Superiority Tests in the Presence of
Recursive Estimation Error}

\subsection{\noindent The Statistic}

This Supplement extends all Lemmas and Theorems in the paper to the case in
which there is non vanishing, recursive estimation error.

Let $T=R+n.$ At each point in time, $t>R,$ update model parameter estimates
prior to the construction of each new forecast, using all the available
information.\footnote{%
In the rolling estimation case, we use only the most recent $R$ observations
to re-estiamte the forecasdting model, for each $t>R.$ The rolling case can
be treated analogously, and it is omitted only for brevity.}

For $j=1,...,k,$ use the first $R$ observations to compute $\widehat{\theta }%
_{j,R},$ and construct the first prediction error: 
\begin{equation*}
\widehat{e}_{j,R+1}=X_{R+1}-\phi _{j}\left( Z_{j,R},\widehat{\theta }%
_{j,R}\right) ,
\end{equation*}%
where $Z_{j,R}$ contains lags of $X$ as well as other regressors. Then, use
the first $R+1$ observations to construct%
\begin{equation*}
\widehat{e}_{j,R+2}=X_{R+2}-\phi _{j}\left( Z_{j,R+1},\widehat{\theta }%
_{j,R+1}\right) .
\end{equation*}%
Proceed in the same manner until a sequence of $n$ prediction errors has
been constructed, defined as: 
\begin{equation}
\widehat{e}_{j,t+1}=X_{t+1}-\phi _{j}\left( Z_{j,t},\widehat{\theta }%
_{j,t}\right) ,  \label{e-hat}
\end{equation}%
for $t=R,...,R+n-1,$ where $\widehat{\theta }_{j,t}$ is the estimator
computed using observations up to time $t.$ In the sequel, assume that $%
\widehat{\theta }_{j,t}$ is a recursive $m-$estimator, so that:%
\begin{equation}
\widehat{\theta }_{j,t}=\arg \min_{\theta _{j}\in \Theta _{j}}\frac{1}{t}%
\sum_{i=2}^{t}m_{j}(X_{i},Z_{j,i-1},\theta _{j}),\text{ \ \ }R\leq t\leq n-1,%
\text{ }j=1,...,k,  \label{theta-i}
\end{equation}%
and%
\begin{equation*}
\theta _{j}^{\dagger }=\arg \min_{\theta _{j}\in \Theta
_{j}}E(m_{j}(X_{i},Z_{j,i-1},\theta _{j})).
\end{equation*}%
For $x\geq 0,$ define:%
\begin{equation}
\widetilde{G}_{j,n}^{+}(x)=\frac{1}{n}\sum_{t=R}^{T-1}\left( 1\left\{ 
\widehat{e}_{j,t+1}\leq x\right\} -1\left\{ \widehat{e}_{1,t+1}\leq
x\right\} \right) =\left( \widetilde{F}_{j,n}(x)-\widetilde{F}%
_{1,n}(x)\right)  \label{Gtilde}
\end{equation}%
and%
\begin{eqnarray}
\widetilde{C}_{j,n}^{+}(x) &=&\int_{x}^{\infty }\left( \widetilde{F}%
_{j,n}(t)-\widetilde{F}_{1,n}(t)\right) dt  \notag \\
&=&\frac{1}{n}\sum_{t=R}^{T-1}\left\{ \left[ \left( \widehat{e}%
_{1,t+1}-x\right) \right] _{+}-\left[ \left( \widehat{e}_{j,t+1}-x\right) %
\right] _{+}\right\} .  \label{Ctilde}
\end{eqnarray}%
Define the following forecast superiority test statistics:%
\begin{equation*}
\widetilde{S}_{n}^{G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max
\left\{ 0,\frac{\sqrt{n}\widetilde{G}_{j,n}^{+}(x)}{\widetilde{\sigma }%
_{j,n}^{G+}(x)+\varepsilon }\right\} \right) ^{2}\mathrm{d}Q(x)
\end{equation*}%
and%
\begin{equation*}
\widetilde{S}_{n}^{C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max
\left\{ 0,\frac{\sqrt{n}\widetilde{C}_{j,n}^{+}(x)}{\widetilde{\sigma }%
_{j,n}^{C+}(x)+\varepsilon }\right\} \right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
where $\widetilde{\sigma }_{j,n}^{G+}(x)$ and $\widetilde{\sigma }%
_{j,n}^{C+}(x)$ include terms accounting for the contribution of parameter
estimation error to asymptotic covariance. Here, $\widetilde{\sigma }%
_{j,n}^{2,G+}(x)$ is defined as:%
\begin{eqnarray*}
\widetilde{\sigma }_{j,n}^{2,G+}(x) &=&\widehat{\sigma }_{j,n}^{2,G+}(x)+2%
\widehat{\Pi }\widehat{f}_{1,n,h}^{2}(x)\widehat{A}_{1}\widehat{\Sigma }_{11}%
\widehat{A}_{1}^{\prime }+2\widehat{\Pi }\widehat{f}_{j,n,h}^{2}(x)\widehat{A%
}_{j}\widehat{\Sigma }_{jj}\widehat{A}_{j}^{\prime } \\
&&-4\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widehat{A}_{1}\widehat{\Sigma }_{1j}%
\widehat{A}_{j}^{\prime }\widehat{f}_{j,n,h}(x)+2\widehat{\Pi }\widehat{f}%
_{1,n,h}(x)\widehat{A}_{1}\widehat{\Sigma }_{u1}(x)-2\widehat{\Pi }\widehat{f%
}_{j,n,h}(x)\widehat{A}_{j}\widehat{\Sigma }_{uj}(x),
\end{eqnarray*}%
where $\widehat{\sigma }_{j,n}^{2,G+}(x)$ is defined as in the text, but
using only the last $n$ observations, $\widehat{\Pi }=1-\frac{R}{n}\ln
\left( 1+\frac{n}{R}\right) ,$%
\begin{equation*}
\widehat{f}_{j,n,h}(x)=\frac{1}{nh}\sum_{t=R+1}^{n}K\left( \frac{\widehat{e}%
_{j,t}-x}{h}\right) ,
\end{equation*}%
\begin{equation*}
\widehat{A}_{j}=\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t+1},\widehat{\theta }_{j,R}\right) ^{\prime }\left( \frac{1%
}{R}\sum_{t=1}^{R}\nabla _{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{%
\theta }_{j,R})\right) ^{-1},
\end{equation*}%
\begin{eqnarray*}
\widehat{\Sigma }_{jj} &=&\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})^{\prime } \\
&&+2\frac{1}{n}\sum_{\tau =1}^{l_{n}}\sum_{t=R+\tau +1}^{T}w_{\tau }\nabla
_{\theta _{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\nabla _{\theta
_{j}}m_{j}(X_{t-\tau },Z_{t-\tau ,i-1},\widehat{\theta }_{j,R})^{\prime },
\end{eqnarray*}%
and%
\begin{eqnarray*}
\widehat{\Sigma }_{uj}(x) &=&\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\left( \left( 1\left\{ 
\widehat{e}_{j,t}\leq x\right\} -\frac{1}{n}\sum_{t=1}^{n}1\left\{ \widehat{e%
}_{j,t}\leq x\right\} \right) \right. \\
&&\left. -\left( 1\left\{ \widehat{e}_{1,t}\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{1,t}\leq x\right\} \right) \right) \\
&&+2\frac{1}{n}\sum_{\tau =1}^{l_{n}}\sum_{t=R+\tau +1}^{T}w_{\tau }\nabla
_{\theta _{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\left( \left(
1\left\{ \widehat{e}_{j,t-\tau }\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{j,t-\tau }\leq x\right\} \right) \right.
\\
&&\left. -\left( 1\left\{ \widehat{e}_{1,t-\tau }\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{1,t-\tau }\leq x\right\} \right) \right)
,
\end{eqnarray*}%
By noting that%
\begin{eqnarray}
&&\widetilde{C}_{j,n}^{+}(x)  \notag \\
&=&\frac{1}{n}\sum_{t=R}^{T-1}\left( \left[ \left( e_{1,t+1}-x\right) \right]
_{+}-\left[ \left( e_{j,t+1}-x\right) \right] _{+}\right)  \notag \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( \widehat{e}_{1,t}-e_{1,t}\right)
1\left\{ e_{1,t}\geq x\right\} -\left( \widehat{e}_{j,t}-e_{j,t}\right)
1\left\{ e_{j,t}\geq x\right\} \right)  \notag \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( e_{1,t}-x\right) \left( 1\left\{ 
\widehat{e}_{1,t}\geq x\right\} -1\left\{ \widehat{e}_{1,t}\geq x\right\}
\right) -\left( e_{j,t}-x\right) \left( 1\left\{ \widehat{e}_{j,t}\geq
x\right\} -1\left\{ e_{j,t}\geq x\right\} \right) \right)  \label{C-PEE} \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( \widehat{e}_{1,t}-e_{1,t}\right)
\left( 1\left\{ \widehat{e}_{1,t}\geq x\right\} -1\left\{ e_{1,t}\geq
x\right\} \right) -\left( \widehat{e}_{j,t}-e_{j,t}\right) \left( 1\left\{ 
\widehat{e}_{j,t}\geq x\right\} -1\left\{ e_{j,t}\geq x\right\} \right)
\right)  \notag
\end{eqnarray}%
we see that $\widetilde{\sigma }_{j,n}^{2,C+}(x)$ is defined as:%
\begin{eqnarray*}
&&\widetilde{\sigma }_{j,n}^{2,G+}(x) \\
&=&\widehat{\sigma }_{j,n}^{2,C+}(x)+2\widehat{\Pi }\widehat{f}%
_{1,n,h}^{2}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }_{11}\widetilde{A}%
_{1}^{\prime }(x)+2\widehat{\Pi }\widehat{f}_{j,n,h}^{2}(x)\widetilde{A}%
_{j}(x)\widehat{\Sigma }_{jj}\widetilde{A}_{j}^{\prime }(x) \\
&&-4\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }%
_{1j}\widetilde{A}_{j}^{\prime }(x)\widehat{f}_{j,n,h}(x)+2\widehat{\Pi }%
\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }_{u1}(x)-2%
\widehat{\Pi }\widehat{f}_{j,n,h}(x)\widetilde{A}_{j}(x)\widehat{\Sigma }%
_{uj}(x) \\
&&+2\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{11}\widetilde{B}%
_{1}^{\prime }(x)+2\widehat{\Pi }\widetilde{B}_{j}^{\prime }(x)\widehat{%
\Sigma }_{jj}\widetilde{B}_{j}^{\prime }(x)-4\widehat{\Pi }\widetilde{B}%
_{1}(x)\widehat{\Sigma }_{1j}\widetilde{B}_{j}^{\prime }(x) \\
&&+2\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{u1}(x)-2\widehat{%
\Pi }\widetilde{B}_{j}(x)^{\prime }\widehat{\Sigma }_{uj}(x) \\
&&+2\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }%
_{11}\widetilde{B}_{1}^{\prime }(x)+2\widehat{\Pi }\widehat{f}_{j,n,h}(x)%
\widetilde{A}_{j}\widehat{\Sigma }_{jj}\widetilde{B}_{j}^{\prime }(x)-2%
\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{1j}\widetilde{A}%
_{j}^{\prime }(x)\widehat{f}_{j,n,h}(x) \\
&&-2\widehat{\Pi }\widetilde{B}_{j}(x)\widehat{\Sigma }_{1j}\widetilde{A}%
_{1}^{\prime }(x)\widehat{f}_{1,n,h}(x),
\end{eqnarray*}%
where $\widehat{\sigma }_{j,n}^{2,C+}(x)$ is defined as in the statement of
Lemma 1, but computed using only the last $n$ observations. Also, 
\begin{equation*}
\widetilde{A}_{j}(x)=\frac{1}{n}\sum_{t=R+1}^{T}\left( \widehat{e}%
_{t+1,j}-x\right) \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\widehat{%
\theta }_{j,R}\right) ^{\prime }\left( \frac{1}{R}\sum_{t=1}^{R}\nabla
_{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{\theta }_{j,R})\right) ^{-1}
\end{equation*}%
and%
\begin{equation*}
\widetilde{B}_{j}(x)=\frac{1}{n}\sum_{t=R+1}^{T}1\left\{ \widehat{e}%
_{t+1,j}>x\right\} \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\widehat{%
\theta }_{j,R}\right) ^{\prime }\left( \frac{1}{R}\sum_{t=1}^{R}\nabla
_{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{\theta }_{j,R})\right)
^{-1}.
\end{equation*}%
In order to formalize the case of asymptotically non-vanishing parameter
estimation error, we require the following assumptions.

\noindent \textbf{Assumption A5: }$\phi _{j}$ is twice continuously
differentiable on the interior of $\Theta _{j}$ and the elements of $\nabla
_{\theta _{j}}\phi _{j}(Z_{j,i-1},\theta _{i})$ and $\nabla _{\theta
_{j}}^{2}\phi _{j}(Z_{j,i-1},\theta _{i})$ are $p-$dominated on $\Theta
_{i}, $ for $j=1,...,k,$ with $p>4.$

\noindent \textbf{Assumption A6: }For $j=1,...,k:$ (i) $\theta _{j}^{\dagger
}$ is uniquely identified (i.e. $E(m_{j}(X_{t},Z_{j,t-1},\theta
_{j}))>E(m_{j}(X_{t},Z_{j,t-1},\theta _{j}^{\dagger })),$ for any $\theta
_{j}\neq \theta _{j}^{\dagger });$ (ii) $m_{j}$ is twice continuously
differentiable on the interior of $\Theta _{j};$ (iii) the elements of $%
\nabla _{\theta _{j}}m_{j}$ and $\nabla _{\theta _{j}}^{2}m_{j}$ are $p-$%
dominated on $\Theta _{j},$ with $p>4$; and (iii) $E\left( -\nabla _{\theta
_{j}}^{2}m_{j}(\theta _{j})\right) $ is positive definite, uniformly on $%
\Theta _{j}.$\footnote{%
We say that $\nabla _{\theta _{j}}\ln f_{j}(y_{t},Z^{t-1},\theta _{j})$ is $%
2r-$dominated on $\Theta _{j}$ if its $v-th$ element, $v=1,...,4,$ is such
that $\left\vert \nabla _{\theta _{j}}\ln f_{j}(y_{t},Z^{t-1},\theta
_{j})\right\vert _{v}\leq D_{t},$ and $E(\left\vert D_{t}\right\vert
^{2r})<\infty .$ For more details on domination conditions, see Gallant and
White (1988, pp. 33).}

\noindent \textbf{Assumption A7: }$T=R+n,$ and as $T\rightarrow \infty ,$ $%
n/R\rightarrow \pi ,$ with $0\leq \pi <\infty .$

As explained earlier, it is crucial to have a consistent estimator of the
variance of the moment conditions. Otherwise, bootstrap critical values are
not scale invariant. Hence, we need to construct estimators which properly
capture parameters estimation error, regardless the fact that we rely on
bootstrap critical values. We have the following result.

\noindent \textbf{Lemma 3: }\textit{Let Assumptions A1-A3, and A5-A7 hold.
If }$l_{n}\approx n^{\delta }$\textit{\ }$\delta <\frac{1}{2},$\textit{\ as
defined in Assumption A1}$,$\textit{\ then:}

\textit{\noindent (i) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{j,n}^{2,G+}(x)-\omega _{j}^{2,G+}(x)\right\vert =o_{p}\left(
1\right) ,$\textit{\ with }$\omega _{j}^{2G+}(x)=avar\left( \sqrt{n}%
\widetilde{G}_{j,n}^{+}(x)\right) ;$\textit{\ and}

\textit{\noindent (ii) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{j,n}^{2,C+}(x)-\omega _{j}^{2,C+}(x)\right\vert =o_{p}\left(
1\right) $\textit{, with }$\omega _{j}^{2C+}(x)=avar\left( \sqrt{n}%
\widetilde{C}_{j,n}^{+}(x)\right) .$

\noindent Lemma 3 mirrors Lemma 1 for the case of non-vanishing estimation
error. In order to provide the analog of Theorem 1, we need define the
counterparts of $S_{n}^{\dag G+}$ and $S_{n}^{\dag C+}$ which take into
account of parameter estimation error, i.e.

\begin{equation*}
S_{n}^{\ddag G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max \left\{ 0,%
\frac{v_{j}^{G+}(x)+h_{j,A,n}^{G+}(x)}{\sqrt{h_{j,B}^{G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
with $h_{j,A,n}^{G+}(x)$ and $h_{j,B,n}^{G+}(x)$ be defined as in Section
3.2 of the main paper,%
\begin{equation*}
S_{n}^{\ddag C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max \left\{ 0,%
\frac{v_{j}^{C+}(x)+h_{j,A,n}^{C+}(x)}{\sqrt{h_{j,2}^{C+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x).
\end{equation*}%
The following result holds.

\noindent \textbf{Theorem 5: }\textit{Let Assumptions A1-A7 hold. }

\textit{(i) Under }$H_{0}^{G+},$\textit{\ tthere exists }$\delta >0$\textit{%
\ and }$a^{G+}>0,$ such that%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{G+}}\left[
P\left( \widetilde{S}_{n}^{G+}>a_{{}}^{G+}\right) -P\left( S_{n}^{\ddag
G+}+\delta >a^{G+}\right) \right] \leq 0
\end{equation*}%
\textit{and}%
\begin{equation*}
\lim \inf_{n\rightarrow \infty }\inf_{P\in \mathcal{P}_{0}^{G+}}\left[
P\left( \widetilde{S}_{n}^{G+}>a^{G+}\right) -P\left( S_{n}^{\ddag
G+}-\delta >a^{G+}\right) \right] \geq 0.
\end{equation*}%
\textit{\noindent \qquad }

\textit{(ii) Under }$H_{0}^{C+},$\textit{\ there exist }$\delta >0$\textit{\
such that:}%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( \widetilde{S}_{n}^{C+}>a^{C+}\right) -P\left( S_{n}^{\ddag
C+}+\delta >a^{C+}\right) \right] \leq 0
\end{equation*}%
\textit{and}%
\begin{equation*}
\lim \inf_{n\rightarrow \infty }\inf_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( \widetilde{S}_{n}^{C+}>a_{h_{A,n}}^{C+}\right) -P\left( S_{n}^{\ddag
C+}-\delta >a_{h_{A,n}}^{C+}\right) \right] \geq 0.
\end{equation*}%
Theorem 5 provides upper and lower bounds for $P\left( \widetilde{S}%
_{n}^{G+}>a^{G+}\right) $ and $P\left( \widetilde{S}_{n}^{C+}>a^{C+}\right)
, $ uniformly, over the probabilities under the null $H_{0}^{G+}$ and $%
H_{0}^{C+},$ respectively.

\subsection{Bootstrap Estimators}

When computing recursive $m-$estimators, it is important to note that
earlier observations are used more frequently than temporally subsequent
observations. On the other hand, in the standard block bootstrap, all blocks
from the original sample have the same probability of being selected,
regardless of the dates of the observations in the blocks. Thus, the
bootstrap estimator, say $\widehat{\theta }_{j,t}^{\ast },$ which is
constructed as a direct analog of $\widehat{\theta }_{j,t}$ in (\ref{theta-i}%
)$,$ is characterized by a location bias that can be either positive or
negative, depending on the sample that we observe. In order to circumvent
this problem, Corradi and Swanson (2007) suggest a re-centering of the
bootstrap score which ensures that the new bootstrap estimator is
asymptotically unbiased. Also, assume that $T=R+n=b_{T}l_{T},$ with $%
b_{T}=b_{n}\frac{T}{n}$ and $l_{T}=l_{n}\frac{T}{n},$ and define:%
\begin{equation*}
\widetilde{\theta }_{j,t}^{\ast }=\arg \min_{\theta _{j}\in \Theta _{j}}%
\frac{1}{t}\sum_{i=1}^{t}\left( m_{j}(X_{i}^{\ast },Z_{j,i-1}^{\ast },\theta
_{j})-\theta _{j}^{\prime }\left( \frac{1}{T}\sum_{k=1}^{T-1}\nabla _{\theta
_{j}}m_{j}(X_{k},Z_{j,k-1},\widehat{\theta }_{j,t})\right) \right) ,
\end{equation*}%
where $X_{i}^{\ast },Z_{j,i-1}^{\ast }$ are resampled via the
\textquotedblleft standard\textquotedblright\ block bootstrap outlined in
the previous section, but with block length $l_{T}.$ Theorem 1 in Corradi
and Swanson (2007) establish that $\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( 
\widetilde{\theta }_{j,t}^{\ast }-\widehat{\theta }_{j,t}\right) $ has the
same limiting distribution as $\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dagger }\right) ,$ conditional of the
sample.

With a slight abuse of notation, let $u_{j,t}^{\ast }(x)=1\{e_{j,t}^{\ast
}\leq x\}-\frac{1}{T}\sum_{t=1}^{T}1\{\widehat{e}_{j,t}\leq x\}$ and $\eta
_{j,t}^{\ast }(x)=\left[ e_{j,t}^{\ast }-x\right] _{+}-\frac{1}{T}%
\sum_{t=1}^{n}\left[ \widehat{e}_{j,t}-x\right] _{+},$ with $e_{j,t+1}^{\ast
}=X_{t+1}^{\ast }-\phi _{j}\left( Z_{j,t}^{\ast },\widehat{\theta }%
_{j,t}\right) $, and let $\widehat{u}_{j,t}^{\ast }(x)=1\{\widehat{e}%
_{j,t}^{\ast }\leq x\}-\frac{1}{T}\sum_{t=1}^{T}1\{\widehat{e}_{j,t}\leq x\}$
and $\widehat{\eta }_{j,t}^{\ast }(x)=\left[ \widehat{e}_{j,t}^{\ast }-x%
\right] _{+}-\frac{1}{T}\sum_{t=1}^{n}\left[ \widehat{e}_{j,t}-x\right]
_{+}, $ with $\widehat{e}_{j,t+1}^{\ast }=X_{t+1}^{\ast }-\phi _{j}\left(
Z_{j,t}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) .$ Our first goal
is to construct the bootstrap counterparts of $\widetilde{\sigma }%
_{j,n}^{2,G+}(x)$ and $\widetilde{\sigma }_{j,n}^{2,C+}(x)$, called $%
\widetilde{\sigma }_{j,n}^{\ast 2,G+}(x)$ and $\widetilde{\sigma }%
_{j,n}^{\ast 2,C+}(x).$ Define:

\begin{eqnarray*}
&&\widetilde{\sigma }_{j,n}^{\ast 2,G+}(x) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{u}_{j,t}^{\ast }(x)-\widehat{u}_{1,t}^{\ast
}(x)\right) \right) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) \right) +%
\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}%
\left( \widehat{f}_{j,n,h}^{\ast }(x)\widehat{PEE^{\ast }}_{j,t}-\widehat{f}%
_{1,n,h}^{\ast }(x)\widehat{PEE^{\ast }}_{1,t}\right) \right) \\
&&-2\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) ,\frac{1%
}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{f}_{j,n,h}^{\ast }(x)\widehat{%
PEE^{\ast }}_{j,t}-\widehat{f}_{1,n,h}^{\ast }(x)\widehat{PEE^{\ast }}%
_{1,t}\right) \right) ,
\end{eqnarray*}%
and%
\begin{eqnarray*}
&&\widetilde{\sigma }_{j,n}^{\ast 2,C+}(x) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{\eta }_{j,t}^{\ast }(x)-\widehat{\eta }%
_{1,t}^{\ast }(x)\right) \right) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast }(x)\right)
\right) \\
&&+\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \left[ \widehat{f}_{j,n,h}^{\ast }\widehat{PEE}%
_{j,t}^{\ast }-x\right] _{+}-\left[ \widehat{f}_{1,n,h}^{\ast }\widehat{PEE}%
_{1,t}^{\ast }-x\right] _{+}\right) \right) + \\
&&-2\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast }(x)\right) ,%
\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \left[ \widehat{f}_{j,n,h}^{\ast }%
\widehat{PEE}_{j,t}^{\ast }-x\right] _{+}-\left[ \widehat{f}_{1,n,h}^{\ast }%
\widehat{PEE}_{1,t}^{\ast }-x\right] _{+}\right) \right) ,
\end{eqnarray*}%
where \textrm{avar}$^{\ast }$ and $\mathrm{acov}^{\ast }$ denote asymptotic
variances and covariances, with respect to the bootstrap probability
measure, $\widehat{f}_{j,n,h}^{\ast }$ is an estimator of the density of $%
e_{j}$ based on the resampled observations, and $\widehat{PEE^{\ast }}_{j,t}$
is an estimator of:%
\begin{eqnarray}
PEE_{j,t}^{\ast } &=&\mathrm{E}^{\ast }\left( \nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) \right) 
\mathrm{E}^{\ast }\left( \nabla _{\theta }^{2}m_{j}\left( X_{i}^{\ast
},Z_{j,i-1}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) \right)  \notag
\\
&&\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta }m_{j}\left( X_{i}^{\ast
},Z_{j,i-1}^{\ast },\widehat{\theta }_{j,t}\right) -\frac{1}{T}%
\sum_{i=1}^{T}\nabla _{\theta _{j}}m_{j}(X_{k},Z_{j,k-1},\widehat{\theta }%
_{j,t}\right) .  \label{PEE*}
\end{eqnarray}%
Closed form expressions for $\widehat{PEE^{\ast }}_{j,t}$, $\widehat{\mathrm{%
avar}^{\ast }}$, and $\widehat{\mathrm{acov}^{\ast }}$ are given in the
proof of Lemma 4.

\noindent \textbf{Lemma 4: }\textit{Let Assumptions A1-A3 and A5-A7 hold.
Then, if }$l_{n}\approx n^{\delta }$\textit{\ }$\delta <\frac{1}{2},$\textit{%
\ and }$\beta $\textit{\ the mixing coefficient in Assumption A1 is such
that }$\beta >\frac{6\delta }{1-2\delta }:$

\textit{\noindent (i) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{j,n}^{\ast 2,G+}(x)-\widetilde{\sigma }_{j,n}^{2,G+}(x)\right\vert
=o_{p}^{\ast }\left( 1\right) $\textit{\ and}

\textit{\noindent (ii) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{j,n}^{\ast 2,C+}(x)-\widetilde{\sigma }_{j,n}^{2,C+}(x)\right\vert
=o_{p}^{\ast }\left( 1\right) .$\textit{\ }

\subsection{Bootstrap Critical Values}

The bootstrap statistics in the non-vanishing recursive parameter estimation
error case are:%
\begin{equation}
\widetilde{S}_{n}^{\ast G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left(
\left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast G+}(x)-\widetilde{\phi }%
_{j,n}^{G+}(x)}{\sqrt{\widetilde{\overline{h}}_{B,j}^{\ast G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),  \label{SG*PEE}
\end{equation}%
with 
\begin{eqnarray*}
\widetilde{v}_{n}^{\ast G+}(x) &=&\frac{1}{\widetilde{\sigma }_{j,n}^{\ast
2,G+}(x)\sqrt{n}}\sum_{i=R+1}^{n}\left( \left( 1\left\{ \widehat{e}%
_{j,i}^{\ast }\leq x\right\} -1\left\{ \widehat{e}_{1,i}^{\ast }\leq
x\right\} \right) \right. \\
&&\left. \frac{1}{T}\sum_{t=1}^{T}\left( 1\left\{ \widehat{e}_{j,t}\leq
x\right\} -1\left\{ \widehat{e}_{1,t}\leq x\right\} \right) \right)
\end{eqnarray*}%
\begin{equation*}
\widetilde{\overline{h}}_{B,j}^{\ast G+}(x)=\sqrt{\frac{\widetilde{\sigma }%
_{j,n}^{\ast 2,G+}(x)+\varepsilon }{\widetilde{\sigma }_{j,n}^{\ast 2,G+}(x)}%
}
\end{equation*}%
\begin{equation*}
\widetilde{\phi }_{j,n}^{G+}(x)=c_{n}1\left\{ \widetilde{\xi }%
_{j,n}^{G+}(x)<-1\right\}
\end{equation*}%
with $\widetilde{\xi }_{j,n}^{G+}(x)=\frac{\widetilde{G}_{jn}^{+}(x)}{\sqrt{%
\widetilde{\sigma }_{j,n}^{\ast 2,G+}(x)+\varepsilon }}$. Finally, also
define 
\begin{equation*}
\widetilde{S}_{n}^{\ast C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left(
\left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast C+}(x)-\widetilde{\phi }%
_{j,n}^{C+}(x)}{\sqrt{\widetilde{\overline{h}}_{B,j}^{\ast C+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
where $\widetilde{v}_{n}^{\ast C+}(x),\widetilde{\xi }_{j,n}^{C+}(x),$ and $%
\widetilde{\phi }_{j,n}^{C+}(x)$ are defined analogously to $\widetilde{v}%
_{n}^{\ast G+}(x),\widetilde{\xi }_{j,n}^{G+}(x),$ and $\widetilde{\phi }%
_{j,n}^{G+}(x)$. It is immediate to see that estimation error contributes to
the bootstrap statistics not only as a scaling factor, but also in
determining which moment conditions are binding. This is why we need an
estimator of the variance, even if inference is based on bootstrap critical
values.

We now define the GMS bootstrap critical values for the case of
non-vanishing recursive estimation error. Let $\widetilde{c}_{n,B,1-\alpha
}^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},\widetilde{\overline{h}}%
_{B,j}^{\ast G+}\right) $ be the $(1-\alpha )$-th \ critical value of $%
\widetilde{S}_{n}^{\ast G+},$ based on $B$ bootstrap replications, with $%
\widetilde{\phi }_{n}^{G+}$ and $\widetilde{\overline{h}}_{B,j}^{\ast G+}(x)$
defined above. The ($1-\alpha )$-th GMS bootstrap critical value, $%
\widetilde{c}_{0,n,1-\alpha }^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},%
\widetilde{\overline{h}}_{B,j}^{\ast G+}\right) $ is defined as:%
\begin{equation*}
\widetilde{c}_{0,n,1-\alpha }^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},%
\widetilde{\overline{h}}_{B,j}^{\ast G+}\right) =\lim_{B\rightarrow \infty }%
\widetilde{c}_{n,B,1-\alpha +\eta }^{\ast G+}\left( \widetilde{\phi }%
_{n}^{G+},\widetilde{\overline{h}}_{B,j}^{\ast G+}\right) +\eta ,
\end{equation*}%
for arbitrarily small $\eta >0$. Also, $\widetilde{c}_{n,B,1-\alpha +\eta
}^{\ast C+}\left( \widetilde{\phi }_{n}^{C+},\widetilde{\overline{h}}%
_{B,j}^{\ast C+}\right) $ and $\widetilde{c}_{0,n,1-\alpha }^{\ast C+}\left( 
\widetilde{\phi }_{n}^{C+},\widetilde{\overline{h}}_{B,j}^{\ast C+}\right) $
are defined analogously. The following result then holds.

\noindent \textbf{Theorem 6: }Let Assumptions A1-A7 hold, and let $%
l_{n}\rightarrow \infty $ and $l_{n}n^{\frac{1}{3}-\varepsilon }\rightarrow
0,$ as $n\rightarrow \infty .$ Under $H_{0}^{G+}:$

\noindent (i) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow \infty $
and $c_{n}/\kappa _{n}\rightarrow 0,$ then%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{G+}}P\left( 
\widetilde{S}_{n}^{G+}\geq \widetilde{c}_{n,B,1-\alpha +\eta }^{\ast
G+}\left( \widetilde{\phi }_{n}^{G+},\widetilde{\overline{h}}_{B,j}^{\ast
G+}\right) \right) \leq \alpha ;
\end{equation*}%
\noindent and (ii) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow
\infty ,$ $c_{n}\rightarrow \infty $, $\sqrt{n}/\kappa _{n}\rightarrow
\infty $ and $Q\left( \mathcal{B}^{G+}\right) >0,$ $\mathcal{B}^{G+}$ as in
Eq. (3.13), then%
\begin{equation*}
\lim_{\eta \rightarrow 0}\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{%
P}_{0}^{G+}}P\left( \widetilde{S}_{n}^{G+}\geq \widetilde{c}_{n,B,1-\alpha
+\eta }^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},\widetilde{\overline{h}}%
_{B,j}^{\ast G+}\right) \right) =\alpha .
\end{equation*}%
\noindent Also, under $H_{0}^{C+},$

\noindent (iii) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow \infty 
$ and $c_{n}/\kappa _{n}\rightarrow 0,$ then%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}P\left( 
\widetilde{S}_{n}^{C+}\geq \widetilde{c}_{0,n,1-\alpha }^{\ast C+}\left( 
\widetilde{\phi }_{n}^{C+},\widetilde{\overline{h}}_{B,j}^{\ast C+}\right)
\right) \leq \alpha ;
\end{equation*}%
\noindent and (iv) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow
\infty ,$ $c_{n}\rightarrow \infty $, $\sqrt{n}/\kappa _{n}\rightarrow
\infty $ and $Q\left( \mathcal{B}^{C+}\right) >0,$ $\mathcal{B}^{C+}$ as in
Eq. (3.14), then%
\begin{equation*}
\lim_{\eta \rightarrow 0}\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{%
P}_{0}^{C+}}P\left( \widetilde{S}_{n}^{C+}\geq \widetilde{c}_{0,n,1-\alpha
}^{\ast C+}\left( \widetilde{\phi }_{n}^{C+},\widetilde{\overline{h}}%
_{B,j}^{\ast C+}\right) \right) =\alpha .
\end{equation*}

Statements (i) and (iii) of Theorem 6 establish that inference based on GMS
bootstrap critical values has uniform correct size, in the parameter
estimation error case. Statements (ii) and (iv) of the theorem establish
that inference based on the GMS bootstrap critical values is asymptotically
non-conservative, whenever $Q\left( \mathcal{B}^{+}\right) >0$ or $Q\left( 
\mathcal{B}^{C+}\right) >0.$

\subsection{Proofs}

\noindent \textbf{Proof of Lemma 3: (i)} Letting $\overline{F}_{j}(x)=\frac{1%
}{n}\sum_{t=R}^{T-1}1\left\{ \widehat{e}_{j,t+1}\leq x\right\} ,$ by an
intermediate value expansion, in the case of a recursive estimation scheme,
we have that%
\begin{eqnarray}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ \widehat{e}_{j,t+1}\leq
x\right\} -F_{j}(x)\right)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ \widehat{%
e}_{j,t+1}\leq x\right\} -1\left\{ e_{j,t+1}\leq x\right\} \right)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{
e_{j,t+1}\leq x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{%
\theta }_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag
}\right) \right\} \right. \right.  \notag \\
&&\left. \left. -F_{j}\left( x-\nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( \widehat{\theta }%
_{j,t}-\theta _{j}^{\dag }\right) \right) \right) -\frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\} -F_{j}(x)\right)
\right)  \notag \\
&&+\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( F_{j}\left( x-\nabla _{\theta
_{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) \right) -F_{j}(x)\right) 
\notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( F_{j}\left(
x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }%
_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right)
\right) -F_{j}(x)\right)  \notag \\
&&+o_{p}(1)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) -f_{j}(x)\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\nabla _{\theta
_{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) +o_{p}(1)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right)  \label{der} \\
&&-f_{j}(x)\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) ^{\prime }\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) ^{-1}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) +o_{p}(1)  \notag
\\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) -f_{j}(x)\widehat{A}_{j}\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}%
\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) +o_{p}(1)  \notag
\end{eqnarray}%
where the $o_{p}(1)$ term on the RHS of the third equality in (\ref{der})
comes from the fact that%
\begin{eqnarray*}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x-\nabla
_{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right)
\left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) \right\} \right. \\
&&\left. -F_{j}\left( x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},%
\overline{\theta }_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta
_{j}^{\dag }\right) \right) \right) -\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}%
\left( 1\left\{ e_{j,t+1}\leq x\right\} -F_{j}(x)\right) =o_{p}(1),
\end{eqnarray*}%
because of stochastic equicontinuity.

Hence,%
\begin{eqnarray*}
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ \widehat{e}_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{ 
\widehat{e}_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \right) \\
&=&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \right) \\
&&+f_{1}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{1}}m_{1}(X_{i},Z_{1,i-1},\theta
_{1}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left( Z_{1,t+1},\theta
_{1}^{\dagger }\right) \right) \\
&&+f_{j}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\theta _{j}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\theta
_{j}^{\dagger }\right) \right) \\
&&-2f_{1}(x)f_{j}(x)\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{j}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{1}}m_{1}(X_{i},Z_{1,i-1},\theta
_{1}^{\dagger })\right) \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\theta
_{j}^{\dagger }\right) \right) \\
&&+2f_{1}(x)\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{1}}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger })\right) \right) \\
&&-2f_{j}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\theta _{j}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right)
\end{eqnarray*}%
(ii) Recalling (\ref{C-PEE}) by a similar argument as in part (i).

\medskip

\noindent \textbf{Proof of Theorem 5:}

\noindent Given Lemma 3, the statement follows by the same argument as in
Theorem 1.

\medskip

\noindent \textbf{Proof of Lemma 4:}

\noindent (i) Note that $\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{%
\sqrt{n}}\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast
}(x)\right) \right) =\widehat{\sigma }_{j,n}^{2\ast G+}(x)$ as defined in
Eq. (3.1), $\widehat{PEE^{\ast }}_{j,t}$ is defined as $PEE_{j,t}^{\ast }$
with \textrm{E}$^{\ast }$ replaced by an average, also%
\begin{eqnarray*}
&&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-\widehat{PEE^{\ast }}%
_{1,t}\right) \right) \\
&=&\frac{1}{b_{n}}\sum_{k=1}^{b_{n}}\left( \frac{1}{l_{n}^{1/2}}%
\sum_{i=1}^{l_{n}}\left( \widehat{PEE^{\ast }}_{j,(k-1)l_{n}+i}-\widehat{%
PEE^{\ast }}_{j,(k-1)l_{n}+i}\right) \right) ^{2}
\end{eqnarray*}%
and by Theorem 1 in Corradi and Swanson (2007),%
\begin{eqnarray*}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-%
\widehat{PEE^{\ast }}_{1,t}\right) \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE}_{j,t}-\widehat{PEE}%
_{1,t}\right) +o_{p}(1)^{\ast }.
\end{eqnarray*}%
\begin{eqnarray*}
&&\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) ,\frac{1%
}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-\widehat{%
PEE^{\ast }}_{1,t}\right) \right) \\
&=&\frac{1}{b_{n}}\sum_{k=1}^{b_{n}}\left( \frac{1}{l_{n}^{1/2}}%
\sum_{i=1}^{l_{n}}\left( \widehat{PEE^{\ast }}_{j,(k-1)l_{n}+i}-\widehat{%
PEE^{\ast }}_{j,(k-1)l_{n}+i}\right) \right. \\
&&\left. \frac{1}{l_{n}^{1/2}}\sum_{i=1}^{l_{n}}\left( u_{j,t}^{\ast
}(x)-u_{1,t}^{\ast }(x)\right) \right)
\end{eqnarray*}%
and for $h\rightarrow 0,$ $nh\rightarrow \infty ,$ $\widehat{f}%
_{j,n,h}^{\ast }(x)=\widehat{f}_{j,n,h}(x)+o_{p^{\ast
}}(1)=f(x)+o_{p}(1)+o_{p^{\ast }}(1).$ The statement then follow by the same
argument as in Lemma 2 and Lemma 3.

\noindent (ii) By a similar argument as in Part (i).

\medskip

\noindent \textbf{Proof of Theorem 6:}

\noindent (i) By a similar argument as in the proof of Theorem 2 in Corradi
and Swanson (2007),%
\begin{eqnarray*}
\widetilde{S}_{n}^{\ast G+} &=&\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max
\left( \left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast G+}(x)-\widetilde{\phi }%
_{j,n}^{G+}(x)}{\sqrt{\widetilde{\overline{h}}_{2,jj}^{\ast G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x) \\
&=&\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left( \left\{ 0,\frac{%
\widetilde{v}_{j,n}^{G+}(x)-\widetilde{\phi }_{j,n}^{G+}(x)}{\sqrt{%
\widetilde{\overline{h}}_{2,jj}^{G+}(x)}}\right\} \right) ^{2}\mathrm{d}%
Q(x)+o_{p^{\ast }}(1)
\end{eqnarray*}%
The statement then follows from Lemma 4 and Theorem 2.

\noindent (ii) By a similar argument as in Part (i).

\section{Appendix SA3: The JCS Test}

In this section, we provide details on the JCS test used in the Monte Carlo
and empirical sections of the paper, which deal with forecast superiority of
judgmental forecasts. For the general case in which we compare model based
prediction, please refer to Jin, Corradi, and Swanson (2017). For $j=2,...,k$%
, let $G_{j,n}(x)$ be defined as in Eq.(2.3) in Section 2 of the paper, and $%
C_{j,n}(x)$ be defined as in Eq.(\ref{Cj}) in Appendix SA1. Then, 
\begin{equation*}
JCS_{n}^{G^{+}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{+}}\sqrt{n}%
G_{j,n}(x)\text{ and }JCS_{n}^{G^{-}}=\underset{j=2,..,k}{\max }\sup_{x\in 
\mathcal{X}^{-}}\sqrt{n}G_{j,n}(x)
\end{equation*}%
and%
\begin{equation*}
JCS_{n}^{C^{+}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{+}}\sqrt{n}%
C_{j,n}(x)\text{ and }JCS_{n}^{C^{-}}=\underset{j=2,..,k}{\max }\sup_{x\in 
\mathcal{X}^{-}}\sqrt{n}C_{j,n}(x)
\end{equation*}%
JCS uses the the stationary boostrap of Politis and Romano (1994) with
smoothing parameter $J_{n}.$ For $j=2,...,k,$ let $G_{k,n}^{\ast }(x)$ and $%
C_{k,n}^{\ast }(x)$ be the bootstrap analogs of $G_{k,n}(x)$ and $%
G_{k,n}(x), $ then boostrap counterparts of $%
JCS_{n}^{G^{+}},JCS_{n}^{G^{-}},JCS_{n}^{C^{+}}$ and $JCS_{n}^{C^{-}}$ read
as%
\begin{equation*}
JCS_{n}^{\ast G^{+}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{+}}%
\sqrt{n}\left( G_{j,n}^{\ast }(x)-G_{j,n}(x)\right) \text{ and }%
JCS_{n}^{\ast G^{-}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{-}}%
\sqrt{n}\left( G_{j,n}^{\ast }(x)-G_{j,n}(x)\right)
\end{equation*}%
and%
\begin{equation*}
JCS_{n}^{\ast C^{+}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{+}}%
\sqrt{n}\left( C_{j,n}^{\ast }(x)-C_{j,n}(x)\right) \text{ and }%
JCS_{n}^{\ast G^{-}}=\underset{j=2,..,k}{\max }\sup_{x\in \mathcal{X}^{-}}%
\sqrt{n}\left( C_{j,n}^{\ast }(x)-C_{j,n}(x)\right)
\end{equation*}%
JCS bootstap critical values are then obtained via the empirical
distribution of $JCS_{n}^{\ast G^{+}},JCS_{n}^{\ast G^{-}},JCS_{n}^{\ast
C^{+}}$ and $JCS_{n}^{\ast C^{-}}.$ They key difference with the new tests
is that in JCS forecasts which are dominated by the benchmark do not
contribute to the statistic but do contribute to its bootstrap counterpart.
Hence, inference based on JCS tests is asymptotically non conservative only
in the least favorable case in the null.

\section{Appendix SA4: Additional Monte Carlo Experimental Results}

In this section, experimental results are tabulated for the following DGPs:

\noindent DGP1: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$ and $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3.$

\noindent DGP2: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$ and $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3,4,5.$

\noindent DGP3: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$, $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3$ and $e_{kt}$ $\sim i.i.d.N(0,1.4^{2}),$ $k=4,5$

\noindent DGP4: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$, $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3$ and $e_{kt}$ $\sim $ $i.i.d.N(0,1.6^{2}),$ $k=4,5$

\noindent DGP5: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$, $e_{kt}$ $\sim $ $%
i.i.d.N(0,0.8^{2}),$ $k=2,3$ and $e_{kt}$ $\sim $ $i.i.d.N(0,1.2^{2}),$ $%
k=4,5.$

\noindent DGP6: $e_{1t}$ $\sim $ $i.i.d.N(0,1),$ $e_{kt}$ $\sim $ $%
i.i.d.N(0,0.8^{2}),$ $k=2,3,4,5$ and $e_{kt}$ $\sim $ $i.i.d.N(0,1.2^{2}),$ $%
k=6,7,8,9.$

\noindent DGP7: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$, $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3$ and $e_{kt}$ $\sim $ $i.i.d.N(0,0.8^{2}),$ $k=4,5.$

\noindent DGP8: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$, $e_{kt}$ $\sim $ $%
i.i.d.N(0,1),$ $k=2,3$ and $e_{kt}$ $\sim $ $i.i.d.N(0,0.6^{2}),$ $k=4,5.$

\noindent DGP9: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$ and $e_{kt}$ $\sim $ $%
i.i.d.N(0,0.8^{2}),$ $k=2,3,4,5.$

\noindent DGP10: $e_{1t}$ $\sim $ $i.i.d.N(0,1)$ and $e_{kt}$ $\sim $ $%
i.i.d.N(0,0.6^{2}),$ $k=2,3,4,5.$

See Tables 1-2 for tabulated results, and Section 4 of the paper for
complete details regarding the experiments that were run.

\section{Appendix SA5: Additional Empirical Results}

Tables 3-4 gather root mean square forecast errors associated with the
models reported on in Tables 3 and 4 of the paper. These results are for
nominal GDP. Results based on the same set of empirical experiments carried
out using real GDP are reported in Tables 5-8. See Section 5 of the paper
for a complete discussion.

\section{References}

\noindent Gallant, R. and H. White (1988). \textbf{A Unified Theory of
Estimation and Inference for Nonlinear Dynamic Models}. Blackwell, New York
.\medskip

\noindent Jin, S., V. Corradi and N.R. Swanson (2017). Robust Forecast
Comparison. \textit{Econometric Theory, }33, 1306-1351.\medskip

\noindent Politis, D. N., J. P. Romano (1994a). The Stationary Bootstrap. 
\textit{Journal of the American Statistical Association} 89, 1303-1313.

\newpage

\bigskip

\begin{table}[tbp]
\caption{Supplemental Monte Carlo Results: $JCS_n^{G+}$, $JCS_n^{G-}$, $%
JCS_n^{C+}$, and $JCS_n^{C-}$ Tests$^*$}{\centering}
\par
\hspace{1cm}
\par
\begin{tabular}{cc|cccc|cccc}
\hline\hline
$DGP$ & $n$ & {$J_n = 0.20$} & {$J_n = 0.35$} & {$J_n = 0.50$} & {$J_n =
0.65 $} & {$J_n = 0.20$} & {$J_n = 0.35$} & {$J_n = 0.50$} & {$J_n = 0.65$}
\\ 
&  & \multicolumn{4}{c}{GL Forecast Superiority} & \multicolumn{4}{c}{CL
Forecast Superiority} \\ \hline
&  & \multicolumn{8}{c}{$Empirical$ $Size$} \\ \hline
DGP1 & 300 & 0.113 & 0.100 & 0.101 & 0.112 & 0.113 & 0.107 & 0.120 & 0.115
\\ 
& 600 & 0.105 & 0.110 & 0.099 & 0.108 & 0.091 & 0.089 & 0.094 & 0.091 \\ 
& 900 & 0.102 & 0.095 & 0.093 & 0.096 & 0.094 & 0.092 & 0.082 & 0.094 \\ 
\hline
DGP2 & 300 & 0.110 & 0.105 & 0.115 & 0.113 & 0.097 & 0.097 & 0.092 & 0.106
\\ 
& 600 & 0.077 & 0.073 & 0.082 & 0.079 & 0.090 & 0.092 & 0.089 & 0.081 \\ 
& 900 & 0.085 & 0.084 & 0.086 & 0.095 & 0.089 & 0.101 & 0.097 & 0.092 \\ 
\hline
DGP3 & 300 & 0.070 & 0.065 & 0.065 & 0.060 & 0.030 & 0.030 & 0.035 & 0.035
\\ 
& 600 & 0.050 & 0.040 & 0.050 & 0.045 & 0.030 & 0.020 & 0.030 & 0.025 \\ 
& 900 & 0.070 & 0.070 & 0.080 & 0.075 & 0.020 & 0.020 & 0.020 & 0.020 \\ 
\hline
DGP4 & 300 & 0.065 & 0.070 & 0.060 & 0.065 & 0.010 & 0.015 & 0.020 & 0.015
\\ 
& 600 & 0.040 & 0.040 & 0.040 & 0.055 & 0.015 & 0.015 & 0.015 & 0.020 \\ 
& 900 & 0.070 & 0.065 & 0.065 & 0.065 & 0.015 & 0.015 & 0.015 & 0.015 \\ 
\hline
&  & \multicolumn{8}{c}{$Empirical$ $Power$} \\ \hline
DGP5 & 300 & 0.496 & 0.485 & 0.490 & 0.477 & 0.753 & 0.759 & 0.745 & 0.759
\\ 
& 600 & 0.775 & 0.771 & 0.773 & 0.773 & 0.991 & 0.986 & 0.989 & 0.981 \\ 
& 900 & 0.943 & 0.951 & 0.947 & 0.938 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP6 & 300 & 0.483 & 0.480 & 0.476 & 0.474 & 0.758 & 0.741 & 0.745 & 0.736
\\ 
& 600 & 0.768 & 0.778 & 0.772 & 0.774 & 0.984 & 0.975 & 0.981 & 0.980 \\ 
& 900 & 0.954 & 0.949 & 0.954 & 0.947 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP7 & 300 & 0.490 & 0.475 & 0.475 & 0.445 & 0.875 & 0.865 & 0.845 & 0.855
\\ 
& 600 & 0.835 & 0.820 & 0.820 & 0.800 & 0.995 & 0.995 & 0.995 & 0.995 \\ 
& 900 & 0.975 & 0.970 & 0.970 & 0.965 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP8 & 300 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP9 & 300 & 0.643 & 0.660 & 0.650 & 0.629 & 0.949 & 0.944 & 0.937 & 0.948
\\ 
& 600 & 0.913 & 0.885 & 0.890 & 0.896 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 0.990 & 0.986 & 0.984 & 0.983 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP10 & 300 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.1in}
{\noindent $^{*}$ Notes: Entries denote rejection frequencies of ($JCS_n^{G+}$,$JCS_n^{G-}$) tests (i.e., GL forecast superiority) and ($JCS_n^{C+}$,$JCS_n^{C-}$) tests 
(i.e., CL forecast superiority) under a variety of data generating processes 
denoted by DGP1-DGP10. In DGP1-DGP4, no alternative outperforms the benchmark model. In DGP5-DGP10, at least one alternative model outperfroms the benchmark model.
Sample sizes include $n$=300, 600, and 900 observations, as indicated in the second column of entries in the table. Nominal test size is 10\%, and tests 
are carried out using critical values 
constructed for values of $J_n$ including 0.20, 0.35, 0.50, and 0.65. See Section 4 for complete details.}
\end{minipage}
\end{table}

\begin{table}[tbp]
\caption{Supplemental Monte Carlo Results: $S_n^{G+}$, $S_n^{G-}$, $S_n^{C+}$%
, and $S_n^{C-}$ Tests$^*$}{\centering}
\par
\hspace{1cm}
\par
\begin{tabular}{cc|cccc|cccc}
\hline\hline
$DGP$ & $n$ & {$\eta = 0.0015$} & {$\eta = 0.002$} & {$\eta = 0.0025$} & {$%
\eta = 0.003$} & {$\eta = 0.0015$} & {$\eta = 0.002$} & {$\eta = 0.0025$} & {%
$\eta = 0.003$} \\ 
&  & \multicolumn{4}{c}{GL Forecast Superiority} & \multicolumn{4}{c}{CL
Forecast Superiority} \\ \hline
&  & \multicolumn{8}{c}{$Empirical$ $Size$} \\ \hline
DGP1 & 300 & 0.078 & 0.078 & 0.076 & 0.076 & 0.095 & 0.094 & 0.094 & 0.091
\\ 
& 600 & 0.096 & 0.096 & 0.095 & 0.095 & 0.116 & 0.116 & 0.115 & 0.114 \\ 
& 900 & 0.120 & 0.119 & 0.118 & 0.117 & 0.096 & 0.096 & 0.095 & 0.095 \\ 
\hline
DGP2 & 300 & 0.068 & 0.067 & 0.067 & 0.066 & 0.095 & 0.095 & 0.095 & 0.094
\\ 
& 600 & 0.097 & 0.096 & 0.095 & 0.095 & 0.096 & 0.096 & 0.095 & 0.094 \\ 
& 900 & 0.111 & 0.108 & 0.108 & 0.105 & 0.106 & 0.105 & 0.105 & 0.105 \\ 
\hline
DGP3 & 300 & 0.021 & 0.021 & 0.021 & 0.021 & 0.038 & 0.038 & 0.038 & 0.038
\\ 
& 600 & 0.070 & 0.070 & 0.069 & 0.068 & 0.086 & 0.086 & 0.085 & 0.085 \\ 
& 900 & 0.071 & 0.070 & 0.069 & 0.069 & 0.083 & 0.083 & 0.083 & 0.082 \\ 
\hline
DGP4 & 300 & 0.010 & 0.010 & 0.01 & 0.010 & 0.041 & 0.041 & 0.041 & 0.040 \\ 
& 600 & 0.064 & 0.064 & 0.064 & 0.063 & 0.077 & 0.077 & 0.076 & 0.076 \\ 
& 900 & 0.069 & 0.067 & 0.064 & 0.063 & 0.083 & 0.083 & 0.083 & 0.083 \\ 
\hline
&  & \multicolumn{8}{c}{$Empirical$ $Power$} \\ \hline
DGP5 & 300 & 0.911 & 0.911 & 0.910 & 0.909 & 0.972 & 0.972 & 0.971 & 0.971
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP6 & 300 & 0.957 & 0.956 & 0.956 & 0.956 & 0.989 & 0.989 & 0.989 & 0.989
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP7 & 300 & 0.874 & 0.872 & 0.871 & 0.870 & 0.925 & 0.924 & 0.923 & 0.923
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP8 & 300 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP9 & 300 & 0.995 & 0.995 & 0.995 & 0.995 & 0.995 & 0.995 & 0.995 & 0.995
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP10 & 300 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.1in}
{\noindent $^{*}$ Notes: Entries denote rejection frequencies of ($S_n^{G+}$,$S_n^{G-}$) tests (i.e., GL forecast superiority) and ($S_n^{C+}$,$S_n^{C-}$) tests (i.e., CL forecast superiority) under a variety of data generating processes 
denoted by DGP1-DGP10. In DGP1-DGP4, no alternative outperforms the benchmark model. In DGP5-DGP10, at least one alternative model outperfroms the benchmark model.
Sample sizes include $n$=300, 600, and 900 observations, as indicated in the second column of entries in the table. Nominal test size is 10\%, and tests are carried out using critical values 
constructed for values of $\eta$ including 0.0015, 0.002, 0.0025, and 0.0030. See Section 4 for complete details.}
\end{minipage}
\end{table}

\bigskip

\linespread{1.05}

\linespread{1.05} 
\begin{table}[tbp]
\caption{Supplemental Empirical Results (RMSFEs)-- SPF Forecast Pooling
Analysis of Quarterly Nominal GDP Using Mean Benchmark Model and Mean Expert
Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007237 & 0.012054 & 0.016277 & 0.020490 & 0.024470 \\ 
& alternative 2 & 0.007264 & 0.012108 & 0.016358 & 0.020575 & 0.024611 \\ 
& alternative 3 & 0.007272 & 0.012141 & 0.016328 & 0.020596 & 0.025276 \\ 
\hline
Group 2 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
\hline
Group 3 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007517 & 0.012197 & 0.015320 & 0.019503 & 0.023099 \\ 
& alternative 2 & 0.007128 & 0.012595 & 0.016236 & 0.021133 & 0.024353 \\ 
& alternative 3 & 0.007110 & 0.012292 & 0.016534 & 0.020799 & 0.024430 \\ 
\hline
Group 4 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007811 & 0.011955 & 0.015533 & 0.019227 & 0.022871 \\ 
& alternative 2 & 0.006971 & 0.012656 & 0.017039 & 0.021008 & 0.026196 \\ 
& alternative 3 & 0.007306 & 0.012764 & 0.017251 & 0.021353 & 0.025116 \\ 
\hline
Group 5 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007257 & 0.012007 & 0.015367 & 0.019122 & 0.023173 \\ 
& alternative 2 & 0.007197 & 0.012401 & 0.016357 & 0.020830 & 0.024536 \\ 
& alternative 3 & 0.007219 & 0.012215 & 0.016720 & 0.020553 & 0.024250 \\ 
\hline
Group 6 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007237 & 0.012054 & 0.016277 & 0.020490 & 0.024470 \\ 
& alternative 2 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 3 & 0.007517 & 0.012197 & 0.015320 & 0.019503 & 0.023099 \\ 
& alternative 4 & 0.007811 & 0.011955 & 0.015533 & 0.019227 & 0.022871 \\ 
& alternative 5 & 0.007257 & 0.012007 & 0.015367 & 0.019122 & 0.023173 \\ 
\hline
Group 7 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007264 & 0.012108 & 0.016358 & 0.020575 & 0.024611 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007128 & 0.012595 & 0.016236 & 0.021133 & 0.024353 \\ 
& alternative 4 & 0.006971 & 0.012656 & 0.017039 & 0.021008 & 0.026196 \\ 
& alternative 5 & 0.007197 & 0.012401 & 0.016357 & 0.020830 & 0.024536 \\ 
\hline
Group 8 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007272 & 0.012141 & 0.016328 & 0.020596 & 0.025276 \\ 
& alternative 2 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
& alternative 3 & 0.007110 & 0.012292 & 0.016534 & 0.020799 & 0.024430 \\ 
& alternative 4 & 0.007306 & 0.012764 & 0.017251 & 0.021353 & 0.025116 \\ 
& alternative 5 & 0.007219 & 0.012215 & 0.016720 & 0.020553 & 0.024250 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.05in}
{\noindent $^{*}$ Notes: Entries are root mean square forecast errors (RMSFEs) of benchmark and alternative forecasting models for $h=${0,1,2,3,4}. 
See Section 5 for complete details.}
\end{minipage}
\end{table}

\bigskip

\begin{table}[tbp]
\caption{Supplemental Empirical Results (RMSFEs) -- SPF Forecast Pooling
Analysis of Quarterly Nominal GDP Using Median Benchmark Model and Median
Expert Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007301 & 0.012033 & 0.016464 & 0.020595 & 0.024785 \\ 
& alternative 2 & 0.007340 & 0.012049 & 0.016419 & 0.020566 & 0.024728 \\ 
& alternative 3 & 0.007364 & 0.012067 & 0.016498 & 0.020632 & 0.025513 \\ 
\hline
Group 2 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
\hline
Group 3 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007825 & 0.012264 & 0.016016 & 0.019494 & 0.023117 \\ 
& alternative 2 & 0.007291 & 0.012425 & 0.016649 & 0.021205 & 0.024090 \\ 
& alternative 3 & 0.007372 & 0.012410 & 0.016961 & 0.021245 & 0.024526 \\ 
\hline
Group 4 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007893 & 0.012048 & 0.015983 & 0.018890 & 0.022662 \\ 
& alternative 2 & 0.007058 & 0.012606 & 0.017110 & 0.021198 & 0.026052 \\ 
& alternative 3 & 0.007231 & 0.012771 & 0.017289 & 0.021477 & 0.025041 \\ 
\hline
Group 5 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007459 & 0.011891 & 0.015555 & 0.019435 & 0.023215 \\ 
& alternative 2 & 0.007197 & 0.012299 & 0.016555 & 0.020814 & 0.024760 \\ 
& alternative 3 & 0.007400 & 0.012164 & 0.017024 & 0.020698 & 0.024653 \\ 
\hline
Group 6 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007301 & 0.012033 & 0.016464 & 0.020595 & 0.024785 \\ 
& alternative 2 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 3 & 0.007825 & 0.012264 & 0.016016 & 0.019494 & 0.023117 \\ 
& alternative 4 & 0.007893 & 0.012048 & 0.015983 & 0.018890 & 0.022662 \\ 
& alternative 5 & 0.007459 & 0.011891 & 0.015555 & 0.019435 & 0.023215 \\ 
\hline
Group 7 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007340 & 0.012049 & 0.016419 & 0.020566 & 0.024728 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007291 & 0.012425 & 0.016649 & 0.021205 & 0.024090 \\ 
& alternative 4 & 0.007058 & 0.012606 & 0.017110 & 0.021198 & 0.026052 \\ 
& alternative 5 & 0.007197 & 0.012299 & 0.016555 & 0.020814 & 0.024760 \\ 
\hline
Group 8 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007364 & 0.012067 & 0.016498 & 0.020632 & 0.025513 \\ 
& alternative 2 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
& alternative 3 & 0.007372 & 0.012410 & 0.016961 & 0.021245 & 0.024526 \\ 
& alternative 4 & 0.007231 & 0.012771 & 0.017289 & 0.021477 & 0.025041 \\ 
& alternative 5 & 0.007400 & 0.012164 & 0.017024 & 0.020698 & 0.024653 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}

{\noindent $^{*}$ Notes: See notes to Table 3.}
\end{minipage}
\end{table}

\begin{table}[tbp]
\caption{Supplemental Empirical Results (Test Statistics) -- SPF Forecast
Pooling Analysis of Quarterly Real GDP Using Mean Benchmark Model and Mean
Expert Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{center}
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Statistic$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & $S_n^G$ & 0.000824 & 0.002252 & 0.000001 & 0.002324 & 0.002887 \\ 
& $S_n^C$ & 0.000002 & 0.002343 & 0.012414 & 0.020483 & 0.014605 \\ 
& $JCS_n^G$ & 0.155230 & 0.077615 & 0.077615 & 0.155230 & 0.232845 \\ 
& $JCS_n^C$ & 0.000054 & 0.001349 & 0.001398 & 0.002951 & 0.002554 \\ \hline
Group 2 & $S_n^G$ & 0.001799 & 0.002707 & 0.011380 & 0.005501 & 0.002788 \\ 
& $S_n^C$ & 0.000152 & 0.009679 & 0.031745 & 0.028096 & 0.094510$^*$ \\ 
& $JCS_n^G$ & 0.232845 & 0.388075 & 1.008996$^*$ & 0.155230 & 0.543305 \\ 
& $JCS_n^C$ & 0.000560 & 0.001153 & 0.005452 & 0.006671 & 0.013698 \\ \hline
Group 3 & $S_n^G$ & 0.000988 & 0.000887 & 0.006405 & 0.008404 & 0.002418 \\ 
& $S_n^C$ & 0.000902 & 0.003550 & 0.020512 & 0.051268 & 0.044420 \\ 
& $JCS_n^G$ & 0.155230 & 0.077615 & 0.620920 & 0.543305 & 0.310460 \\ 
& $JCS_n^C$ & 0.001551 & 0.002792 & 0.002252 & 0.011279 & 0.006633 \\ \hline
Group 4 & $S_n^G$ & 0.001005 & 0.000740 & 0.008447 & 0.006479 & 0.008698 \\ 
& $S_n^C$ & 0.000206 & 0.007098 & 0.023468 & 0.038963 & 0.055278 \\ 
& $JCS_n^G$ & 0.310460 & 0.077615 & 0.698535 & 0.388075 & 0.388075 \\ 
& $JCS_n^C$ & 0.000912 & 0.003747 & 0.002675 & 0.010032 & 0.006216 \\ \hline
Group 5 & $S_n^G$ & 0.001896 & 0.003768 & 0.000848 & 0.004601 & 0.009403 \\ 
& $S_n^C$ & 0.000287 & 0.004256 & 0.033755 & 0.029876 & 0.097775 \\ 
& $JCS_n^G$ & 0.465690 & 0.155230 & 0.155230 & 0.543305$^*$ & 0.155230 \\ 
& $JCS_n^C$ & 0.000808 & 0.000114 & 0.004158$^*$ & 0.006916 & 0.008606$^*$
\\ \hline
Group 6 & $S_n^G$ & 0.000673 & 0.002223 & 0.011862 & 0.005185 & 0.003483 \\ 
& $S_n^C$ & 0.000058 & 0.001989 & 0.037812 & 0.044141 & 0.113234 \\ 
& $JCS_n^G$ & 0.232845 & 0.077615 & 0.388075 & 0.698535 & 0.543305 \\ 
& $JCS_n^C$ & 0.000431 & 0.005559 & 0.003439 & 0.003138 & 0.013694 \\ \hline
Group 7 & $S_n^G$ & 0.003083 & 0.002330 & 0.002004 & 0.006939 & 0.005146 \\ 
& $S_n^C$ & 0.001370 & 0.001372 & 0.016322 & 0.052538 & 0.062255 \\ 
& $JCS_n^G$ & 0.388075 & 0.077615 & 0.232845 & 0.232845 & 0.232845 \\ 
& $JCS_n^C$ & 0.001549 & 0.000073 & 0.002155 & 0.008447 & 0.004336 \\ \hline
Group 8 & $S_n^G$ & 0.001306 & 0.005194 & 0.003816 & 0.014861 & 0.021174 \\ 
& $S_n^C$ & 0.000062 & 0.024702 & 0.065485 & 0.071887 & 0.131380 \\ 
& $JCS_n^G$ & 0.155230 & 0.077615 & 0.232845 & 0.698535 & 0.543305 \\ 
& $JCS_n^C$ & 0.000713 & 0.003734 & 0.004056 & 0.007718 & 0.008207 \\ 
\hline\hline
\end{tabular}%
\end{center}
\par
\begin{minipage}{1\columnwidth}
\vspace{0.05in}
{\noindent $^{*}$ Notes: Entries are $S_n^G$, $S_n^C$, $JCS_n^G$, and $JCS_n^C$ test statistics reported for forecast horizons $h={0,1,2,3,4}$. More specifically, 
$S_n^G = S_n^{G+}$ $if$ $p_{B,n,S_n^{G+}}^{G+} \le p_{B,n,S_n^{G-}}^{G-}$; $otherwise$ $S_n^G = S_n^{G-}$. $S_n^C$, $JCS_n^G$, and $JCS_n^C$ are defined analogously. 
Rejections of the null of no forecast superiority at a 10\% level are denoted by a superscipt *. See Section 5 for complete details.}
\end{minipage}
\end{table}

\bigskip

\begin{table}[tbp]
\caption{Supplemental Empirical Results (Test Statistics) -- SPF Forecast
Pooling Analysis of Quarterly Real GDP Using Median Benchmark Model and
Median Expert Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{center}
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Statistic$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & $S_n^G$ & 0.000000 & 0.000000 & 0.000000 & 0.005349 & 0.000000 \\ 
& $S_n^C$ & 0.000054 & 0.001003 & 0.003760 & 0.022297 & 0.005915 \\ 
& $JCS_n^G$ & 0.000000 & 0.077615 & 0.077615 & 0.155230 & 0.000000 \\ 
& $JCS_n^C$ & 0.000186 & 0.000548 & 0.000630 & 0.002178 & 0.001092 \\ \hline
Group 2 & $S_n^G$ & 0.002210 & 0.003952 & 0.007185 & 0.003319 & 0.004003 \\ 
& $S_n^C$ & 0.000192 & 0.010594 & 0.029682 & 0.015513 & 0.071462 \\ 
& $JCS_n^G$ & 0.155230 & 0.155230 & 0.853766$^*$ & 0.077615 & 0.543305 \\ 
& $JCS_n^C$ & 0.000632 & 0.001430 & 0.005290 & 0.004591 & 0.011701 \\ \hline
Group 3 & $S_n^G$ & 0.001890 & 0.002990 & 0.008263 & 0.003040 & 0.010323 \\ 
& $S_n^C$ & 0.001800 & 0.002079 & 0.043400 & 0.017020 & 0.026739 \\ 
& $JCS_n^G$ & 0.310460 & 0.232845 & 0.543305$^*$ & 0.232845 & 0.388075 \\ 
& $JCS_n^C$ & 0.002035 & 0.000282 & 0.004896 & 0.010389 & 0.008321 \\ \hline
Group 4 & $S_n^G$ & 0.000853 & 0.002997 & 0.006742 & 0.003129 & 0.002204 \\ 
& $S_n^C$ & 0.000241 & 0.007327 & 0.034950 & 0.028378 & 0.049325 \\ 
& $JCS_n^G$ & 0.310460 & 0.155230 & 0.698535 & 0.388075 & 0.388075 \\ 
& $JCS_n^C$ & 0.000854 & 0.003705 & 0.005250 & 0.011190 & 0.007344 \\ \hline
Group 5 & $S_n^G$ & 0.000740 & 0.000748 & 0.000147 & 0.005364 & 0.010838 \\ 
& $S_n^C$ & 0.001083 & 0.004036 & 0.030223 & 0.031610 & 0.047832 \\ 
& $JCS_n^G$ & 0.155230 & 0.077615 & 0.155230 & 0.232845 & 0.388075 \\ 
& $JCS_n^C$ & 0.001163 & 0.002133 & 0.004524$^*$ & 0.005652 & 0.008434$^*$
\\ \hline
Group 6 & $S_n^G$ & 0.001219 & 0.002221 & 0.012655 & 0.003963 & 0.003489 \\ 
& $S_n^C$ & 0.000206 & 0.002231 & 0.066063 & 0.034762 & 0.103800 \\ 
& $JCS_n^G$ & 0.310460 & 0.077615 & 0.388075 & 0.310460 & 0.620920 \\ 
& $JCS_n^C$ & 0.000496 & 0.005717 & 0.004404 & 0.011729 & 0.011775 \\ \hline
Group 7 & $S_n^G$ & 0.001638 & 0.004847 & 0.001687 & 0.002461 & 0.007289 \\ 
& $S_n^C$ & 0.003125 & 0.001426 & 0.016467 & 0.037540 & 0.014719 \\ 
& $JCS_n^G$ & 0.310460 & 0.155230 & 0.155230 & 0.077615 & 0.232845 \\ 
& $JCS_n^C$ & 0.002033 & 0.000234 & 0.002944 & 0.005121 & 0.001894 \\ \hline
Group 8 & $S_n^G$ & 0.002070 & 0.005195 & 0.002250 & 0.007771 & 0.002820 \\ 
& $S_n^C$ & 0.000044 & 0.022723 & 0.055627 & 0.041172 & 0.082716 \\ 
& $JCS_n^G$ & 0.155230 & 0.077615 & 0.077615 & 0.388075 & 0.310460 \\ 
& $JCS_n^C$ & 0.000293 & 0.003702 & 0.004050 & 0.005970 & 0.005798 \\ 
\hline\hline
\end{tabular}%
\end{center}
\par
\begin{minipage}{1\columnwidth}

{\noindent $^{*}$ Notes: See notes to Table 6.}
\end{minipage}
\end{table}

\begin{table}[tbp]
\caption{Supplemental Empirical Results (RMSFEs) -- SPF Forecast Pooling
Analysis of Quarterly Real GDP Using Mean Benchmark Model and Mean Expert
Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064388 & 0.096675 & 0.120861 & 0.140806 & 0.158194 \\ 
& alternative 2 & 0.064363 & 0.096571 & 0.120695 & 0.140615 & 0.157738 \\ 
& alternative 3 & 0.064371 & 0.096558 & 0.120535 & 0.140318 & 0.15863 \\ 
\hline
Group 2 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064301 & 0.096925 & 0.120608 & 0.140357 & 0.156271 \\ 
& alternative 2 & 0.064669 & 0.096909 & 0.12127 & 0.140601 & 0.157682 \\ 
& alternative 3 & 0.064602 & 0.096323 & 0.120285 & 0.140601 & 0.156228 \\ 
\hline
Group 3 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064426 & 0.09699 & 0.120392 & 0.140216 & 0.157832 \\ 
& alternative 2 & 0.064184 & 0.096764 & 0.12081 & 0.140282 & 0.157844 \\ 
& alternative 3 & 0.064511 & 0.096548 & 0.120742 & 0.140341 & 0.157491 \\ 
\hline
Group 4 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064287 & 0.096933 & 0.120559 & 0.140288 & 0.157503 \\ 
& alternative 2 & 0.064347 & 0.096876 & 0.120944 & 0.140352 & 0.158064 \\ 
& alternative 3 & 0.06455 & 0.096397 & 0.120589 & 0.140437 & 0.156927 \\ 
\hline
Group 5 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064395 & 0.096659 & 0.120372 & 0.14074 & 0.157251 \\ 
& alternative 2 & 0.064199 & 0.09671 & 0.120781 & 0.140915 & 0.157701 \\ 
& alternative 3 & 0.064597 & 0.096458 & 0.120587 & 0.140237 & 0.157023 \\ 
\hline
Group 6 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064388 & 0.096675 & 0.120861 & 0.140806 & 0.158194 \\ 
& alternative 2 & 0.064301 & 0.096925 & 0.120608 & 0.140357 & 0.156271 \\ 
& alternative 3 & 0.064426 & 0.09699 & 0.120392 & 0.140216 & 0.157832 \\ 
& alternative 4 & 0.064287 & 0.096933 & 0.120559 & 0.140288 & 0.157503 \\ 
& alternative 5 & 0.064395 & 0.096659 & 0.120372 & 0.14074 & 0.157251 \\ 
\hline
Group 7 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064363 & 0.096571 & 0.120695 & 0.140615 & 0.157738 \\ 
& alternative 2 & 0.064669 & 0.096909 & 0.12127 & 0.140601 & 0.157682 \\ 
& alternative 3 & 0.064184 & 0.096764 & 0.12081 & 0.140282 & 0.157844 \\ 
& alternative 4 & 0.064347 & 0.096876 & 0.120944 & 0.140352 & 0.158064 \\ 
& alternative 5 & 0.064199 & 0.09671 & 0.120781 & 0.140915 & 0.157701 \\ 
\hline
Group 8 & benchmark & 0.064387 & 0.096676 & 0.120863 & 0.140802 & 0.158234
\\ 
& alternative 1 & 0.064371 & 0.096558 & 0.120535 & 0.140318 & 0.15863 \\ 
& alternative 2 & 0.064602 & 0.096323 & 0.120285 & 0.140601 & 0.156228 \\ 
& alternative 3 & 0.064511 & 0.096548 & 0.120742 & 0.140341 & 0.157491 \\ 
& alternative 4 & 0.06455 & 0.096397 & 0.120589 & 0.140437 & 0.156927 \\ 
& alternative 5 & 0.064597 & 0.096458 & 0.120587 & 0.140237 & 0.157023 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.05in}
{\noindent $^{*}$ Notes: Entries are root mean square forecast errors (RMSFEs) of benchmark and alternative forecasting models for $h=${0,1,2,3,4}. 
See Section 5 for complete details.}
\end{minipage}
\end{table}

\bigskip

\begin{table}[tbp]
\caption{Supplemental Empirical Results (RMSFEs) -- SPF Forecast Pooling
Analysis of Quarterly Real GDP Using Median Benchmark Model and Median
Expert Pool Predictions$^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064387 & 0.096706 & 0.120832 & 0.14062 & 0.157779 \\ 
& alternative 2 & 0.064384 & 0.09664 & 0.120718 & 0.140338 & 0.157551 \\ 
& alternative 3 & 0.064365 & 0.096643 & 0.12069 & 0.140163 & 0.158671 \\ 
\hline
Group 2 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064301 & 0.096925 & 0.120608 & 0.140357 & 0.156271 \\ 
& alternative 2 & 0.064669 & 0.096909 & 0.12127 & 0.140601 & 0.157682 \\ 
& alternative 3 & 0.064602 & 0.096323 & 0.120285 & 0.140601 & 0.156228 \\ 
\hline
Group 3 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064304 & 0.097125 & 0.119762 & 0.140274 & 0.156968 \\ 
& alternative 2 & 0.06403 & 0.096705 & 0.120802 & 0.140387 & 0.157693 \\ 
& alternative 3 & 0.064528 & 0.096639 & 0.120692 & 0.140597 & 0.157651 \\ 
\hline
Group 4 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064307 & 0.096955 & 0.12033 & 0.14019 & 0.156868 \\ 
& alternative 2 & 0.064356 & 0.09688 & 0.120832 & 0.140262 & 0.157751 \\ 
& alternative 3 & 0.064542 & 0.096429 & 0.120483 & 0.14044 & 0.156674 \\ 
\hline
Group 5 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064254 & 0.096676 & 0.120342 & 0.14018 & 0.156869 \\ 
& alternative 2 & 0.06408 & 0.096744 & 0.120701 & 0.140605 & 0.157766 \\ 
& alternative 3 & 0.064595 & 0.09655 & 0.120673 & 0.140136 & 0.157148 \\ 
\hline
Group 6 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064387 & 0.096706 & 0.120832 & 0.14062 & 0.157779 \\ 
& alternative 2 & 0.064301 & 0.096925 & 0.120608 & 0.140357 & 0.156271 \\ 
& alternative 3 & 0.064304 & 0.097125 & 0.119762 & 0.140274 & 0.156968 \\ 
& alternative 4 & 0.064307 & 0.096955 & 0.12033 & 0.14019 & 0.156868 \\ 
& alternative 5 & 0.064254 & 0.096676 & 0.120342 & 0.14018 & 0.156869 \\ 
\hline
Group 7 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064384 & 0.09664 & 0.120718 & 0.140338 & 0.157551 \\ 
& alternative 2 & 0.064669 & 0.096909 & 0.12127 & 0.140601 & 0.157682 \\ 
& alternative 3 & 0.06403 & 0.096705 & 0.120802 & 0.140387 & 0.157693 \\ 
& alternative 4 & 0.064356 & 0.09688 & 0.120832 & 0.140262 & 0.157751 \\ 
& alternative 5 & 0.06408 & 0.096744 & 0.120701 & 0.140605 & 0.157766 \\ 
\hline
Group 8 & benchmark & 0.064398 & 0.096714 & 0.120815 & 0.1406 & 0.157726 \\ 
& alternative 1 & 0.064365 & 0.096643 & 0.12069 & 0.140163 & 0.158671 \\ 
& alternative 2 & 0.064602 & 0.096323 & 0.120285 & 0.140601 & 0.156228 \\ 
& alternative 3 & 0.064528 & 0.096639 & 0.120692 & 0.140597 & 0.157651 \\ 
& alternative 4 & 0.064542 & 0.096429 & 0.120483 & 0.14044 & 0.156674 \\ 
& alternative 5 & 0.064595 & 0.09655 & 0.120673 & 0.140136 & 0.157148 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}

{\noindent $^{*}$ Notes: See notes to Table 7.}
\end{minipage}
\end{table}

\end{document}

%%%%% End /document/Supplemental-Appendix-New-Robust-Nov-12-2022.tex %%%%%
