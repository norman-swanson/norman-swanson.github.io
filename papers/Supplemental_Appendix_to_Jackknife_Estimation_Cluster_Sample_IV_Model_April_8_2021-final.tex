%2multibyte Version: 5.50.0.2960 CodePage: 936


\documentclass[12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage[onehalfspacing]{setspace}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2606}
%TCIDATA{Codepage=936}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Sunday, July 18, 2004 16:10:34}
%TCIDATA{LastRevised=Friday, April 09, 2021 21:53:50}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{CSTFile=article.cst}
%TCIDATA{PageSetup=72,72,72,72,0}
%TCIDATA{AllPages=
%F=36,\PARA{038<p type="texpara" tag="Body Text" >\hfill \thepage}
%}


\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\renewcommand{\baselinestretch}{1.3} 
\textwidth=6.8in
\textheight=8.7in
\oddsidemargin=0in
\evensidemargin=0in
\topmargin=0in
\baselineskip=10pt
\linespread{1.0}
\input{tcilatex}
\geometry{left=1in,right=1in,top=1.25in,bottom=1.25in}

\begin{document}

\title{\textbf{Supplemental Appendix to Jackknife Estimation of a
Cluster-Sample IV Regression Model with Many Weak Instruments (not intended
for publication)\thanks{%
Corresponding author: John C. Chao, Department of Economics, 7343 Preinkert
Drive, University of Maryland, chao@econ.umd.edu. Norman R. Swanson,
Department of Economics, 9500 Hamilton Street, Rutgers University,
nswanson@econ.rutgers.edu. Tiemen Woutersen, Department of Economics, 1130 E
Helen Street, University of Arizona, woutersen@arizona.edu. The authors owe
special thanks to Jerry Hausman and Whitney Newey for many discussions on
the topic of this paper over a number of years. In addition, thanks are owed
to all of the particpants of the 2019 MIT Conference Honoring Whitney Newey
for comments on and advice given about the topic of this paper. Finally, the
authors wish to thank Miriam Arden for excellent research assistance. Chao
thanks the University of Maryland for research support, and Woutersen's work
was supported by an Eller College of Management Research Grant.}}}
\author{John C. Chao, Norman R. Swanson, and Tiemen Woutersen}
\date{April 8, 2021}
\maketitle

\begin{abstract}
This Supplemental Appendix is comprised of two sub-appendices. Appendix S1
gives the proofs for Theorems 2 and 3 stated in section 3 of the main paper.
Appendix S2, on the other hand, states and proves additional supporting
lemmas whose results are used to prove the main theorems of the paper.
\end{abstract}

\noindent \noindent \setcounter{page}{1}

\section*{Appendix S1: Proofs of Theorems 2 and 3}

\noindent \textbf{Proof of Theorem 2: }Define $\mathcal{Y}_{n}=\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon $.{\small \ }Note that, by the result of
Lemma S2-9 given in Appendix S2 below, we have that $D_{\mu }^{-1}\widehat{%
\Delta }\left( \delta _{0}\right) =\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}+D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon
+o_{p}\left( 1\right) =\mathcal{Y}_{n}+o_{p}\left( 1\right) $.

\noindent We now establish the asymptotic normality of $\mathcal{Y}_{n}$,
upon appropriate standardization, in the case where $K_{2,n}/\left( \mu
_{n}^{\min }\right) ^{2}=O\left( 1\right) $. To proceed, let $a\in \mathbb{R}%
^{d}$ such that $\left\Vert a\right\Vert _{2}=1$ and define $b_{1n}=\Sigma
_{n}^{-1/2}a$. and $b_{2n}=\sqrt{K_{2,n}}D_{\mu }^{-1}\Sigma _{n}^{-1/2}a$.
Now, let $\mathcal{L}_{\left( i,t\right) ,n}=b_{1n}^{\prime }\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\varepsilon _{\left(
i,t\right) }/\sqrt{n}$ and $\mathcal{N}_{\left( i,t\right)
,n}=K_{2,n}^{-1/2}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }\left[ \underline{u}_{2,\left(
i,t\right) ,n}\varepsilon _{\left( j,s\right) }+\underline{u}_{2,\left(
j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right] $, where $\underline{u%
}_{2,\left( i,t\right) ,n}=$ $b_{2n}^{\prime }\underline{U}_{\left(
i,t\right) }$, with $\underline{u}_{2,\left( j,s\right) ,n}$ similarly
defined, and where $e_{\left( i,t\right) }$ denotes an $m_{n}\times 1$
elementary vector whose $\left( i,t\right) ^{th}$ component is $1$ and all
other components are $0$. In addition, write, as in the proof of part (d) of
Lemma S2-3, $\Sigma _{n}=VC\left( \mathcal{Y}_{n}|\mathcal{F}_{n}^{W}\right)
=\Sigma _{1,n}+\Sigma _{2,n}$, where $\Sigma _{1,n}=VC\left( \Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) $
and $\Sigma _{2,n}=VC\left( D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon
|\mathcal{F}_{n}^{W}\right) $ as previously defined. Using these notations,
note that we can write $a^{\prime }\Sigma _{n}^{-1/2}\mathcal{Y}_{n}=%
\mathcal{L}_{\left( 1,1\right) ,n}+\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\left\{ \mathcal{L}_{\left( i,t\right) ,n}+\mathcal{N}_{\left(
i,t\right) ,n}\right\} $. Next, observe that%
\begin{eqnarray*}
E\left[ \mathcal{L}_{\left( 1,1\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
&=&E\left[ \varepsilon _{\left( 1,1\right) }^{2}|\mathcal{F}_{n}^{W}\right] 
\frac{\left[ a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( 1,1\right) }\right] ^{2}}{n} \\
&\leq &E\left[ \varepsilon _{\left( 1,1\right) }^{2}|\mathcal{F}_{n}^{W}%
\right] a^{\prime }\Sigma _{n}^{-1}a\left( \frac{\left\Vert \Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }e_{\left( 1,1\right) }\right\Vert _{2}}{\sqrt{n}}%
\right) ^{2}\text{ (by CS inequality)} \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) a^{\prime
}\Sigma _{n}^{-1}a\left( \frac{\max_{1\leq \left( i,t\right) \leq
m_{n}}\left\Vert \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }\right\Vert _{2}}{\sqrt{n}}\right) ^{2} \\
&=&o_{p}\left( 1\right) \text{ (by Assumptions 2(i) and 7(iv) and part (d)
of Lemma S2-3)}
\end{eqnarray*}%
Moreover, under Assumptions 2 and 3(iii), there exists a positive constant $%
C^{\ast }$ such that 
\begin{eqnarray*}
E_{W_{n}}\left\{ \left( E\left[ \mathcal{L}_{\left( 1,1\right) ,n}^{2}|%
\mathcal{F}_{n}^{W}\right] \right) ^{2}\right\} &=&\frac{E_{W_{n}}\left\{ %
\left[ a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( 1,1\right) }\right] ^{4}\left( E\left[ \varepsilon
_{\left( 1,1\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) ^{2}\right\} }{%
n^{2}} \\
&\leq &\frac{C}{n^{2}}E\left( \left[ a^{\prime }\Sigma _{n}^{-1/2}\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( 1,1\right) }\right]
^{4}\right) \text{ (by Assumption 2(i))} \\
&\leq &CE\left( \frac{a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma \Sigma _{n}^{-1/2}a}{n}\right) ^{2}\text{
(by CS inequality)} \\
&\leq &C\overline{C}=C^{\ast }<\infty \text{ (by Assumption 3(iii) and Lemma
S2-3(d))}
\end{eqnarray*}%
Since the upper bound above does not depend on $n$, we further deduce that

\noindent $\sup_{n}E_{W_{n}}\left\{ \left( E\left[ \mathcal{L}_{\left(
1,1\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \right) ^{2}\right\} <\infty $%
. It follows by the law of iterated expectations and by Theorem 25.12 of
Billingsley (1995) that $E\left( \mathcal{L}_{\left( 1,1\right)
,n}^{2}\right) =E_{W_{n}}\left( E\left[ \mathcal{L}_{\left( 1,1\right)
,n}^{2}|\mathcal{F}_{n}^{W}\right] \right) \rightarrow 0$. Application of
Markov's inequality then allows us to deduce that $\mathcal{L}_{\left(
1,1\right) ,n}=b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( 1,1\right) }\varepsilon _{\left( 1,1\right) }/\sqrt{n}$

\noindent $=o_{p}\left( 1\right) $, from which we obtain the representation%
\begin{equation*}
a^{\prime }\Sigma _{n}^{-1/2}\mathcal{Y}_{n}=\mathcal{V}_{n}+o_{p}\left(
1\right) \text{, }
\end{equation*}%
where $\mathcal{V}_{n}=\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}\mathcal{%
V}_{\left( i,t\right) ,n}$ with $\mathcal{V}_{\left( i,t\right) ,n}=\mathcal{%
L}_{\left( i,t\right) ,n}+\mathcal{N}_{\left( i,t\right) ,n}$. Note we can
also write $\mathcal{V}_{n}=\mathcal{L}_{n}+\mathcal{N}_{n}$, where $%
\mathcal{L}_{n}=\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}\mathcal{L}%
_{\left( i,t\right) ,n}$ and $\mathcal{N}_{n}=\dsum\nolimits_{\left(
i,t\right) =2}^{m_{n}}\mathcal{N}_{\left( i,t\right) ,n}$.

\noindent \qquad Next, define the $\sigma $-fields $\mathcal{F}_{\left(
i,t\right) ,n}=\sigma \left( \left\{ \varepsilon _{\left( k,\upsilon \right)
},U_{\left( k,\upsilon \right) }\right\} _{\left( k,\upsilon \right)
=1}^{\left( i,t\right) },W_{n}\right) $ \ for $\left( i,t\right)
=1,2,...,m_{n}$, note that by construction $\mathcal{F}_{\left( i,t\right)
-1,n}\subseteq \mathcal{F}_{\left( i,t\right) ,n}$ for $\left( i,t\right)
=2,...,m_{n}$ and $\mathcal{V}_{\left( i,t\right) ,n}$ is $\mathcal{F}%
_{\left( i,t\right) ,n}$-measurable. Note also that, under Assumption 1, it
is easily seen that $E\left[ \mathcal{V}_{\left( i,t\right) ,n}|\mathcal{F}%
_{\left( i,t\right) -1,n}\right] =0$. In addition, note that, by part (d)\
of Lemma S2-3 and Lemma S2-6, and Assumption 2(i); 
\begin{eqnarray}
E\left[ \underline{u}_{2,\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
&\leq &\left( b_{2n}^{\prime }b_{2n}\right) \max_{1\leq \left( i,t\right)
\leq m_{n}}E\left[ \left\Vert \underline{U}_{\left( i,t\right) }\right\Vert
_{2}^{2}|\mathcal{F}_{n}^{W}\right]  \notag \\
&\leq &\frac{K_{2,n}}{\left( \mu _{n}^{\min }\right) ^{2}}a^{\prime }\Sigma
_{n}^{-1}a\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert 
\underline{U}_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}%
\right] =O_{a.s.}\left( 1\right)  \label{var u2 bound}
\end{eqnarray}%
since, for this theorem, we assume that $K_{2,n}/\left( \mu _{n}^{\min
}\right) ^{2}=O\left( 1\right) $. It follows then from straightforward
calculations, from applying the triangle and CS inequalities, as well as
from expression (\ref{var u2 bound}), part (d) of Lemma S2-1, part (d) of
Lemma S2-3, and Assumptions 2(i) and 3(iii) that there exists a positive
constant $\overline{C}$ such that%
\begin{eqnarray*}
&&Var\left( \mathcal{V}_{\left( i,t\right) ,n}|\mathcal{F}_{n}^{W}\right) \\
&=&E\left[ \mathcal{L}_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
+E\left[ \mathcal{N}_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) a^{\prime
}\Sigma _{n}^{-1}a\lambda _{\max }\left( \frac{\Gamma ^{\prime }\Gamma }{n}%
\right) \\
&&+\frac{4}{K_{2,n}}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right)
\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \underline{u}%
_{2,\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \right) \left(
\max_{1\leq \left( i,t\right) \leq m_{n}}\dsum\limits_{\left( j,s\right)
=1}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\right) \\
&=&O_{a.s.}\left( 1\right) +O_{a.s.}\left( \frac{K_{2,n}}{\left( \mu
_{n}^{\min }\right) ^{2}n}\right) =O_{a.s.}\left( 1\right)
\end{eqnarray*}%
By the law of iterated expectations and Theorem 16.1 of Billingsley (1995),
there exists a constant $\overline{C}$ such that $Var\left( \mathcal{V}%
_{\left( i,t\right) ,n}\right) =E\left( \mathcal{V}_{\left( i,t\right)
,n}^{2}\right) =E_{W_{n}}\left[ E\left( \mathcal{V}_{\left( i,t\right)
,n}^{2}|\mathcal{F}_{n}^{W}\right) \right] \leq \overline{C}<\infty $ for
all $n$ sufficiently large. These results show that $\left\{ \mathcal{V}%
_{\left( i,t\right) ,n},\mathcal{F}_{\left( i,t\right) ,n},1\leq \left(
i,t\right) \leq m_{n}\text{, }n\geq 1\right\} $ forms a square-integrable
martingale difference array.

To show the asymptotic normality of $\mathcal{V}_{n}$, we verify the
conditions of the central limit theorem for martingale difference arrays
given in Lemma S2-15. To proceed, first consider condition (\ref{MDA CLT
cond 1b}), which, as noted in the remark which follows Lemma S2-15, is a
sufficient condition for condition (\ref{MDA CLT cond 1}) of Lemma S2-15. We
shall verify (\ref{MDA CLT cond 1b}) for the case where $\delta =2$. Note
first that, by applying Lo\`{e}ve's $c_{r}$ inequality, we get

\begin{equation*}
\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{V}_{\left(
i,t\right) ,n}^{4}\right] =\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[
\left( \mathcal{L}_{\left( i,t\right) ,n}+\mathcal{N}_{\left( i,t\right)
,n}\right) ^{4}\right] \leq 8\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E%
\left[ \mathcal{L}_{\left( i,t\right) ,n}^{4}\right] +8\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}E\left[ \mathcal{N}_{\left( i,t\right) ,n}^{4}\right]
\end{equation*}%
Hence, to verify condition (\ref{MDA CLT cond 1b}), it suffices to show that 
$\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{L}_{\left(
i,t\right) ,n}^{4}\right] =o\left( 1\right) $ and $\dsum\nolimits_{\left(
i,t\right) =2}^{m_{n}}E\left[ \mathcal{N}_{\left( i,t\right) ,n}^{4}\right]
=o\left( 1\right) $. To do this, we first focus on a conditional expectation
analogue of $\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{L}%
_{\left( i,t\right) ,n}^{4}\right] $. Note that 
\begin{eqnarray*}
&&\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{L}_{\left(
i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] \\
&=&\frac{1}{n^{2}}\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\left[
a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( i,t\right) }\right] ^{4}E\left[ \varepsilon _{\left( i,t\right)
}^{4}|\mathcal{F}_{n}^{W}\right] \\
&\leq &a^{\prime }\Sigma _{n}^{-1}a\frac{1}{n}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\left[ a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right] ^{2}\left( \frac{%
\left\Vert \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right)
}\right\Vert _{2}}{\sqrt{n}}\right) ^{2}E\left[ \varepsilon _{\left(
i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \text{ } \\
&&\text{(by CS inequality)} \\
&\leq &a^{\prime }\Sigma _{n}^{-1}a\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \right) \left( \frac{\max_{1\leq \left( i,t\right) \leq
m_{n}}\left\Vert \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }\right\Vert _{2}}{\sqrt{n}}\right) ^{2} \\
&&\times \frac{1}{n}a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\dsum\limits_{\left( i,t\right) =1}^{m_{n}}e_{\left(
i,t\right) }e_{\left( i,t\right) }^{\prime }M^{\left( Z_{1},Q\right) }\Gamma
\Sigma _{n}^{-1/2}a \\
&\leq &a^{\prime }\Sigma _{n}^{-1}a\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \right) \left( \frac{\max_{1\leq \left( i,t\right) \leq
m_{n}}\left\Vert \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }\right\Vert _{2}}{\sqrt{n}}\right) ^{2} \\
&&\times \frac{a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma \Sigma _{n}^{-1/2}a}{n} \\
&\leq &\left( a^{\prime }\Sigma _{n}^{-1}a\right) ^{2}\left( \max_{1\leq
\left( i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|%
\mathcal{F}_{n}^{W}\right] \right) \left( \frac{\max_{1\leq \left(
i,t\right) \leq m_{n}}\left\Vert \Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( i,t\right) }\right\Vert _{2}}{\sqrt{n}}\right) ^{2}\lambda _{\max
}\left( \frac{\Gamma ^{\prime }\Gamma }{n}\right) \\
&\leq &C\left( \frac{\max_{1\leq \left( i,t\right) \leq m_{n}}\left\Vert
\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right\Vert
_{2}}{\sqrt{n}}\right) ^{2}=o_{p}\left( 1\right) \text{ }
\end{eqnarray*}%
where the last line above follows from Assumptions 2(i), 3(iii), and 7(iv)
and by Lemma S2-3(d). Next, note that, under Assumptions 2 and 3(iii), there
exists a positive constant $C^{\ast }$ such that 
\begin{eqnarray*}
&&E_{W_{n}}\left( \dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ 
\mathcal{L}_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] \right)
^{2} \\
&=&\frac{1}{n^{4}}\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =2}^{m_{n}}E\left( _{W_{n}}\left[
a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( i,t\right) }\right] ^{4}\left[ a^{\prime }\Sigma
_{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( j,s\right) }%
\right] ^{4}\right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times E%
\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] E%
\left[ \varepsilon _{\left( j,s\right) }^{4}|\mathcal{F}_{n}^{W}\right]
\right) \\
&\leq &\frac{C}{n^{4}}\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =2}^{m_{n}}E_{W_{n}}\left( \left[
a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( i,t\right) }\right] ^{4}\left[ a^{\prime }\Sigma
_{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( j,s\right) }%
\right] ^{4}\right) \text{ } \\
&\leq &\frac{C}{n^{4}}E_{W_{n}}\left\{ a^{\prime }\Sigma _{n}^{-1/2}\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}e_{\left( i,t\right) }e_{\left( i,t\right) }^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma \Sigma _{n}^{-1/2}aa^{\prime }\Sigma
_{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ }\left. \times \text{\ }\dsum\limits_{\left(
j,s\right) =1}^{m_{n}}e_{\left( j,s\right) }e_{\left( j,s\right) }^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma \Sigma _{n}^{-1/2}a\left( a^{\prime
}\Sigma _{n}^{-1/2}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma \Sigma
_{n}^{-1/2}a\right) ^{2}\right\} \text{ \ } \\
&=&CE_{W_{n}}\left( \frac{a^{\prime }\Sigma _{n}^{-1/2}\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma \Sigma _{n}^{-1/2}a}{n}\right) ^{4} \\
&\leq &C\overline{C}=C^{\ast }<\infty \text{ \ (by Assumption 3(iii) and
Lemma S2-3(d))}
\end{eqnarray*}%
where the second inequality above follows from applying the CS inequality.
Since the upper bound above does not depend on $n$, we further deduce that

\noindent $\sup_{n}E_{W_{n}}\left( \dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}E\left[ \mathcal{L}_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}%
\right] \right) ^{2}<\infty $. It follows by the law of iterated
expectations and by Theorem 25.12 of Billingsley (1995) that 
\begin{equation*}
\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{L}_{\left(
i,t\right) ,n}^{4}\right] =\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}E_{W_{n}}\left( E\left[ \mathcal{L}_{\left( i,t\right) ,n}^{4}|%
\mathcal{F}_{n}^{W}\right] \right) \rightarrow 0\text{. }
\end{equation*}

Turning our attention to the bilinear term, note that by Lo\`{e}ve's $c_{r}$
inequality we have $\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}E\left[ 
\mathcal{N}_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] \leq 
\mathcal{R}_{1}+\mathcal{R}_{2}$, where

\noindent $\mathcal{R}_{1}=\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\left( 8/K_{2,n}^{2}\right) E\left[ \left( \dsum\nolimits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}\underline{u}_{2,\left( i,t\right) }\varepsilon _{\left( j,s\right)
}\right) ^{4}|\mathcal{F}_{n}^{W}\right] $ and

\noindent $\mathcal{R}_{2}=\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\left( 8/K_{2,n}^{2}\right) E\left[ \left( \dsum\nolimits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}\underline{u}_{2,\left( j,s\right) }\varepsilon _{\left( i,t\right)
}\right) ^{4}|\mathcal{F}_{n}^{W}\right] $. Focusing first on the term $%
\mathcal{R}_{1}$, note that, by straightforward calculations as well as by
making use of Assumptions 2(i) and 5(ii), parts (b) and (c) of Lemma S2-1,
part (d) of Lemma S2-3, and Lemma S2-6; we deduce that, there exists a
positive constant $\overline{C}$ such that 
\begin{eqnarray*}
\frac{\left( \mu _{n}^{\min }\right) ^{4}n}{K_{2,n}^{2}}\mathcal{R}_{1}
&\leq &24n\left( a^{\prime }\Sigma _{n}^{-1}a\right) ^{2}\left( \max_{1\leq
\left( i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}_{\left(
i,t\right) }\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}\right] \right) \left(
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left(
i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \right)  \\
&&\times \left[ \frac{1}{K_{2,n}^{2}}\dsum\limits_{\substack{ \left(
i,t\right) ,\left( j,s\right) =1 \\ \left( i,t\right) \neq \left( j,s\right) 
}}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{4}+\frac{1}{K_{2,n}^{2}}%
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits_{\substack{ \left(
j,s\right) ,\left( k,\upsilon \right) =1 \\ \left( j,s\right) \neq \left(
i,t\right) ,\left( k,\upsilon \right) \neq \left( i,t\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}A_{\left( i,t\right)
,\left( k,\upsilon \right) }^{2}\right]  \\
&\leq &\overline{C}n\left[ \frac{1}{K_{2,n}^{2}}\dsum\limits_{\substack{ %
\left( i,t\right) ,\left( j,s\right) =1 \\ \left( i,t\right) \neq \left(
j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{4}+\frac{1}{%
K_{2,n}^{2}}\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits
_{\substack{ \left( j,s\right) ,\left( k,\upsilon \right) =1 \\ \left(
j,s\right) \neq \left( i,t\right) ,\left( k,\upsilon \right) \neq \left(
i,t\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}A_{\left(
i,t\right) ,\left( k,\upsilon \right) }^{2}\right]  \\
&=&O_{a.s.}\left( \frac{K_{2,n}}{n}\right) +O_{a.s.}\left( 1\right)
=O_{a.s.}\left( 1\right) \text{.}
\end{eqnarray*}%
Applying the law of iterated expectations and Theorem 16.1 of Billingsley
(1995), we then have%
\begin{eqnarray*}
&&\frac{\left( \mu _{n}^{\min }\right) ^{4}n}{K_{2,n}^{2}}E_{W_{n}}\left( 
\mathcal{R}_{1}\right)  \\
&\leq &\overline{C}nE_{W_{n}}\left[ \frac{1}{K_{2,n}^{2}}\dsum\limits
_{\substack{ \left( i,t\right) ,\left( j,s\right) =1 \\ \left( i,t\right)
\neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{4}+\frac{1}{K_{2,n}^{2}}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{\substack{ \left( j,s\right) ,\left( k,\upsilon
\right) =1 \\ \left( j,s\right) \neq \left( i,t\right) ,\left( k,\upsilon
\right) \neq \left( i,t\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}A_{\left( i,t\right) ,\left( k,\upsilon \right) }^{2}\right] 
\\
&=&O\left( 1\right) 
\end{eqnarray*}%
from which we further deduce that%
\begin{eqnarray*}
E_{W_{n}}\left( \mathcal{R}_{1}\right)  &=&\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\frac{8}{K_{2,n}^{2}}E\left[ \left( \dsum\nolimits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}\underline{u}_{2,\left( i,t\right) }\varepsilon _{\left( j,s\right)
}\right) ^{4}\right]  \\
&=&O\left( \frac{K_{2,n}^{2}}{\left( \mu _{n}^{\min }\right) ^{4}n}\right)
=o\left( 1\right) 
\end{eqnarray*}%
In a similar way, we can also show that

\noindent $E_{W_{n}}\left( \mathcal{R}_{2}\right) =\left(
8/K_{2,n}^{2}\right) E\left[ \dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\left( \dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( j,s\right) ,\left( i,t\right) }\underline{u}_{2,\left(
j,s\right) }\varepsilon _{\left( i,t\right) }\right) ^{4}\right] =o\left(
1\right) $. It follows that

\noindent $\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{N}%
_{\left( i,t\right) ,n}^{4}\right] \leq E_{W_{n}}\left( \mathcal{R}%
_{1}\right) +E_{W_{n}}\left( \mathcal{R}_{2}\right) =o\left( 1\right) $.
This verifies condition (\ref{MDA CLT cond 1b}).

Next, we verify condition (\ref{MDA CLT cond 2}) of Lemma S2-15. To proceed,
first let $s_{W}^{2}=Var\left[ \mathcal{V}_{n}|\mathcal{F}_{n}^{W}\right]
=Var\left( \dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}\mathcal{V}_{\left(
i,t\right) ,n}|\mathcal{F}_{n}^{W}\right) $, and note that%
\begin{equation}
s_{W}^{2}=Var\left( \frac{b_{1n}^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon }{\sqrt{n}}+\frac{b_{2n}^{\prime }\underline{U}%
^{\prime }A\varepsilon }{\sqrt{K_{2,n}}}|\mathcal{F}_{n}^{W}\right)
+o_{p}\left( 1\right) =a^{\prime }\Sigma _{n}^{-1/2}\Sigma _{n}\Sigma
_{n}^{-1/2}a+o_{p}\left( 1\right) =1+o_{p}\left( 1\right)
\label{limit of s2}
\end{equation}

\noindent On the other hand, by straightforward calculation, we can write%
\begin{eqnarray}
s_{W}^{2} &=&\frac{1}{n}\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\left[
b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }\right] ^{2}E\left[ \varepsilon _{\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right]  \notag \\
&&+\frac{1}{K_{2,n}}\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{ E\left[ \underline{u}%
_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] +E\left[ \underline{u}%
_{2,\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right\}  \notag \\
&&+\frac{2}{K_{2,n}}\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \underline{u}%
_{2,\left( i,t\right) }\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}%
\right] E\left[ \underline{u}_{2,\left( j,s\right) }\varepsilon _{\left(
j,s\right) }|\mathcal{F}_{n}^{W}\right]  \label{s2}
\end{eqnarray}%
Making use of expression (\ref{s2}), we obtain, after some further
calculations,%
\begin{eqnarray}
&&\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{V}_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{\left( i,t\right) -1,n}\right] -s_{W_{n}}^{2}
\notag \\
&=&\frac{2}{\sqrt{n}}\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right) -1}\left[
b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }\right] \frac{A_{\left( i,t\right) ,\left( j,s\right) }}{\sqrt{%
K_{2,n}}}\left\{ \varepsilon _{\left( j,s\right) }E\left[ \varepsilon
_{\left( i,t\right) }\underline{u}_{2,\left( i,t\right) }|\mathcal{F}_{n}^{W}%
\right] +\underline{u}_{2,\left( j,s\right) }E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right\}  \notag \\
&&+\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\dsum\limits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}\frac{A_{\left( i,t\right) ,\left(
j,s\right) }^{2}}{K_{2,n}}\left( \varepsilon _{\left( j,s\right) }^{2}-E%
\left[ \varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\right) E\left[ \underline{u}_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}%
\right]  \notag \\
&&+\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\dsum\limits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}\frac{A_{\left( i,t\right) ,\left(
j,s\right) }^{2}}{K_{2,n}}\left( \underline{u}_{2,\left( j,s\right) }^{2}-E%
\left[ \underline{u}_{2,\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\right) E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}%
\right]  \notag \\
&&+2\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\dsum\limits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}\frac{A_{\left( i,t\right) ,\left(
j,s\right) }^{2}}{K_{2,n}}\left( \varepsilon _{\left( j,s\right) }\underline{%
u}_{2,\left( j,s\right) }-E\left[ \underline{u}_{2,\left( j,s\right)
}\varepsilon _{\left( j,s\right) }|\mathcal{F}_{n}^{W}\right] \right) E\left[
\underline{u}_{2,\left( i,t\right) }\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right]  \notag \\
&&+2\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left(
j,s\right) =2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}\frac{A_{\left( i,t\right) ,\left( j,s\right)
}A_{\left( i,t\right) ,\left( k,\upsilon \right) }}{K_{2,n}}E\left[ 
\underline{u}_{2,\left( i,t\right) }\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] \left\{ \underline{u}_{2,\left( j,s\right)
}\varepsilon _{\left( k,\upsilon \right) }+\varepsilon _{\left( j,s\right) }%
\underline{u}_{2,\left( k,\upsilon \right) }\right\}  \notag \\
&&+2\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left(
j,s\right) =2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}\frac{A_{\left( i,t\right) ,\left( j,s\right)
}A_{\left( i,t\right) ,\left( k,\upsilon \right) }}{K_{2,n}}\varepsilon
_{\left( j,s\right) }\varepsilon _{\left( k,\upsilon \right) }E\left[ 
\underline{u}_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]  \notag
\end{eqnarray}
\begin{eqnarray*}
&&+2\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left(
j,s\right) =2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}\frac{A_{\left( i,t\right) ,\left( j,s\right)
}A_{\left( i,t\right) ,\left( k,\upsilon \right) }}{K_{2,n}}\underline{u}%
_{2,\left( j,s\right) }\underline{u}_{2,\left( k,\upsilon \right) }E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \\
&=&\mathcal{T}_{1}+\mathcal{T}_{2}+\mathcal{T}_{3}+\mathcal{T}_{4}+\mathcal{T%
}_{5}+\mathcal{T}_{6}+\mathcal{T}_{7},\text{ }\left( say\right)
\end{eqnarray*}

Note first that, by applying parts (a)-(c) of Lemma S2-14, we have $\mathcal{%
T}_{1}\overset{p}{\rightarrow }0$, $\mathcal{T}_{2}\overset{p}{\rightarrow }%
0 $, and $\mathcal{T}_{3}\overset{p}{\rightarrow }0$. Consider next the term%
\begin{equation*}
\mathcal{T}_{4}=2\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right) -1}\frac{%
A_{\left( i,t\right) ,\left( j,s\right) }^{2}}{K_{2,n}}\left( \underline{u}%
_{2,\left( j,s\right) }\varepsilon _{\left( j,s\right) }-E\left[ \underline{u%
}_{2,\left( j,s\right) }\varepsilon _{\left( j,s\right) }|\mathcal{F}_{n}^{W}%
\right] \right) E\left[ \underline{u}_{2,\left( i,t\right) }\varepsilon
_{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] .
\end{equation*}%
In this case, we apply part (a) of Lemma S2-8 with $u_{\left( j,s\right) }=%
\underline{u}_{2,\left( j,s\right) }$, $\overline{\psi }_{\left( j,s\right)
}=E\left[ \underline{u}_{2,\left( j,s\right) }\varepsilon _{\left(
j,s\right) }|\mathcal{F}_{n}^{W}\right] $, and $\phi _{\left( i,t\right) }=E%
\left[ \underline{u}_{2,\left( i,t\right) }\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] $. Note that, in this case, $\left\{ \left( 
\underline{u}_{2,\left( i,t\right) },\varepsilon _{\left( i,t\right)
}\right) \right\} _{\left( i,t\right) =1}^{m_{n}}$ is independent
conditional on $\mathcal{F}_{n}^{W}$, and $\sup_{1\leq \left( i,t\right)
\leq m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \leq C$ $a.s.$ by Assumptions 1 and 2(i), respectively. Moreover,
note that Assumption 2, part (d) of Lemma S2-3, Lemma S2-6, and the fact
that $K_{2,n}/\left( \mu _{n}^{\min }\right) ^{2}=O\left( 1\right) $ in this
case together imply that there exists a constant $C\geq 1$ such that $E\left[
\underline{u}_{2,\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \leq %
\left[ K_{2,n}^{2}/\left( \mu _{n}^{\min }\right) ^{4}\right] E\left[
\left\Vert \underline{U}_{\left( i,t\right) }\right\Vert _{2}^{4}|\mathcal{F}%
_{n}^{W}\right] \left( a^{\prime }\Sigma _{n}^{-1}a\right) ^{2}\leq C<\infty 
$ \ $a.s.$ for all $\left( i,t\right) \in \left\{ 1,2,...,m_{n}\right\} $
and for all $n$ sufficiently large, so that

\noindent $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \underline{u}%
_{2,\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \leq C$ \ $a.s.n.$
Finally, using the upper bound derived in expression (\ref{bd on cov of e
and u2}) in the proof of part (a) of Lemma S2-14, we obtain $\max_{1\leq
\left( i,t\right) \leq m_{n}}\left\vert \phi _{\left( i,t\right)
}\right\vert \leq \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\left\vert \underline{u}_{2,\left( i,t\right) }\varepsilon _{\left(
i,t\right) }\right\vert |\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.n.$ and

\noindent $\max_{1\leq \left( j,s\right) \leq m_{n}}\left\vert \overline{%
\psi }_{\left( j,s\right) }\right\vert \leq \max_{1\leq \left( i,t\right)
\leq m_{n}}E\left[ \left\vert \underline{u}_{2,\left( j,s\right)
}\varepsilon _{\left( j,s\right) }\right\vert |\mathcal{F}_{n}^{W}\right]
\leq C$ $a.s.n.$ It follows by part (a) of Lemma S2-8 that $\mathcal{T}_{4}%
\overset{p}{\rightarrow }0$.

Now, consider $\mathcal{T}_{5}$. Here, we apply part (b) of Lemma S2-8 with $%
u_{\left( j,s\right) }=\underline{u}_{2,\left( j,s\right) }$ and $\phi
_{\left( i,t\right) }=E\left[ \underline{u}_{2,\left( i,t\right)
}\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] $. Note again
that $\left\{ \left( \underline{u}_{2,\left( i,t\right) },\varepsilon
_{\left( i,t\right) }\right) \right\} _{\left( i,t\right) =1}^{m_{n}}$ is
independent conditional on $\mathcal{F}_{n}^{W}$, and $\max_{1\leq \left(
i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{%
F}_{n}^{W}\right] \leq C$ $a.s.$ by Assumptions 1 and 2(i), respectively.
Moreover, previously, we have shown that $E\left[ \underline{u}_{2,\left(
i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \leq C$ \ $a.s.n.$ and $%
\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \phi _{\left( i,t\right)
}\right\vert \leq C$ $a.s.n.$ Hence, applying part (b) of Lemma S2-8, we
deduce that $\mathcal{T}_{5}\overset{p}{\rightarrow }0$.

Turning our attention to $\mathcal{T}_{6}$, we note that, for this term, we
can apply part (c) of Lemma S2-8 with $\phi _{\left( i,t\right) }=E\left[ 
\underline{u}_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $. From (%
\ref{var u2 bound}), there exists a positive constant $C$ such that $E\left[ 
\underline{u}_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \leq
C<\infty $ \ $a.s.$ for all $\left( i,t\right) \in \left\{
1,2,...,m_{n}\right\} $ and for all $n$ sufficiently large, so that

\noindent $\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \phi _{\left(
i,t\right) }\right\vert =\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ 
\underline{u}_{2,\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \leq C$ $%
a.s.n.$ Hence, applying part (c) of Lemma S2-8, we obtain $\mathcal{T}_{6}%
\overset{p}{\rightarrow }0.$

Finally, consider $\mathcal{T}_{7}$. In this case, we apply part (d) of
Lemma S2-8 with $u_{\left( j,s\right) }=\underline{u}_{2,\left( j,s\right) }$%
, $u_{\left( k,\upsilon \right) }=\underline{u}_{2,\left( k,\upsilon \right)
}$, and $\phi _{\left( i,t\right) }=E\left[ \varepsilon _{\left( i,t\right)
}^{2}|\mathcal{F}_{n}^{W}\right] $. Using a conditional version of
Liapounov's inequality and Assumption 2(i), we obtain $E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \leq \left( E\left[
\varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \right)
^{1/2}\leq C<\infty $ \ $a.s.$ for all $\left( i,t\right) \in \left\{
1,2,...,m_{n}\right\} $ and for all $n$, so that $\max_{1\leq \left(
i,t\right) \leq m_{n}}\left\vert \phi _{\left( i,t\right) }\right\vert $

\noindent $=\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.$
Moreover, as noted previously, Assumption 2, part (d) of Lemma S2-3, Lemma
S2-6, and the fact that $K_{2,n}/\left( \mu _{n}^{\min }\right) ^{2}=O\left(
1\right) $ together imply that $\max_{1\leq \left( i,t\right) \leq m_{n}}E%
\left[ \underline{u}_{2,\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right]
\leq C$ \ $a.s.n.$ It follows by applying part (d) of Lemma S2-8 that $%
\mathcal{T}_{7}\overset{p}{\rightarrow }0.$

The above argument shows that $\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}E%
\left[ \mathcal{V}_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{\left( i,t\right)
-1,n}\right] -s_{W_{n}}^{2}=\dsum\nolimits_{k=1}^{7}\mathcal{T}%
_{k}=o_{p}\left( 1\right) $. On the other hand, expression (\ref{limit of s2}%
) above implies that $s_{W_{n}}^{2}-1=o_{p}\left( 1\right) $. Putting these
two results together, we obtain $\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}E\left[ \mathcal{V}_{\left( i,t\right) ,n}^{2}|\mathcal{F}%
_{\left( i,t\right) -1,n}\right] -1=o_{p}\left( 1\right) $, which
establishes condition (\ref{MDA CLT cond 2}) of Lemma S2-15.

It now follows from Lemma S2-15 that

\noindent $\mathcal{V}_{n}=\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\left\{ b_{1n}^{\prime }\Gamma ^{\prime }M^{Q}e_{\left(
i,t\right) }\varepsilon _{\left( i,t\right) }/\sqrt{n}+\dsum\nolimits_{%
\left( j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left(
j,s\right) }\left[ \underline{u}_{2,\left( i,t\right) }\varepsilon _{\left(
j,s\right) }+\underline{u}_{2,\left( j,s\right) }\varepsilon _{\left(
i,t\right) }\right] \right\} \overset{d}{\rightarrow }N\left( 0,1\right) $.
Since, previously, we have shown that $a^{\prime }\Sigma _{n}^{-1/2}\mathcal{%
Y}_{n}=\mathcal{V}_{n}+o_{p}\left( 1\right) $, this further implies that $%
a^{\prime }\Sigma _{n}^{-1/2}\mathcal{Y}_{n}\overset{d}{\rightarrow }N\left(
0,1\right) $. Given that this result holds for all $a\in \mathbb{R}^{d}$
such that $\left\Vert a\right\Vert _{2}=1$, we can then apply the Cram\'{e}%
r-Wold device to obtain%
\begin{equation}
\Sigma _{n}^{-1/2}\mathcal{Y}_{n}=\Sigma _{n}^{-1/2}\left( \frac{\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{\sqrt{n}}+D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon \right) \overset{d}{\rightarrow }N\left(
0,I_{d}\right)  \label{asy normality V}
\end{equation}%
Next, let $H_{n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n$, $%
\Lambda _{I,n}=H_{n}^{-1}\Sigma _{n}H_{n}^{-1}$, and $\mathcal{Y}_{n}=\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon $, as given above. Consider first $%
\widehat{\delta }_{L,n}$. Theorem 1 has already shown that $\widehat{\delta }%
_{L,n}\overset{p}{\rightarrow }\delta _{0}$. To show asymptotic normality of 
$\widehat{\delta }_{L}$, note first that, by Lemma S2-11, $\widehat{\delta }%
_{L,n}$ satisfies the set of (normalized) first-order conditions $\widehat{%
\Delta }\left( \widehat{\delta }_{L,n}\right) =0$, where

\noindent $\widehat{\Delta }\left( \delta \right) =-\left[ \left( y-X\delta
\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) /2%
\right] \left[ \partial \widehat{Q}_{FELIM}\left( \delta \right) /\partial
\delta \right] $ with

\noindent $\widehat{\ell }\left( \delta \right) =\left[ \left( y-X\delta
\right) ^{\prime }A\left( y-X\delta \right) \right] /\left[ \left( y-X\delta
\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) \right] 
$. Applying the mean-value theorem to each component of $\widehat{\Delta }%
\left( \delta \right) $ and expanding it around the point $\delta =\delta
_{0}$, we obtain $0=\widehat{\Delta }\left( \widehat{\delta }_{L,n}\right) =%
\widehat{\Delta }\left( \delta _{0}\right) +\left( \partial \widehat{\Delta }%
\left( \overline{\delta }_{n}\right) /\partial \delta ^{\prime }\right)
\left( \widehat{\delta }_{L,n}-\delta _{0}\right) $, with $\overline{\delta }%
_{n}$ lying on the line segment between $\widehat{\delta }_{L,n}$ and $%
\delta _{0}$. Multiplying both sides of this equation by $D_{\mu }^{-1}$, we
further obtain%
\begin{equation}
0=D_{\mu }^{-1}\widehat{\Delta }\left( \delta _{0}\right) +D_{\mu }^{-1}%
\frac{\partial \widehat{\Delta }\left( \overline{\delta }_{n}\right) }{%
\partial \delta ^{\prime }}\left( \widehat{\delta }_{L,n}-\delta _{0}\right)
=D_{\mu }^{-1}\widehat{\Delta }\left( \delta _{0}\right) +D_{\mu }^{-1}\frac{%
\partial \widehat{\Delta }\left( \overline{\delta }_{n}\right) }{\partial
\delta ^{\prime }}D_{\mu }^{-1}D_{\mu }\left( \widehat{\delta }_{L,n}-\delta
_{0}\right)  \label{mean value approx}
\end{equation}%
From the result of Lemma S2-10, we have $-D_{\mu }^{-1}\left( \partial 
\widehat{\Delta }\left( \overline{\delta }_{n}\right) /\partial \delta
^{\prime }\right) D_{\mu }^{-1}=H_{n}+o_{p}\left( 1\right) $, where $%
H_{n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n$ is a positive
definite matrix $a.s.n.$ by Assumption 3(iii), which, in turn, implies that $%
D_{\mu }^{-1}\left( \partial \widehat{\Delta }\left( \overline{\delta }%
_{n}\right) /\partial \delta ^{\prime }\right) D_{\mu }^{-1}$ is nonsingular
and, thus, invertible w.p.a.1. It follows that, for all $n$ sufficiently
large, we can solve for $D_{\mu }\left( \widehat{\delta }_{L,n}-\delta
_{0}\right) $ in (\ref{mean value approx}) above to get%
\begin{eqnarray}
D_{\mu }\left( \widehat{\delta }_{L,n}-\delta _{0}\right) &=&-\left[ D_{\mu
}^{-1}\left( \frac{\partial \widehat{\Delta }\left( \overline{\delta }%
_{n}\right) }{\partial \delta ^{\prime }}\right) D_{\mu }^{-1}\right]
^{-1}D_{\mu }^{-1}\widehat{\Delta }\left( \delta _{0}\right)  \notag \\
&=&H_{n}^{-1}\left( \frac{\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon }{\sqrt{n}}+D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon
\right) \left[ 1+o_{p}\left( 1\right) \right] \text{,}  \label{scaled FELIM}
\end{eqnarray}%
where the last equality follows by applying Lemma S2-9. By part (d) of Lemma
S2-3, $\Sigma _{n}$ is positive definite $a.s.n.$, so that $\Sigma _{n}^{-1}$
is well-defined for all $n$ sufficiently large, and both $\Sigma _{n}^{1/2}$
and $\Sigma _{n}^{-1/2}$ can be taken to be symmetric matrices. Since $H_{n}$
is also symmetric, it further follows that $\Lambda _{I,n}=H_{n}^{-1}\Sigma
_{n}H_{n}^{-1}$ is symmetric and positive definite $a.s.n.$, and both $%
\Lambda _{I,n}^{-1}=\left( H_{n}^{-1}\Sigma _{n}H_{n}^{-1}\right) ^{-1}$ and 
$\Lambda _{I,n}^{-1/2}=\left( H_{n}^{-1}\Sigma _{n}H_{n}^{-1}\right) ^{-1/2}$
are well-defined for all $n$ sufficiently large. Multiplying both sides of
the equation above by $\Lambda _{I,n}^{-1/2}$, we then get $\Lambda
_{I,n}^{-1/2}D_{\mu }\left( \widehat{\delta }_{L,n}-\delta _{0}\right)
=\left( H_{n}^{-1}\Sigma _{n}H_{n}^{-1}\right) ^{-1/2}H_{n}^{-1}\mathcal{Y}%
_{n}\left[ 1+o_{p}\left( 1\right) \right] $, where $\mathcal{Y}_{n}=\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon $. Let $R_{W,n}=\left( H_{n}^{-1}\Sigma
_{n}H_{n}^{-1}\right) ^{-1/2}H_{n}^{-1}\Sigma _{n}^{1/2}$, and note that $%
R_{W,n}R_{W,n}^{\prime }=I_{d}$ for all $n$ sufficiently large. It, thus,
follows from the result given in (\ref{asy normality V}) above and the
continuous mapping theorem that $\Lambda _{I,n}^{-1/2}D_{\mu }\left( 
\widehat{\delta }_{L,n}-\delta _{0}\right) \overset{d}{\rightarrow }N\left(
0,I_{d}\right) $, as $n\rightarrow \infty $, as required.

Turning our attention now to $\widehat{\delta }_{F,n}$, note that we can
write this estimator, appropriately standardized, as%
\begin{eqnarray}
&&D_{\mu }\left( \widehat{\delta }_{F,n}-\delta _{0}\right)  \notag \\
&=&\left( D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }_{F,n}M^{\left(
Z_{1},Q\right) }\right] XD_{\mu }^{-1}\right) ^{-1}D_{\mu }^{-1}X^{\prime }%
\left[ A-\widehat{\ell }_{F,n}M^{\left( Z_{1},Q\right) }\right] \left(
y-X\delta _{0}\right)  \label{FEFUL standardized}
\end{eqnarray}%
so that, multiplying by $\Lambda _{I,n}^{-1/2}=\left( H_{n}^{-1}\Sigma
_{n}H_{n}^{-1}\right) ^{-1/2}$ and applying Lemmas S2-12 and S2-13, we
obtain $\Lambda _{I,n}^{-1/2}D_{\mu }\left( \widehat{\delta }_{F,n}-\delta
_{0}\right) =\left( H_{n}^{-1}\Sigma _{n}H_{n}^{-1}\right) ^{-1/2}H_{n}^{-1}%
\mathcal{Y}_{n}\left[ 1+o_{p}\left( 1\right) \right] $. It follows from the
result given in (\ref{asy normality V}) above and the continuous mapping
theorem that $\Lambda _{I,n}^{-1/2}D_{\mu }\left( \widehat{\delta }%
_{F,n}-\delta _{0}\right) \overset{d}{\rightarrow }N\left( 0,I_{d}\right) $,
as $n\rightarrow \infty $, as required. $\square $

\medskip

\noindent \textbf{Proof of Theorem 3: }To proceed, note that, in this case, $%
\left( \mu _{n}^{\min }\right) /\sqrt{K_{2,n}}=o\left( 1\right) $ but $\sqrt{%
K_{2,n}}/\left( \mu _{n}^{\min }\right) ^{2}\rightarrow 0$, so that, by the
result given in Lemma S2-9, we have%
\begin{equation}
\frac{\mu _{n}^{\min }}{\sqrt{K_{2,n}}}D_{\mu }^{-1}\widehat{\Delta }\left(
\delta _{0}\right) =\frac{\mu _{n}^{\min }}{\sqrt{K_{2,n}}}D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon +o_{p}\left( 1\right)
\label{Thm3 initial approx}
\end{equation}

\noindent where $\underline{U}=U-\varepsilon \rho ^{\prime }$. Again, let $%
H_{n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n$, and $\Sigma
_{2,n}=VC\left( D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon |\mathcal{F}%
_{n}^{W}\right) =D_{\mu }^{-1}VC\left( \underline{U}^{\prime }A\varepsilon |%
\mathcal{F}_{n}^{W}\right) D_{\mu }^{-1}$. Now, by assumption, $\widetilde{L}%
_{n}$ can be any sequence of bounded $\left( l\times d\right) $ non-random
matrices such that $\lambda _{\min }\left( \left( \mu _{n}^{\min }\right)
^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }/K_{2,n}\right) \geq \underline{C}$ \ $a.s.n.$ for some
constant $\underline{C}$ $>0$. It follows that $\left( \mu _{n}^{\min
}\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }/K_{2,n}$ is positive definite $a.s.n.$, so that, with
probability one, $\left( \left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}%
_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) ^{-1/2}$ is well-defined for all $n$ sufficiently large.
Hence, we can let

\noindent $\widetilde{\mathcal{N}}_{n}=\left( \left( \mu _{n}^{\min }\right)
^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }/K_{2,n}\right) ^{-1/2}\widetilde{L}_{n}H_{n}^{-1}\left( \mu
_{n}^{\min }/\sqrt{K_{2,n}}\right) D_{\mu }^{-1}\underline{U}^{\prime
}A\varepsilon $ and construct the linear combination $\mathcal{J}%
_{n}=a^{\prime }\widetilde{\mathcal{N}}_{n}$ for any $a\in \mathbb{R}^{d}$
such that $\left\Vert a\right\Vert _{2}=1$. Next, define $\underline{u}%
_{\left( i,t\right) ,n}=a^{\prime }\left( \left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) ^{-1/2}\widetilde{L}_{n}H_{n}^{-1}D_{\mu }^{-1}\underline{U}%
_{\left( i,t\right) }$, with $\underline{u}_{\left( j,s\right) ,n}$
similarly defined, and we can write $\mathcal{J}_{n}=\left( \mu _{n}^{\min }/%
\sqrt{K_{2,n}}\right) \dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }\left[ \underline{u}_{\left(
i,t\right) ,n}\varepsilon _{\left( j,s\right) }+\underline{u}_{\left(
j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right] =\dsum\nolimits_{%
\left( i,t\right) =2}^{m_{n}}\mathcal{J}_{\left( i,t\right) ,n}$, where $%
\mathcal{J}_{\left( i,t\right) ,n}=\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}%
\right) \dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }\left[ \underline{u}_{\left(
i,t\right) ,n}\varepsilon _{\left( j,s\right) }+\underline{u}_{\left(
j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right] $. Again, define the $%
\sigma $-fields $\mathcal{F}_{\left( i,t\right) ,n}=\sigma \left( \left\{
\varepsilon _{\left( k,\upsilon \right) },U_{\left( k,\upsilon \right)
}\right\} _{\left( k,\upsilon \right) =1}^{\left( i,t\right) },W_{n}\right) $
\ for $\left( i,t\right) =1,2,...,m_{n}$, noting that by construction $%
\mathcal{F}_{\left( i,t\right) -1,n}\subseteq \mathcal{F}_{\left( i,t\right)
,n}$ for $\left( i,t\right) =2,...,m_{n}$ and $\mathcal{J}_{\left(
i,t\right) ,n}$ is $\mathcal{F}_{\left( i,t\right) ,n}-$measurable. In
addition, note that, using Assumption 1, it is easily seen that $E\left[ 
\underline{u}_{\left( i,t\right) ,n}|\mathcal{F}_{\left( i,t\right) -1,n}%
\right] =0$ and $E\left[ \varepsilon _{\left( i,t\right) }|\mathcal{F}%
_{\left( i,t\right) -1,n}\right] =0$, from which it follows that $E\left[ 
\mathcal{J}_{\left( i,t\right) ,n}|\mathcal{F}_{\left( i,t\right) -1,n}%
\right] =$

\noindent $\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right)
\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right) -1}A_{\left(
i,t\right) ,\left( j,s\right) }\left\{ \varepsilon _{\left( j,s\right) }E%
\left[ \underline{u}_{\left( i,t\right) ,n}|\mathcal{F}_{\left( i,t\right)
-1,n}\right] +\underline{u}_{\left( j,s\right) ,n}E\left[ \varepsilon
_{\left( i,t\right) }|\mathcal{F}_{\left( i,t\right) -1,n}\right] \right\}
=0 $. Moreover, applying the CS inequality and making use of the fact that 
\begin{equation}
E\left[ \underline{u}_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
\leq \frac{\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert 
\underline{U}_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}%
\right] \left\Vert \widetilde{L}_{n}\right\Vert _{F}^{2}}{\lambda _{\min
}\left( \left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma
_{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime }/K_{2,n}\right) \left[ \lambda
_{\min }\left( H_{n}\right) \right] ^{2}}\left( \frac{1}{\mu _{n}^{\min }}%
\right) ^{2}=O_{a.s.}\left( \frac{1}{\left( \mu _{n}^{\min }\right) ^{2}}%
\right)  \label{2nd moment bd on u bar}
\end{equation}%
and that $E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}%
\right] \leq \overline{C}$ $a.s.$ by Assumption 2(i), we see that%
\begin{eqnarray}
&&Var\left( \mathcal{J}_{\left( i,t\right) ,n}|\mathcal{F}_{n}^{W}\right) 
\notag \\
&\leq &\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}%
\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right) -1}A_{\left(
i,t\right) ,\left( j,s\right) }^{2}\left( E\left[ \underline{u}_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon _{\left(
j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] +E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \underline{u}_{\left(
j,s\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. +2\sqrt{E\left[ \underline{u}%
_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\sqrt{E\left[ 
\underline{u}_{\left( j,s\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] E\left[
\varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\right) 
\notag \\
&\leq &\frac{4\overline{C}^{2}}{\left( \mu _{n}^{\min }\right) ^{2}}\frac{%
\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left( j,s\right)
=1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}=%
\frac{4\overline{C}^{2}}{K_{2,n}}\dsum\limits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\text{ }a.s.n.
\label{cond var of J}
\end{eqnarray}%
Hence, applying the law of iterated expectations, part (d) of Lemma S2-1,
and Theorem 16.1 of Billingsley (1995), we further deduce that $Var\left( 
\mathcal{J}_{\left( i,t\right) ,n}\right) =E_{W}\left[ E\left( \mathcal{J}%
_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right) \right] \leq \left( 4%
\overline{C}^{2}/K_{2,n}\right) \dsum\nolimits_{\left( j,s\right)
=1}^{\left( i,t\right) -1}E_{W}\left[ A_{\left( i,t\right) ,\left(
j,s\right) }^{2}\right] \leq C$ for some positive constant $C$ for all $n$
sufficiently large. These results show that $\left\{ \mathcal{J}_{\left(
i,t\right) ,n},\mathcal{F}_{\left( i,t\right) ,n},1\leq \left( i,t\right)
\leq m_{n}\text{, }n\geq 1\right\} $ forms a square-integrable martingale
difference array.

Next, we verify condition (\ref{MDA CLT cond 1b}) of the central limit
theorem for martingale difference arrays given in Lemma S2-15 below. By Lo%
\`{e}ve's $c_{r}$ inequality we have%
\begin{eqnarray}
&&\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \left( \frac{\mu
_{n}^{\min }}{\sqrt{K_{2,n}}}\dsum\limits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }\left[ \underline{u}%
_{\left( i,t\right) ,n}\varepsilon _{\left( j,s\right) }+\underline{u}%
_{\left( j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right] \right) ^{4}|%
\mathcal{F}_{n}^{W}\right]  \notag \\
&\leq &8\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\frac{\left( \mu
_{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}E\left[ \left( \dsum\limits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}\underline{u}_{\left( i,t\right) ,n}\varepsilon _{\left( j,s\right)
}\right) ^{4}|\mathcal{F}_{n}^{W}\right]  \notag \\
&&+8\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\frac{\left( \mu _{n}^{\min
}\right) ^{4}}{K_{2,n}^{2}}E\left[ \left( \dsum\limits_{\left( j,s\right)
=1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }%
\underline{u}_{\left( j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right)
^{4}|\mathcal{F}_{n}^{W}\right]  \notag \\
&=&\mathcal{E}_{1}+\mathcal{E}_{2},\text{ }\left( say\right) \text{.}
\label{4th moment bilinear form}
\end{eqnarray}%
Focusing first on $\mathcal{E}_{1}$, it is easy to see that there exists
some positive constant $C$ such that%
\begin{eqnarray*}
\mathcal{E}_{1} &=&\frac{8\left( \mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}E%
\left[ \dsum\limits_{\left( i,t\right) =2}^{m_{n}}\left(
\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right) -1}A_{\left(
i,t\right) ,\left( j,s\right) }\underline{u}_{\left( i,t\right)
,n}\varepsilon _{\left( j,s\right) }\right) ^{4}|\mathcal{F}_{n}^{W}\right]
\\
&\leq &\frac{8\left( \mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}%
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits_{\substack{ \left(
j,s\right) =1  \\ \left( j,s\right) \neq \left( i,t\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{4}E\left[ \underline{u}%
_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( j,s\right) }^{4}|\mathcal{F}_{n}^{W}\right] \\
&&+\frac{24\left( \mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}%
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits_{\substack{ \left(
j,s\right) ,\left( k,\upsilon \right) =1  \\ \left( j,s\right) \neq \left(
i,t\right) ,\left( k,\upsilon \right) \neq \left( i,t\right)  \\ \left(
j,s\right) \neq \left( k,\upsilon \right) }}^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}A_{\left( i,t\right) ,\left( k,\upsilon \right)
}^{2}\left\{ E\left[ \underline{u}_{\left( i,t\right) ,n}^{4}|\mathcal{F}%
_{n}^{W}\right] \right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\times \left. E\left[ \varepsilon
_{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( k,\upsilon \right) }^{2}|\mathcal{F}_{n}^{W}\right] \right\} \\
&\leq &\frac{C}{K_{2,n}}\left[ \frac{1}{K_{2,n}}\dsum\limits_{\substack{ %
\left( i,t\right) ,\left( j,s\right) =1  \\ \left( j,s\right) \neq \left(
i,t\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{4}+\frac{1}{%
K_{2,n}}\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits_{\substack{ %
\left( j,s\right) ,\left( k,\upsilon \right) =1  \\ \left( j,s\right) \neq
\left( i,t\right) ,\left( k,\upsilon \right) \neq \left( i,t\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}A_{\left( i,t\right)
,\left( k,\upsilon \right) }^{2}\right]
\end{eqnarray*}%
where the second inequality above follows from Assumption 2(i) and from an
upper bound on the conditional fourth moment of

\noindent $\underline{u}_{\left( i,t\right) ,n}=a^{\prime }\left( \left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }/K_{2,n}\right) ^{-1/2}\widetilde{L}%
_{n}H_{n}^{-1}D_{\mu }^{-1}\underline{U}_{\left( i,t\right) }$ given by%
\begin{equation*}
E\left[ \underline{u}_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right]
\leq \frac{1}{\left( \mu _{n}^{\min }\right) ^{4}}\left( \max_{1\leq \left(
i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}_{\left( i,t\right)
}\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}\right] \right) \frac{1}{\left[
\lambda _{\min }\left( H_{n}\right) \right] ^{4}}
\end{equation*}%
\begin{eqnarray}
&&\times \left\Vert \widetilde{L}_{n}\right\Vert _{F}^{4}\left( \frac{1}{%
\lambda _{\min }\left( \left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}%
_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) }\right) ^{2}  \notag \\
&\leq &\frac{C^{\ast }}{\left( \mu _{n}^{\min }\right) ^{4}}\text{ \ }a.s.n.%
\text{, for some constant }C^{\ast }>0\text{.}
\label{4th moment bd on u tilde}
\end{eqnarray}%
Note also that, in deriving the upper bound given in (\ref{4th moment bd on
u tilde}), we have applied Assumption 3(iii), Lemma S2-6, the boundedness of 
$\left\Vert \widetilde{L}_{n}\right\Vert _{F}^{2}$ , and the assumption that

\noindent $\lambda _{\min }\left( \left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) \geq \underline{C}>0$ \ $a.s.n.$ Moreover, by parts (b) and
(c) of Lemma S2-1, we have that $K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{4}=O_{a.s.}\left(
K_{2,n}^{2}/n^{2}\right) $ and

\noindent $K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
=1}^{m_{n}}\dsum\nolimits_{\left( j,s\right) ,\left( k,\upsilon \right)
=1,\left( j,s\right) \neq \left( i,t\right) ,\left( k,\upsilon \right) \neq
\left( i,t\right) }^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}A_{\left( i,t\right) ,\left( k,\upsilon \right) }^{2}=O_{a.s.}\left(
K_{2,n}/n\right) $. from which it follows that $n\mathcal{E}%
_{1}=O_{a.s.}\left( 1\right) $ in light of Assumption 5(ii). Hence, by
applying the law of iterated expectations and Theorem 16.1 of Billingsley
(1995), we obtain%
\begin{eqnarray*}
&&nE_{W_{n}}\left[ \mathcal{E}_{1}\right]  \\
&=&\frac{8n\left( \mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}E_{W_{n}}\left\{
E\left[ \dsum\limits_{\left( i,t\right) =2}^{m_{n}}\left(
\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right) -1}A_{\left(
i,t\right) ,\left( j,s\right) }\underline{u}_{\left( i,t\right)
,n}\varepsilon _{\left( j,s\right) }\right) ^{4}|\mathcal{F}_{n}^{W}\right]
\right\}  \\
&\leq &\frac{Cn}{K_{2,n}}\left\{ E_{W_{n}}\left[ \frac{1}{K_{2,n}}%
\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1 \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{4}+\frac{1}{K_{2,n}}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{\substack{ \left( j,s\right) ,\left( k,\upsilon
\right) =1 \\ \left( j,s\right) \neq \left( i,t\right) ,\left( k,\upsilon
\right) \neq \left( i,t\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}A_{\left( i,t\right) ,\left( k,\upsilon \right) }^{2}\right]
\right\}  \\
&&\text{(for all }n\text{ sufficiently large)} \\
&=&O\left( 1\right) \text{,}
\end{eqnarray*}%
which shows that $E_{W_{n}}\left[ \mathcal{E}_{1}\right] =O\left( 1/n\right)
=o\left( 1\right) $. In a similar way, we can also show that

\noindent $E_{W_{n}}\left[ \mathcal{E}_{2}\right] =8\left[ \left( \mu
_{n}^{\min }\right) ^{4}/K_{2,n}^{2}\right] E\left[ \dsum\nolimits_{\left(
i,t\right) =2}^{m_{n}}\left( \dsum\nolimits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}A_{\left( j,s\right) ,\left( i,t\right) }\underline{u}_{\left(
i,t\right) ,n}\varepsilon _{\left( i,t\right) }\right) ^{4}\right] =o\left(
1\right) $. Condition (\ref{MDA CLT cond 1b}) of Lemma S2-15 then follows
from these calculations since%
\begin{equation*}
\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \left( \frac{\mu
_{n}^{\min }}{\sqrt{K_{2,n}}}\dsum\limits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }\left[ \underline{u}%
_{\left( i,t\right) ,n}\varepsilon _{\left( j,s\right) }+\underline{u}%
_{\left( j,s\right) ,n}\varepsilon _{\left( i,t\right) }\right] \right) ^{4}%
\right] \leq E_{W_{n}}\left[ \mathcal{E}_{1}\right] +E_{W_{n}}\left[ 
\mathcal{E}_{2}\right] =o\left( 1\right)
\end{equation*}

Next, we verify condition (\ref{MDA CLT cond 2}) of Lemma S2-15. Note first
that, by construction, $Var\left( \mathcal{J}_{n}|\mathcal{F}_{n}^{W}\right) 
$

\noindent $=a^{\prime }\left( \widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}%
_{n}^{\prime }\right) ^{-1/2}\widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}%
_{n}^{\prime }\left( \widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}%
_{n}^{\prime }\right) ^{-1/2}a=1$, with $\Lambda _{II,n}=\left[ \left( \mu
_{n}^{\min }\right) ^{2}/K_{2,n}\right] H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}$.
This, in turn, implies that $Var\left( \mathcal{J}_{n}\right) =E_{W_{n}}%
\left[ E\left( \mathcal{J}_{n}^{2}|\mathcal{F}_{n}^{W}\right) \right]
=E_{W_{n}}\left[ Var\left( \mathcal{J}_{n}|\mathcal{F}_{n}^{W}\right) \right]
=1$. On the other hand, by direct calculation, we obtain%
\begin{eqnarray}
1 &=&Var\left( \mathcal{J}_{n}|\mathcal{F}_{n}^{W}\right)  \notag \\
&=&\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \varepsilon _{\left(
j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \underline{u}_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]  \notag \\
&&+\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \underline{u}%
_{\left( j,s\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]  \notag \\
&&+2\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \underline{u}%
_{\left( j,s\right) ,n}\varepsilon _{\left( j,s\right) }|\mathcal{F}_{n}^{W}%
\right] E\left[ \underline{u}_{\left( i,t\right) ,n}\varepsilon _{\left(
i,t\right) }|\mathcal{F}_{n}^{W}\right]  \label{normalized cond var}
\end{eqnarray}%
Making use of expression (\ref{normalized cond var}), we obtain, after some
further calculations,%
\begin{eqnarray*}
&&\dsum\limits_{\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{J}_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{\left( i,t\right) -1,n}\right] -1 \\
&=&\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \varepsilon _{\left(
j,s\right) }^{2}-E\left[ \varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}%
_{n}^{W}\right] \right) E\left[ \underline{u}_{\left( i,t\right) ,n}^{2}|%
\mathcal{F}_{n}^{W}\right] \\
&&+\frac{\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \underline{u}_{\left(
j,s\right) ,n}^{2}-E\left[ \underline{u}_{\left( j,s\right) ,n}^{2}|\mathcal{%
F}_{n}^{W}\right] \right) E\left[ \varepsilon _{\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right] \\
&&+\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \underline{u}_{\left(
j,s\right) ,n}\varepsilon _{\left( j,s\right) }-E\left[ \underline{u}%
_{\left( j,s\right) ,n}\varepsilon _{\left( j,s\right) }|\mathcal{F}_{n}^{W}%
\right] \right) E\left[ \underline{u}_{\left( i,t\right) ,n}\varepsilon
_{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] \\
&&+\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right) =2}^{\left( i,t\right)
-1}\dsum\limits_{\left( k,\upsilon \right) =1}^{\left( j,s\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left(
k,\upsilon \right) }E\left[ \underline{u}_{\left( i,t\right) ,n}\varepsilon
_{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] \left\{ \underline{u}%
_{\left( j,s\right) ,n}\varepsilon _{\left( k,\upsilon \right) }+\varepsilon
_{\left( j,s\right) }\underline{u}_{\left( k,\upsilon \right) ,n}\right\} \\
&&+\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right) =2}^{\left( i,t\right)
-1}\dsum\limits_{\left( k,\upsilon \right) =1}^{\left( j,s\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left(
k,\upsilon \right) }\varepsilon _{\left( j,s\right) }\varepsilon _{\left(
k,\upsilon \right) }E\left[ \underline{u}_{\left( i,t\right) ,n}^{2}|%
\mathcal{F}_{n}^{W}\right]
\end{eqnarray*}%
\begin{eqnarray}
&&+\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}\dsum\limits_{\left(
i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right) =2}^{\left( i,t\right)
-1}\dsum\limits_{\left( k,\upsilon \right) =1}^{\left( j,s\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left(
k,\upsilon \right) }\underline{u}_{\left( j,s\right) ,n}\underline{u}%
_{\left( k,\upsilon \right) ,n}E\left[ \varepsilon _{\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right]  \notag \\
&=&\mathcal{TT}_{1}+\mathcal{TT}_{2}+\mathcal{TT}_{3}+\mathcal{TT}_{4}+%
\mathcal{TT}_{5}+\mathcal{TT}_{6}  \label{Thm 3 cond 2}
\end{eqnarray}

To analyze the terms $\mathcal{TT}_{k}$ $\left( k=1,..,6\right) $, note
first that, by applying parts (b) and (a) of Lemma S2-16, we obtain $%
\mathcal{TT}_{1}\overset{p}{\rightarrow }0$ and $\mathcal{TT}_{2}\overset{p}{%
\rightarrow }0$, respectively. Consider now the term 
\begin{equation*}
\mathcal{TT}_{3}=\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}%
\dsum\limits_{\left( i,t\right) =2}^{m_{n}}\dsum\limits_{\left( j,s\right)
=1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\left( \underline{u}_{\left( j,s\right) ,n}\varepsilon _{\left(
j,s\right) }-E\left[ \underline{u}_{\left( j,s\right) ,n}\varepsilon
_{\left( j,s\right) }|\mathcal{F}_{n}^{W}\right] \right) E\left[ \underline{u%
}_{\left( i,t\right) ,n}\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}%
\right]
\end{equation*}%
In this case, we apply part (a) of Lemma S2-8 with $u_{\left( j,s\right)
,n}=\left( \mu _{n}^{\min }\right) \underline{u}_{\left( j,s\right) ,n}$,

\noindent $\overline{\psi }_{\left( j,s\right) }=E\left[ \left( \mu
_{n}^{\min }\right) \underline{u}_{\left( j,s\right) ,n}\varepsilon _{\left(
j,s\right) }|\mathcal{F}_{n}^{W}\right] $, and $\phi _{\left( i,t\right) }=E%
\left[ \left( \mu _{n}^{\min }\right) \underline{u}_{\left( i,t\right)
,n}\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] $. Note
that, in this case, $\left\{ \left( u_{\left( i,t\right) ,n},\varepsilon
_{\left( i,t\right) }\right) \right\} _{\left( i,t\right) =1}^{m_{n}}$ is
independent conditional on $\mathcal{F}_{n}^{W}=\sigma \left( W_{n}\right) $%
, and

\noindent $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.$ by
Assumptions 1(i) and 2(i), respectively. Moreover, the upper bound given by (%
\ref{4th moment bd on u tilde}) implies that there exists a constant $%
C^{\ast }>0$ such that $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
u_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] =\max_{1\leq \left(
i,t\right) \leq m_{n}}\left( \mu _{n}^{\min }\right) ^{4}E\left[ \underline{u%
}_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right] \leq \left( \mu
_{n}^{\min }\right) ^{4}C^{\ast }/\left( \mu _{n}^{\min }\right)
^{4}=C^{\ast }$ \ $a.s.n.$ Finally, note that, by using the fact that

\noindent $\underline{u}_{\left( i,t\right) ,n}=a^{\prime }\left( \left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }/K_{2,n}\right) ^{-1/2}\widetilde{L}%
_{n}H_{n}^{-1}D_{\mu }^{-1}\underline{U}_{\left( i,t\right) }$ and by
applying Assumption 2(i), Lemma S2-6, and the assumption that

\noindent $\lambda _{\min }\left( \left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) \geq \underline{C}>0$ \ $a.s.n.$; we can show that there
exists a constant $C>0$ such that%
\begin{eqnarray}
&&E\left[ \left\vert \left( \mu _{n}^{\min }\right) \underline{u}_{\left(
i,t\right) ,n}\varepsilon _{\left( i,t\right) }\right\vert |\mathcal{F}%
_{n}^{W}\right]  \notag \\
&=&\left( \mu _{n}^{\min }\right) E\left[ \left\vert \varepsilon _{\left(
i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }D_{\mu
}^{-1}H_{n}^{-1}\widetilde{L}_{n}^{\prime }\left( \frac{\left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }}{K_{2,n}}\right) ^{-1/2}a\right\vert |\mathcal{F}%
_{n}^{W}\right]  \notag \\
&\leq &\left( \mu _{n}^{\min }\right) \sqrt{E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\left[ a^{\prime }\left( \frac{%
\left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma
_{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime }}{K_{2,n}}\right) ^{-1/2}%
\widetilde{L}_{n}H_{n}^{-1}D_{\mu }^{-1}\right.  \notag \\
&&\left. \times E\left[ \underline{U}_{\left( i,t\right) }\underline{U}%
_{\left( i,t\right) }^{\prime }|\mathcal{F}_{n}^{W}\right] D_{\mu
}^{-1}H_{n}^{-1}\widetilde{L}_{n}^{\prime }\left( \frac{\left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }}{K_{2,n}}\right) ^{-1/2}a\right] ^{1/2}\text{ \
\ }  \notag \\
&&\text{(by CS inequality)}  \notag
\end{eqnarray}%
\begin{eqnarray}
&\leq &\left( \mu _{n}^{\min }\right) \sqrt{E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\frac{1}{\left( \mu _{n}^{\min
}\right) }\left( \sqrt{\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\left\Vert \underline{U}_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}%
_{n}^{W}\right] }\right)  \notag \\
&&\times \frac{1}{\lambda _{\min }\left( \Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma /n\right) }\left\Vert \widetilde{L}_{n}\right\Vert
_{F}\left( \frac{1}{\sqrt{\lambda _{\min }\left( \left( \mu _{n}^{\min
}\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }/K_{2,n}\right) }}\right)  \notag \\
&\leq &C<\infty \text{ \ }a.s.\text{ for all }\left( i,t\right) \in \left\{
1,2,...,m_{n}\right\} \text{ and for all }n\text{ sufficiently large}
\label{bd on cov of e and u tilde}
\end{eqnarray}%
from which we further deduce that $\max_{\left( i,t\right) }\left\vert \phi
_{\left( i,t\right) }\right\vert \leq \max_{\left( i,t\right) }E\left[
\left\vert \left( \mu _{n}^{\min }\right) \underline{u}_{\left( i,t\right)
,n}\varepsilon _{\left( i,t\right) }\right\vert |\mathcal{F}_{n}^{W}\right]
\leq C$ $a.s.n.$and also that $\max_{\left( j,s\right) }\left\vert \overline{%
\psi }_{\left( j,s\right) }\right\vert \leq \max_{\left( j,s\right) }E\left[
\left\vert \left( \mu _{n}^{\min }\right) \underline{u}_{\left( j,s\right)
,n}\varepsilon _{\left( j,s\right) }\right\vert |\mathcal{F}_{n}^{W}\right]
\leq C$ $a.s.n.$ Hence, applying part (a) of Lemma S2-8, we have $\mathcal{TT%
}_{3}\overset{p}{\rightarrow }0$.

Next, consider the term%
\begin{equation*}
\mathcal{TT}_{4}=\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}%
\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right)
=2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left(
i,t\right) ,\left( k,\upsilon \right) }E\left[ \underline{u}_{\left(
i,t\right) ,n}\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right]
\left\{ \underline{u}_{\left( j,s\right) ,n}\varepsilon _{\left( k,\upsilon
\right) }+\varepsilon _{\left( j,s\right) }\underline{u}_{\left( k,\upsilon
\right) ,n}\right\}
\end{equation*}%
Here, we apply part (b) of Lemma S2-8 with $u_{\left( j,s\right) ,n}=\left(
\mu _{n}^{\min }\right) \underline{u}_{\left( j,s\right) ,n}$ and

\noindent $\phi _{\left( i,t\right) }=E\left[ \left( \mu _{n}^{\min }\right) 
\underline{u}_{\left( i,t\right) ,n}\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] $. Note that $\left\{ \left( u_{\left( i,t\right)
,n},\varepsilon _{\left( i,t\right) }\right) \right\} _{\left( i,t\right)
=1}^{m_{n}}$ is independent conditional on $\mathcal{F}_{n}^{W}=\sigma
\left( W_{n}\right) $, and

\noindent $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.$ by
Assumptions 1 and 2(i), respectively. Moreover, from calculations given
previously, we have $\max_{1\leq \left( i,t\right) \leq m_{n}}\left( \mu
_{n}^{\min }\right) ^{4}E\left[ \underline{u}_{\left( i,t\right) ,n}^{4}|%
\mathcal{F}_{n}^{W}\right] \leq C$ \ $a.s.n.$ and $\max_{\left( i,t\right)
}\left\vert \phi _{\left( i,t\right) }\right\vert \leq C$ $a.s.n.$ Hence, by
applying part (b) of Lemma S2-8, we deduce that $\mathcal{TT}_{4}\overset{p}{%
\rightarrow }0$.

Turning our attention to the term%
\begin{equation*}
\mathcal{TT}_{5}=\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}%
\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right)
=2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left(
i,t\right) ,\left( k,\upsilon \right) }\varepsilon _{\left( j,s\right)
}\varepsilon _{\left( k,\upsilon \right) }E\left[ \underline{u}_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
\end{equation*}%
For this term, we apply part (c) of Lemma S2-8 with $\phi _{\left(
i,t\right) }=E\left[ u_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] 
$ with $u_{\left( i,t\right) ,n}=\left( \mu _{n}^{\min }\right) \underline{u}%
_{\left( i,t\right) ,n}$. From (\ref{2nd moment bd on u bar}), there exists
a positive constant $C$ such that $E\left[ u_{\left( i,t\right) ,n}^{2}|%
\mathcal{F}_{n}^{W}\right] =\left( \mu _{n}^{\min }\right) ^{2}E\left[ 
\underline{u}_{\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \leq
C<\infty $ \ $a.s.$ for all $\left( i,t\right) \in \left\{
1,2,...,m_{n}\right\} $ and for all $n$ sufficiently large, so that $%
\max_{\left( i,t\right) }\left\vert \phi _{\left( i,t\right) }\right\vert $

\noindent $=\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ u_{\left(
i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.n.$ Hence,
applying part (c) of Lemma S2-8, we obtain $\mathcal{TT}_{5}\overset{p}{%
\rightarrow }0$.

Finally, consider the term%
\begin{equation*}
\mathcal{TT}_{6}=\frac{2\left( \mu _{n}^{\min }\right) ^{2}}{K_{2,n}}%
\dsum\limits_{\left( i,t\right) =3}^{m_{n}}\dsum\limits_{\left( j,s\right)
=2}^{\left( i,t\right) -1}\dsum\limits_{\left( k,\upsilon \right)
=1}^{\left( j,s\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }A_{\left(
i,t\right) ,\left( k,\upsilon \right) }\underline{u}_{\left( j,s\right) ,n}%
\underline{u}_{\left( k,\upsilon \right) ,n}E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\end{equation*}%
In this case, we apply part (d) of Lemma S2-8 with $u_{\left( j,s\right)
}=\left( \mu _{n}^{\min }\right) \underline{u}_{\left( j,s\right) ,n}$, $%
u_{\left( k,\upsilon \right) }=\left( \mu _{n}^{\min }\right) \underline{u}%
_{\left( k,\upsilon \right) ,n}$, and $\phi _{\left( i,t\right) }=E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $. Using a
conditional version of Liapounov's inequality and Assumption 2(i), we obtain 
$E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\leq \left( E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \right) ^{1/2}\leq C<\infty $ \ $a.s.$ for all $\left( i,t\right)
\in \left\{ 1,2,...,m_{n}\right\} $ and for all $n$ sufficiently large, so
that $\max_{\left( i,t\right) }\left\vert \phi _{\left( i,t\right)
}\right\vert =\max_{\left( i,t\right) }E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \leq C$ $a.s.n.$ Moreover, the
upper bound in (\ref{4th moment bd on u tilde}) implies that $\max_{1\leq
\left( i,t\right) \leq m_{n}}E\left[ u_{\left( i,t\right) ,n}^{4}|\mathcal{F}%
_{n}^{W}\right] =\max_{1\leq \left( i,t\right) \leq m_{n}}\left( \mu
_{n}^{\min }\right) ^{4}E\left[ \underline{u}_{\left( i,t\right) ,n}^{4}|%
\mathcal{F}_{n}^{W}\right] \leq C$ \ $a.s.n.$ It follows by applying part
(d) of Lemma S2-8 that $\mathcal{TT}_{6}\overset{p}{\rightarrow }0$.

It follows from the above calculations that the terms $\mathcal{TT}_{k}$ $%
\overset{p}{\rightarrow }0$ for each $k\in \left\{ 1,...,6\right\} $, which
in light of equation (\ref{Thm 3 cond 2}) implies that $\dsum\nolimits_{%
\left( i,t\right) =2}^{m_{n}}E\left[ \mathcal{J}_{\left( i,t\right) ,n}^{2}|%
\mathcal{F}_{\left( i,t\right) -1,n}\right] -1=o_{p}\left( 1\right) $. This
establishes condition (\ref{MDA CLT cond 2}) of Lemma S2-15. It now follows
from Lemma S2-15 that $\mathcal{J}_{n}=\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}%
\right) a^{\prime }\left( \left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}%
_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) ^{-1/2}\widetilde{L}_{n}H_{n}^{-1}D_{\mu }^{-1}\underline{U}%
^{\prime }A\varepsilon \overset{d}{\rightarrow }N\left( 0,1\right) $. Since
this result holds for all $a\in \mathbb{R}^{d}$ such that $\left\Vert
a\right\Vert _{2}=1$, applying the Cram\'{e}r-Wold device, we further deduce
that%
\begin{equation}
\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right) \left( \widetilde{L}%
_{n}\Lambda _{II,n}\widetilde{L}_{n}^{\prime }\right) ^{-1/2}\widetilde{L}%
_{n}H_{n}^{-1}D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon \overset{d}{%
\rightarrow }N\left( 0,I_{d}\right) ,  \label{asy normality CaseII}
\end{equation}%
where $\Lambda _{II,n}=\left( \mu _{n}^{\min }\right) ^{2}H_{n}^{-1}\Sigma
_{2,n}H_{n}^{-1}/K_{2,n}$ with $H_{n}=\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma /n$. Next, recall that $\widehat{\Delta }\left(
\delta \right) =-\left[ \left( y-X\delta \right) ^{\prime }M^{\left(
Z_{1},Q\right) }\left( y-X\delta \right) /2\right] \left[ \partial \widehat{Q%
}_{FELIM}\left( \delta \right) /\partial \delta \right] $; and note that, by
Lemma S2-10, we have $-D_{\mu }^{-1}\left( \partial \widehat{\Delta }\left( 
\overline{\delta }_{n}\right) /\partial \delta ^{\prime }\right) D_{\mu
}^{-1}=H_{n}+o_{p}\left( 1\right) $, with $H_{n}$ being positive definite
given Assumption 3(iii), so that upon inverting the expansion given in
expression (\ref{mean value approx}) above and multiplying by $\left( \mu
_{n}^{\min }\right) /\sqrt{K_{2,n}}$, we obtain%
\begin{eqnarray*}
\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right) D_{\mu }\left( \widehat{\delta 
}_{L,n}-\delta _{0}\right) &=&\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right)
H_{n}^{-1}D_{\mu }^{-1}\widehat{\Delta }\left( \delta _{0}\right) \left[
1+o_{p}\left( 1\right) \right] \\
&=&\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right) H_{n}^{-1}D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon \left[ 1+o_{p}\left( 1\right) \right] 
\text{,}
\end{eqnarray*}

\noindent where the last equality comes from applying expression (\ref{Thm3
initial approx}). It follows by multiplying both sides of the equation above
by $\left( \widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}_{n}\right) ^{-1/2}%
\widetilde{L}_{n}$ and applying the result given in expression (\ref{asy
normality CaseII}) that $\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right)
\left( \widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}_{n}\right) ^{-1/2}%
\widetilde{L}_{n}D_{\mu }\left( \widehat{\delta }_{L,n}-\delta _{0}\right) 
\overset{d}{\rightarrow }N\left( 0,I_{d}\right) $. \textit{\ }

Turning our attention now to $\widehat{\delta }_{F,n}$, note that, using
expression (\ref{FEFUL standardized}) above, we can write%
\begin{eqnarray*}
&&\frac{\left( \mu _{n}^{\min }\right) D_{\mu }\left( \widehat{\delta }%
_{F,n}-\delta _{0}\right) }{\sqrt{K_{2,n}}} \\
&=&\frac{\left( \mu _{n}^{\min }\right) \left( D_{\mu }^{-1}X^{\prime }\left[
A-\widehat{\ell }_{F,n}M^{\left( Z_{1},Q\right) }\right] XD_{\mu
}^{-1}\right) ^{-1}D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }%
_{F,n}M^{\left( Z_{1},Q\right) }\right] \left( y-X\delta _{0}\right) }{\sqrt{%
K_{2,n}}}
\end{eqnarray*}%
It follows by applying Lemmas S2-12 and S2-13 that%
\begin{equation}
\frac{\mu _{n}^{\min }}{\sqrt{K_{2,n}}}D_{\mu }\left( \widehat{\delta }%
_{F,n}-\delta _{0}\right) =\frac{\mu _{n}^{\min }}{\sqrt{K_{2,n}}}%
H_{n}^{-1}D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon +o_{p}\left(
1\right) \text{,}  \label{normalized FEFUL II}
\end{equation}%
noting that, in this case, $\left( \mu _{n}^{\min }\right) /\sqrt{K_{2,n}}%
=o\left( 1\right) $ but $\sqrt{K_{2,n}}/\left( \mu _{n}^{\min }\right)
^{2}\rightarrow 0$. Again, let $\Lambda _{II,n}=\left( \mu _{n}^{\min
}\right) ^{2}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}/K_{2,n}$ and let $\widetilde{L%
}_{n}$ be any sequence of bounded $\left( l\times d\right) $ non-random
matrices such that $\lambda _{\min }\left( \widetilde{L}_{n}\Lambda _{II,n}%
\widetilde{L}_{n}^{\prime }\right) \geq \underline{C}$ \ $a.s.n.$ It follows
by multiplying both sides of equation (\ref{normalized FEFUL II}) above by $%
\left( \widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}_{n}\right) ^{-1/2}%
\widetilde{L}_{n}$ and applying the result given in expression (\ref{asy
normality CaseII}) that$\left( \mu _{n}^{\min }/\sqrt{K_{2,n}}\right) \left( 
\widetilde{L}_{n}\Lambda _{II,n}\widetilde{L}_{n}\right) ^{-1/2}\widetilde{L}%
_{n}D_{\mu }\left( \widehat{\delta }_{F,n}-\delta _{0}\right) \overset{d}{%
\rightarrow }N\left( 0,I_{d}\right) $. $\square $

\section*{\noindent Appendix S2: Key Lemmas Used in Proving the Main
Theorems\noindent}

\noindent \textbf{Lemma S2-1: }Let $A=P^{\perp }-M^{\left( Z,Q\right) }D_{%
\widehat{\vartheta }}M^{\left( Z,Q\right) }$. Then, under Assumptions 2-7,
the following statements hold as $K_{2,n}$, $n\rightarrow \infty $.

\noindent (a) $\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right)
=1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}=O_{a.s.}\left( K_{2,n}\right) $.

\noindent (b) $\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right)
=1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{4}=O_{a.s.}\left( K_{2,n}^{3}/n^{2}\right) $.

\noindent (c) $\dsum\nolimits_{\left( j,s\right)
=1}^{m_{n}}\dsum\nolimits_{\left( i,t\right) ,\left( k,\upsilon \right)
=1,\left( i,t\right) \neq \left( j,s\right) ,\left( k,\upsilon \right) \neq
\left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}A_{\left( j,s\right) ,\left( k,\upsilon \right) }^{2}=O_{a.s.}\left(
K_{2,n}^{2}/n\right) $.

\noindent (d) $\max_{1\leq \left( i,t\right) \leq m_{n}}\left(
\dsum\nolimits_{\left( j,s\right) =1}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}\right) =O_{a.s.}\left( K_{2,n}/n\right) $.

\noindent \textbf{Proof of Lemma S2-1:}

To show part (a), note first that, given Lemma 1, there exists a constant $%
C>0$ such that%
\begin{eqnarray*}
tr\left\{ D_{\widehat{\vartheta }}^{2}\right\} &=&d_{P^{\perp }}^{\prime
}\left( M^{\left( Z,Q\right) }\circ M^{\left( Z,Q\right) }\right)
^{-2}d_{P^{\perp }}\leq \frac{d_{P^{\perp }}^{\prime }d_{P^{\perp }}}{\left[
\lambda _{\min }\left( M^{\left( Z,Q\right) }\circ M^{\left( Z,Q\right)
}\right) \right] ^{2}} \\
&\leq &\left( \frac{1}{C}\right) ^{2}d_{P^{\perp }}^{\prime }d_{P^{\perp }}%
\text{ \ }a.s.\text{ (by Lemma 1)}
\end{eqnarray*}%
\textbf{\ }%
\begin{eqnarray*}
&\leq &\left( \frac{1}{C}\right) ^{2}\left( \max_{1\leq \left( i,t\right)
\leq m_{n}}P_{\left( i,t\right) ,\left( i,t\right) }^{\perp }\right)
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}P_{\left( i,t\right) ,\left(
i,t\right) }^{\perp } \\
&=&O_{a.s.}\left( \frac{K_{2,n}^{2}}{n}\right) \text{ \ (by Assumption
5(iv)).\ \ }
\end{eqnarray*}%
Now, by straightforward calculations and by making use of the inequality $%
\left\vert \dsum\nolimits_{i=1}^{G}a_{i}\right\vert ^{r}\leq
G^{r-1}\dsum\nolimits_{i=1}^{G}\left\vert a_{i}\right\vert ^{r}$, we get%
\begin{eqnarray*}
\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2} &=&\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}^{m_{n}}\left(
P_{\left( i,t\right) ,\left( j,s\right) }^{\perp }-e_{\left( i,t\right)
}^{\prime }M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}M^{\left(
Z,Q\right) }e_{\left( j,s\right) }\right) ^{2} \\
&\leq &2\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ %
\left( i,t\right) \neq \left( j,s\right) }}^{m_{n}}\left( P_{\left(
i,t\right) ,\left( j,s\right) }^{\perp }\right) ^{2}+2\dsum\limits 
_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right)
\neq \left( j,s\right) }}^{m_{n}}\left( e_{\left( i,t\right) }^{\prime
}M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}M^{\left( Z,Q\right)
}e_{\left( j,s\right) }\right) ^{2} \\
&\leq &2\left[ K_{2,n}+\dsum\limits_{\left( i,t\right) =1}^{m_{n}}e_{\left(
i,t\right) }^{\prime }M^{\left( Z,Q\right) }D_{\widehat{\vartheta }%
}M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}M^{\left( Z,Q\right)
}e_{\left( i,t\right) }\right] \\
&\leq &2\left[ K_{2,n}+tr\left\{ M^{\left( Z,Q\right) }D_{\widehat{\vartheta 
}}^{2}M^{\left( Z,Q\right) }\right\} \right] =2\left[ K_{2,n}+tr\left\{ D_{%
\widehat{\vartheta }}M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}\right\} %
\right] \\
&\leq &2\left[ K_{2,n}+tr\left\{ D_{\widehat{\vartheta }}^{2}\right\} \right]
=O_{a.s.}\left( K_{2,n}\right) +O_{a.s.}\left( \frac{K_{2,n}^{2}}{n}\right)
=O_{a.s.}\left( K_{2,n}\right) \text{.}
\end{eqnarray*}

Parts (b)-(d) can be shown by using arguments that are analogous to that
given to show part (a) above. Hence, for the sake of brevity, we will not
provide an explicit proof for these parts here, but a proof can be obtained
from the authors upon request. $\square $

\medskip

\noindent \textbf{Lemma S2-2: }Suppose that Assumptions 1-7 are satisfied.
Then, the following statements are true: (a) $D_{\mu }^{-1}X^{\prime
}M^{\left( Z_{1},Q\right) }XD_{\mu }^{-1}=O_{p}\left( n\left( \mu _{n}^{\min
}\right) ^{-2}\right) $; (b) $D_{\mu }^{-1}X^{\prime }AXD_{\mu
}^{-1}=H_{n}+o_{p}\left( 1\right) $, where $H_{n}=\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma /n=O_{p}\left( 1\right) $.

\noindent \textbf{Proof of Lemma S2-2:}

To show part (a), note first that $D_{\mu }^{-1}X^{\prime }M^{\left(
Z_{1},Q\right) }XD_{\mu }^{-1}$

\noindent $\leq 3\left[ \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma
/n+D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left( Z_{1},Q\right) }FD_{\kappa
}D_{\mu }^{-1}/n+D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right) }UD_{\mu
}^{-1}\right] $, where

\noindent $F=\left( f\left( W_{1,\left( 1,1\right) }\right) ,..,f\left(
W_{1,\left( 1,T_{1}\right) }\right) ,...,f\left( W_{1,\left( n,1\right)
}\right) ,..,f\left( W_{1,\left( n,T_{n}\right) }\right) \right) ^{\prime }$
and where we take $A$

\noindent $\leq B$ for two square matrices $A$ and $B$ to mean that $A-B$ is
negative semi-definite, or, alternatively, $B-A$ is positive semidefinite.
Now, for $a,b\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert
_{2}=\left\Vert b\right\Vert _{2}=1$, we can apply the CS inequality and
Assumption 3(iii) to obtain $\left\vert a^{\prime }\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma b/n\right\vert \leq \sqrt{a^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma a/n}\sqrt{b^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma b/n}\leq \sqrt{a^{\prime
}\Gamma ^{\prime }\Gamma a/n}\sqrt{b^{\prime }\Gamma ^{\prime }\Gamma b/n}%
=O_{a.s.}\left( 1\right) $. Since the above argument holds for all $a$, $%
b\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert _{2}=\left\Vert
b\right\Vert _{2}=1$, we further deduce that $\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma /n=O_{p}\left( 1\right) $. Next, note that, for $%
a,b\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert _{2}=\left\Vert
b\right\Vert _{2}=1$, we can apply the CS inequality along with Assumptions
3(ii), 4(i), 5(i), and 7(ii) to obtain%
\begin{eqnarray*}
\left\vert \frac{a^{\prime }D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left(
Z_{1},Q\right) }FD_{\kappa }D_{\mu }^{-1}b}{n}\right\vert &\leq &\sqrt{\frac{%
a^{\prime }D_{\mu }^{-1}D_{\kappa }\left( F-Z_{1}\Theta ^{K_{1,n}}\right)
^{\prime }M^{\left( Z_{1},Q\right) }\left( F-Z_{1}\Theta ^{K_{1,n}}\right)
D_{\kappa }D_{\mu }^{-1}a}{n}} \\
&&\times \sqrt{\frac{b^{\prime }D_{\mu }^{-1}D_{\kappa }\left( F-Z_{1}\Theta
^{K_{1,n}}\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( F-Z_{1}\Theta
^{K_{1,n}}\right) D_{\kappa }D_{\mu }^{-1}b}{n}}
\end{eqnarray*}%
\begin{equation*}
\leq \frac{m_{n}d}{n}\left\Vert f\left( \cdot \right) -\Theta
^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert _{\infty ,d}^{2}\frac{%
\left( \kappa _{n}^{\max }\right) ^{2}}{\left( \mu _{n}^{\min }\right) ^{2}}%
=O_{a.s.}\left( \frac{\left( \kappa _{n}^{\max }\right) ^{2}}{\left( \mu
_{n}^{\min }\right) ^{2}K_{1,n}^{2\varrho _{f}}}\right)
\end{equation*}%
Since the above argument holds for all $a$, $b\in \mathbb{R}^{d}$ such that $%
\left\Vert a\right\Vert _{2}=\left\Vert b\right\Vert _{2}=1$, we further
deduce that $D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left( Z_{1},Q\right)
}FD_{\kappa }D_{\mu }^{-1}/n=O_{a.s.}\left( \left( \kappa _{n}^{\max
}\right) ^{2}\left( \mu _{n}^{\min }\right) ^{-2}K_{1,n}^{-2\varrho
_{f}}\right) $. Finally, note that, by the CS inequality, $\left\vert
a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right) }UD_{\mu
}^{-1}b\right\vert \leq \sqrt{a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left(
Z_{1},Q\right) }UD_{\mu }^{-1}a}\sqrt{b^{\prime }D_{\mu }^{-1}U^{\prime
}M^{\left( Z_{1},Q\right) }UD_{\mu }^{-1}b}$. Now, by Assumptions 2(i),
3(ii), and 6(ii), 
\begin{eqnarray*}
E\left[ a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right) }UD_{\mu
}^{-1}a|\mathcal{F}_{n}^{W}\right] &\leq &a^{\prime }D_{\mu }^{-1}E\left[
U^{\prime }U|\mathcal{F}_{n}^{W}\right] D_{\mu }^{-1}a \\
&\leq &\frac{\overline{T}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E%
\left[ \left\Vert U_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}%
_{n}^{W}\right] \right) n}{\left( \mu _{n}^{\min }\right) ^{2}}%
=O_{a.s.}\left( \frac{n}{\left( \mu _{n}^{\min }\right) ^{2}}\right) \text{,}
\end{eqnarray*}%
where $\overline{T}=\max_{1\leq \left( i,t\right) \leq m_{n}}T_{i}$. Hence,
by Theorem 16.1 of Billingsley (1995), there exists a constant $\overline{C}%
<\infty $ such that for all $n$ sufficiently large $E\left[ \left( \left(
\mu _{n}^{\min }\right) ^{2}/n\right) a^{\prime }D_{\mu }^{-1}U^{\prime
}M^{\left( Z_{1},Q\right) }UD_{\mu }^{-1}a\right] =E_{W_{n}}\left( \left(
\left( \mu _{n}^{\min }\right) ^{2}/n\right) E\left[ \frac{a^{\prime
}U^{\prime }M^{\left( Z_{1},Q\right) }Ua}{n}|\mathcal{F}_{n}^{W}\right]
\right) \leq \overline{C}$. It follows from the Markov's inequality that $%
a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right) }UD_{\mu
}^{-1}a=O_{p}\left( n\left( \mu _{n}^{\min }\right) ^{-2}\right) $. In the
same way, we also have

\noindent $b^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right)
}UD_{\mu }^{-1}b=O_{p}\left( n/\left( \mu _{n}^{\min }\right) ^{2}\right) $,
so that $\left\vert a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left(
Z_{1},Q\right) }UD_{\mu }^{-1}b\right\vert \leq $

\noindent $\sqrt{a^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right)
}UD_{\mu }^{-1}a}\sqrt{b^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left(
Z_{1},Q\right) }UD_{\mu }^{-1}b}=O_{p}\left( n\left( \mu _{n}^{\min }\right)
^{-2}\right) $. Since this result holds for all $a$, $b\in \mathbb{R}^{d}$
such that $\left\Vert a\right\Vert _{2}=\left\Vert b\right\Vert _{2}=1$, we
further deduce that $D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right)
}UD_{\mu }^{-1}=O_{p}\left( n\left( \mu _{n}^{\min }\right) ^{-2}\right) $.
Putting these results together, it follows that $D_{\mu }^{-1}X^{\prime
}M^{\left( Z_{1},Q\right) }XD_{\mu }^{-1}=O_{p}\left( 1\right) +O_{p}\left(
\left( \kappa _{n}^{\max }\right) ^{2}\left( \mu _{n}^{\min }\right)
^{-2}K_{1,n}^{-2\varrho _{f}}\right) +O_{p}\left( n\left( \mu _{n}^{\min
}\right) ^{-2}\right) =O_{p}\left( n\left( \mu _{n}^{\min }\right)
^{-2}\right) $, as required to show part (a).

Part (b) of this lemma can be proved by generalizing the argument for Lemma
A2 of Hausman et al (2012) to our setting here with cluster sampling, fixed
effects, and possibly many covariates. For the sake of brevity, we will not
include an explicit proof here, but the details of a proof can be obtained
from the authors upon request. $\square $

\medskip

\noindent \textbf{Lemma S2-3: }Let $\underline{U}=U-\varepsilon \rho
^{\prime }$ and $\underline{U}_{\left( i,t\right) }=U_{\left( i,t\right)
}-\rho \varepsilon _{\left( i,t\right) }$ and let $VC\left( X|\mathcal{F}%
_{n}^{W}\right) $ denote the conditional covariance matrix of the random
vector $X$ given $\mathcal{F}_{n}^{W}$. Under Assumptions 1-2, 5-6, and 8;
there exists positive constants $0<\underline{C}\leq \overline{C}<\infty $
such that the following statements are true.

\noindent (a) $\lambda _{\max }\left[ VC\left( \Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) \right]
\leq \overline{C}$ \ \ $a.s.$ and $\lambda _{\min }\left[ VC\left( \Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}%
_{n}^{W}\right) \right] \geq \underline{C}$ \ $a.s.$ for all $n$
sufficiently large.

\noindent (b) $VC\left( \underline{U}^{\prime }A\varepsilon /\sqrt{K_{2,n}}|%
\mathcal{F}_{n}^{W}\right) \geq \underline{C}I_{d}>\underset{d\times d}{0}$
\ $a.s.$, for all $n$ sufficiently large.

\noindent (c) $\lambda _{\max }\left( VC\left[ \underline{U}^{\prime
}A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}_{n}^{W}\right] \right) \leq 
\overline{C}$ \ $a.s.$, $\lambda _{\max }\left( VC\left[ \underline{U}%
^{\prime }A\varepsilon /\sqrt{K_{2,n}}\right] \right) \leq \overline{C}$,

\noindent $\lambda _{\max }\left( VC\left[ U^{\prime }A\varepsilon /\sqrt{%
K_{2,n}}|\mathcal{F}_{n}^{W}\right] \right) \leq \overline{C}$ \ $a.s.$, and 
$\lambda _{\max }\left( VC\left[ U^{\prime }A\varepsilon /\sqrt{K_{2,n}}%
\right] \right) \leq \overline{C}$, for all $n$ sufficiently large.

\noindent (d) For any $a\in \mathbb{R}^{d}$ with $\left\Vert a\right\Vert
_{2}=1$ and for all $n$ sufficiently large, $\lambda _{\min }\left( \Sigma
_{n}\right) \geq \underline{C}>0$ $a.s.$ and $a^{\prime }\Sigma
_{n}^{-1}a\leq \overline{C}<\infty $ \ $a.s.$, where $\Sigma _{n}=VC\left( 
\mathcal{Y}_{n}|\mathcal{F}_{n}^{W}\right) =\Sigma _{1,n}+\Sigma _{2,n}$, as
defined in section 3 of the main paper, and where $\mathcal{Y}_{n}=\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon $.

\noindent \textbf{Proof of Lemma S2-3:}

For part (a), note that, by Assumptions 1, 2, and 3(iii); there exists a
pair of constants $0<\underline{C}$ $\leq \overline{C}<\infty $ such that,
for any $b\in \mathbb{R}^{d}$ such that $\left\Vert b\right\Vert =1$ and for
all $n$ sufficiently large, $b^{\prime }VC\left( \Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) b=b^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }E\left[ \varepsilon \varepsilon
^{\prime }|\mathcal{F}_{n}^{W}\right] M^{\left( Z_{1},Q\right) }\Gamma
b/n\leq \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) \lambda _{\max
}\left( \Gamma ^{\prime }\Gamma /n\right) \leq \overline{C}$ $a.s.$ and $%
b^{\prime }VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /%
\sqrt{n}|\mathcal{F}_{n}^{W}\right) b\geq \left( \min_{1\leq \left(
i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{%
F}_{n}^{W}\right] \right) b^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma b/n\geq \underline{C}$ $a.s.$ Since the above bounds
hold for any $b\in \mathbb{R}^{d}$ such that $\left\Vert b\right\Vert _{2}=1$%
, it follows that, almost surely, $\lambda _{\max }\left[ VC\left( \Gamma
^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}%
_{n}^{W}\right) \right] =\max_{\left\Vert b\right\Vert =1}b^{\prime
}VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|%
\mathcal{F}_{n}^{W}\right) b\leq \overline{C}<\infty $ and $\lambda _{\min }%
\left[ VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /%
\sqrt{n}|\mathcal{F}_{n}^{W}\right) \right] =\min_{\left\Vert b\right\Vert
=1}b^{\prime }VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) b\geq \underline{C}>0$ for
all $n$ sufficiently large, which establishes the required result.

To show part (b), let $a\in \mathbb{R}^{d}$ such that $\left\Vert
a\right\Vert _{2}=1$, and we define $\underline{U}_{\left( i,t\right)
}=U_{\left( i,t\right) }-\rho \varepsilon _{\left( i,t\right) }$, $%
\underline{u}_{a,\left( i,t\right) }=a^{\prime }\underline{U}_{\left(
i,t\right) }$, $\sigma _{\left( i,t\right) }^{2}=E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $, $\widetilde{\omega }%
_{a,\left( i,t\right) }^{2}=E\left[ \underline{u}_{a,\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right] $, $\widetilde{\psi }_{a,\left( i,t\right) }=E%
\left[ \varepsilon _{\left( i,t\right) }\underline{u}_{a,\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] $, and $\varrho _{a,\left( i,t\right) }=%
\widetilde{\psi }_{a,\left( i,t\right) }/\left( \sigma _{\left( i,t\right) }%
\widetilde{\omega }_{a,\left( i,t\right) }\right) $; for $\left( i,t\right)
=1,..,m_{n}$, where we have suppressed the dependence of $\sigma _{\left(
i,t\right) }^{2}$, $\widetilde{\omega }_{a,\left( i,t\right) }^{2}$, $%
\widetilde{\psi }_{a,\left( i,t\right) }$, and $\varrho _{a,\left(
i,t\right) }$ on $\mathcal{F}_{n}^{W}=\sigma \left( W_{n}\right) $ for
notational convenience. Note also that we can write $a^{\prime }VC\left( 
\underline{U}^{\prime }A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}%
_{n}^{W}\right) a=$

\noindent $K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \left( \varepsilon
_{\left( j,s\right) }\underline{u}_{a,\left( i,t\right) }+\varepsilon
_{\left( i,t\right) }\underline{u}_{a,\left( j,s\right) }\right) ^{2}|%
\mathcal{F}_{n}^{W}\right] $, since, by construction, $A_{\left( i,t\right)
,\left( i,t\right) }=0$ for $\left( i,t\right) =1,...,m_{n}$. Moreover,
define $\delta _{a,\left( i,t\right) ,\left( j,s\right) }=\left( 
\begin{array}{cc}
\sigma _{\left( j,s\right) }\widetilde{\omega }_{a,\left( i,t\right) } & 
\sigma _{\left( i,t\right) }\widetilde{\omega }_{a,\left( j,s\right) }%
\end{array}%
\right) ^{\prime }$ and 
\begin{equation*}
\Delta _{\left( i,t\right) ,\left( j,s\right) }^{a}=\left( 
\begin{array}{cc}
1 & \varrho _{a,\left( i,t\right) }\varrho _{a,\left( j,s\right) } \\ 
\varrho _{a,\left( i,t\right) }\varrho _{a,\left( j,s\right) } & 1%
\end{array}%
\right)
\end{equation*}%
and note that, given that $\left( j,s\right) <\left( i,t\right) $, we have $E%
\left[ \left( \varepsilon _{\left( j,s\right) }\underline{u}_{a,\left(
i,t\right) }+\varepsilon _{\left( i,t\right) }\underline{u}_{a,\left(
j,s\right) }\right) ^{2}|\mathcal{F}_{n}^{W}\right] =\delta _{a,\left(
i,t\right) ,\left( j,s\right) }^{\prime }\Delta _{\left( i,t\right) ,\left(
j,s\right) }^{a}\delta _{a,\left( i,t\right) ,\left( j,s\right) }$. Now, by
the quadratic formula, the smallest eigenvalue of $\Delta _{\left(
i,t\right) ,\left( j,s\right) }^{a}$ is given by $\lambda _{\min }\left(
\Delta _{\left( i,t\right) ,\left( j,s\right) }^{a}\right) =1-\left\vert
\varrho _{a,\left( i,t\right) }\right\vert \left\vert \varrho _{a,\left(
j,s\right) }\right\vert $. In addition, write%
\begin{eqnarray*}
\widetilde{\Omega }_{\left( i,t\right) } &=&\left( 
\begin{array}{cc}
E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] & E 
\left[ \varepsilon _{\left( i,t\right) }\underline{U}_{\left( i,t\right)
}^{\prime }|\mathcal{F}_{n}^{W}\right] \\ 
E\left[ \underline{U}_{\left( i,t\right) }\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] & E\left[ \underline{U}_{\left( i,t\right) }%
\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}_{n}^{W}\right]%
\end{array}%
\right) \\
&=&\left( 
\begin{array}{cc}
1 & 0 \\ 
-\rho & I_{d}%
\end{array}%
\right) \left( 
\begin{array}{cc}
E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] & E 
\left[ \varepsilon _{\left( i,t\right) }U_{\left( i,t\right) }^{\prime }|%
\mathcal{F}_{n}^{W}\right] \\ 
E\left[ U_{\left( i,t\right) }\varepsilon _{\left( i,t\right) }|\mathcal{F}%
_{n}^{W}\right] & E\left[ U_{\left( i,t\right) }U_{\left( i,t\right)
}^{\prime }|\mathcal{F}_{n}^{W}\right]%
\end{array}%
\right) \left( 
\begin{array}{cc}
1 & -\rho ^{\prime } \\ 
0 & I_{d}%
\end{array}%
\right) \\
&=&L_{\rho }\Omega _{\left( i,t\right) }L_{\rho }^{\prime }\text{, where }%
L_{\rho }\text{ }\mathbb{=}\left( 
\begin{array}{cc}
1 & 0 \\ 
-\rho & I_{d}%
\end{array}%
\right) \text{. }
\end{eqnarray*}%
Note that $L_{\rho }$ is nonsingular, so that $L_{\rho }L_{\rho }^{\prime }$
is positive definite. Hence, by Assumption 2 part (ii) and by the fact that $%
L_{\rho }L_{\rho }^{\prime }$ is a fixed, finite-dimensional positive
definite matrix, there exists some constant $C_{1}>1$ such that 
\begin{equation}
\min_{1\leq \left( i,t\right) \leq m_{n}}\lambda _{\min }\left( \widetilde{%
\Omega }_{\left( i,t\right) }\right) \geq \min_{1\leq \left( i,t\right) \leq
m_{n}}\lambda _{\min }\left( \Omega _{\left( i,t\right) }\right) \lambda
_{\min }\left( L_{\rho }L_{\rho }^{\prime }\right) \geq 1/C_{1}>0\text{ }%
a.s.n.  \label{Omegtilde lower bd}
\end{equation}

Next, let%
\begin{equation*}
\underset{\left( d+1\right) \times 2}{D_{a}}=\left( 
\begin{array}{cc}
1 & 0 \\ 
0 & a%
\end{array}%
\right) \text{, }D_{SD,\left( i,t\right) }=\left( 
\begin{array}{cc}
\sigma _{\left( i,t\right) } & 0 \\ 
0 & \widetilde{\omega }_{a,\left( i,t\right) }%
\end{array}%
\right) \text{, and }D_{\varrho ,\left( i,t\right) }=\left( 
\begin{array}{cc}
1 & \varrho _{a,\left( i,t\right) } \\ 
\varrho _{a,\left( i,t\right) } & 1%
\end{array}%
\right)
\end{equation*}%
and note that%
\begin{eqnarray*}
D_{a}^{\prime }\widetilde{\Omega }_{\left( i,t\right) }D_{a} &=&\left( 
\begin{array}{cc}
1 & 0 \\ 
0 & a^{\prime }%
\end{array}%
\right) \left( 
\begin{array}{cc}
E\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] & E 
\left[ \varepsilon _{\left( i,t\right) }\underline{U}_{\left( i,t\right)
}^{\prime }|\mathcal{F}_{n}^{W}\right] \\ 
E\left[ \underline{U}_{\left( i,t\right) }\varepsilon _{\left( i,t\right) }|%
\mathcal{F}_{n}^{W}\right] & E\left[ \underline{U}_{\left( i,t\right) }%
\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}_{n}^{W}\right]%
\end{array}%
\right) \left( 
\begin{array}{cc}
1 & 0 \\ 
0 & a%
\end{array}%
\right) \\
&=&\left( 
\begin{array}{cc}
\sigma _{\left( i,t\right) } & 0 \\ 
0 & \widetilde{\omega }_{a,\left( i,t\right) }%
\end{array}%
\right) \left( 
\begin{array}{cc}
1 & \varrho _{a,\left( i,t\right) } \\ 
\varrho _{a,\left( i,t\right) } & 1%
\end{array}%
\right) \left( 
\begin{array}{cc}
\sigma _{\left( i,t\right) } & 0 \\ 
0 & \widetilde{\omega }_{a,\left( i,t\right) }%
\end{array}%
\right) =D_{SD,\left( i,t\right) }D_{\varrho ,\left( i,t\right)
}D_{SD,\left( i,t\right) }\text{,}
\end{eqnarray*}%
Now, as can be seen from expression (\ref{Omegtilde lower bd}) above, an
implication of Assumption 2(ii) is that%
\begin{eqnarray}
\min_{1\leq \left( i,t\right) \leq m_{n}}\sigma _{\left( i,t\right) }^{2}
&=&e_{1,d+1}^{\prime }\widetilde{\Omega }_{\left( i,t\right) }e_{1,d+1}\geq
1/C_{1}>0\text{ }a.s.n.  \label{sigma2 lower bd} \\
\min_{1\leq \left( i,t\right) \leq m_{n}}\widetilde{\omega }_{a,\left(
i,t\right) }^{2} &=&\underline{a}^{\prime }\widetilde{\Omega }_{\left(
i,t\right) }\underline{a}\geq 1/C_{1}>0\text{ }a.s.n.
\label{omega2 lower bd}
\end{eqnarray}%
where $\underset{\left( d+1\right) \times 1}{e_{1,d+1}}=\left( 
\begin{array}{cc}
1 & 0^{\prime }%
\end{array}%
\right) ^{\prime }$ and $\underset{\left( d+1\right) \times 1}{\underline{a}}%
=\left( 
\begin{array}{cc}
0 & a^{\prime }%
\end{array}%
\right) ^{\prime }$, from which we deduce that $D_{SD,\left( i,t\right) }$
is invertible almost surely for each $\left( i,t\right) \in \left\{
1,...,m_{n}\right\} $ and for all $n$ sufficiently large. The invertibility
of $D_{SD,\left( i,t\right) }$ then allows us to write $D_{\varrho ,\left(
i,t\right) }=D_{SD,\left( i,t\right) }^{-1}D_{a}^{\prime }\widetilde{\Omega }%
_{\left( i,t\right) }D_{a}D_{SD,\left( i,t\right) }^{-1}$. On the other
hand, Assumption 2(i) implies that there exists some constant $C_{2}>1$ such
that%
\begin{equation}
\min_{1\leq \left( i,t\right) \leq m_{n}}\lambda _{\min }\left( D_{SD,\left(
i,t\right) }^{-1}\right) =\frac{1}{\max_{1\leq \left( i,t\right) \leq
m_{n}}\lambda _{\max }\left( D_{SD,\left( i,t\right) }\right) }\geq \frac{1}{%
C_{2}}>0\text{ \ }a.s.  \label{DSD lower bd}
\end{equation}%
It follows from the fact that $\lambda _{\min }\left( D_{a}^{\prime
}D_{a}\right) =\lambda _{\min }\left( I_{2}\right) =1$ and from making use
of Assumptions 2(i) and (ii) and the lower bounds given in (\ref{Omegtilde
lower bd}) and (\ref{DSD lower bd}) that%
\begin{eqnarray*}
\min_{1\leq \left( i,t\right) \leq m_{n}}\lambda _{\min }\left( D_{\varrho
,\left( i,t\right) }\right) &\geq &\min_{1\leq \left( i,t\right) \leq
m_{n}}\lambda _{\min }\left( \widetilde{\Omega }_{\left( i,t\right) }\right)
\lambda _{\min }\left( D_{a}^{\prime }D_{a}\right) \lambda _{\min }\left(
D_{SD,\left( i,t\right) }^{-2}\right) \\
&\geq &\frac{\min_{1\leq \left( i,t\right) \leq m_{n}}\lambda _{\min }\left( 
\widetilde{\Omega }_{\left( i,t\right) }\right) }{\max_{1\leq \left(
i,t\right) \leq m_{n}}\left( \lambda _{\max }\left( D_{SD,\left( i,t\right)
}\right) \right) ^{2}}\geq \frac{1}{C^{3}}>0\text{ \ }a.s.n.,
\end{eqnarray*}%
where $C=\max \left\{ C_{1},C_{2}\right\} $. Moreover, by solving the
characteristic equation of $D_{\varrho ,\left( i,t\right) }$, we see that
the smallest eigenvalue of $D_{\varrho ,\left( i,t\right) }$ is given by $%
\lambda _{\min }\left( D_{\varrho ,\left( i,t\right) }\right) =1-\left\vert
\varrho _{a,\left( i,t\right) }\right\vert $, so that $\min_{1\leq \left(
i,t\right) \leq m_{n}}\lambda _{\min }\left( D_{\varrho ,\left( i,t\right)
}\right) =1-\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \rho
_{a,\left( i,t\right) }\right\vert \geq 1/C^{3}>0$ \ $a.s.n.$, from which we
further deduce that $\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert
\varrho _{a,\left( i,t\right) }\right\vert \leq 1-\left( 1/C^{3}\right) <1$
\ $a.s.n.$ Applying this upper bound along with the lower bounds given by (%
\ref{sigma2 lower bd}) and (\ref{omega2 lower bd}) as well as the fact that $%
\lambda _{\min }\left( \Delta _{\left( i,t\right) ,\left( j,s\right)
}^{a}\right) =1-\left\vert \varrho _{a,\left( i,t\right) }\right\vert
\left\vert \varrho _{a,\left( j,s\right) }\right\vert $, as derived earlier,
we have%
\begin{eqnarray*}
E\left[ \left( \varepsilon _{\left( j,s\right) }\underline{u}_{a,\left(
i,t\right) }+\varepsilon _{\left( i,t\right) }\underline{u}_{a,\left(
j,s\right) }\right) ^{2}|\mathcal{F}_{n}^{W}\right] &=&\delta _{a,\left(
i,t\right) ,\left( j,s\right) }^{\prime }\Delta _{\left( i,t\right) ,\left(
j,s\right) }^{a}\delta _{a,\left( i,t\right) ,\left( j,s\right) } \\
&\geq &\left[ 1-\left\vert \varrho _{a,\left( i,t\right) }\right\vert
\left\vert \varrho _{a,\left( j,s\right) }\right\vert \right] \left[ \sigma
_{\left( j,s\right) }^{2}\widetilde{\omega }_{a,\left( i,t\right)
}^{2}+\sigma _{\left( i,t\right) }^{2}\widetilde{\omega }_{a,\left(
j,s\right) }^{2}\right] \\
&\geq &\left( \frac{2}{C^{3}}-\frac{1}{C^{6}}\right) \left[ \frac{1}{C^{2}}+%
\frac{1}{C^{2}}\right] \geq \frac{2}{C^{5}}>0\text{ \ \ \ }a.s.n.
\end{eqnarray*}%
Summing over $1\leq \left( j,s\right) <\left( i,t\right) \leq m_{n}$, we
obtain

\noindent $K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \left( \varepsilon
_{\left( j,s\right) }\underline{u}_{a,\left( i,t\right) }+\varepsilon
_{\left( i,t\right) }\underline{u}_{a,\left( j,s\right) }\right) ^{2}|%
\mathcal{F}_{n}^{W}\right] $

\noindent $\geq \left( 2/C^{5}\right) K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) =2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}=\left(
1/C^{5}\right) K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
=1}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}$, where the last equality follows from the symmetry
of $A$ and by the fact that $A_{\left( i,t\right) ,\left( i,t\right) }=0$
for $\left( i,t\right) =1,...,m_{n}$. Furthermore, by straightforward
calculation, we obtain%
\begin{eqnarray*}
\frac{1}{K_{2,n}}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2} &=&\frac{1}{K_{2,n}}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{\left( j,s\right) =1}^{m_{n}}\left[ e_{\left(
i,t\right) }^{\prime }P^{\perp }e_{\left( j,s\right) }-e_{\left( i,t\right)
}^{\prime }M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}M^{\left(
Z,Q\right) }e_{\left( j,s\right) }\right] ^{2} \\
&=&1+\frac{1}{K_{2,n}}\dsum\limits_{\left( i,t\right) =1}^{m_{n}}e_{\left(
i,t\right) }^{\prime }M^{\left( Z,Q\right) }D_{\widehat{\vartheta }%
}M^{\left( Z,Q\right) }D_{\widehat{\vartheta }}M^{\left( Z,Q\right)
}e_{\left( i,t\right) }\geq 1
\end{eqnarray*}%
Putting everything together, we have

\noindent $K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}E\left[ \left( \varepsilon
_{\left( j,s\right) }\underline{u}_{a,\left( i,t\right) }+\varepsilon
_{\left( i,t\right) }\underline{u}_{a,\left( j,s\right) }\right) ^{2}|%
\mathcal{F}_{n}^{W}\right] \geq $

\noindent $\left( 1/C^{5}\right) K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) =1}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{m_{n}}A_{\left(
i,t\right) ,\left( j,s\right) }^{2}\geq 1/C^{5}\geq \underline{C}>0$, by
choosing $\underline{C}$ such that $0<\underline{C}\leq 1/C^{5}$. Since the
above argument holds for any $a\in \mathbb{R}^{d}$ such that $\left\Vert
a\right\Vert =1$, it further follows that $VC\left( \underline{U}^{\prime
}A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}_{n}^{W}\right) \geq \underline{C}%
I_{d}>0$ \ $a.s.$, as required.

To show part (c), note first that, given Assumption 2(i), there exists a
positive constant $C$ such that%
\begin{eqnarray}
&&\max_{1\leq \left( j,s\right) \leq m_{n}}\lambda _{\max }\left( E\left[ 
\underline{U}_{\left( j,s\right) }\underline{U}_{\left( j,s\right) }^{\prime
}|\mathcal{F}_{n}^{W}\right] \right)  \notag \\
&\leq &\max_{1\leq \left( j,s\right) \leq m_{n}}tr\left\{ E\left[ \underline{%
U}_{\left( j,s\right) }\underline{U}_{\left( j,s\right) }^{\prime }|\mathcal{%
F}_{n}^{W}\right] \right\}  \notag \\
&\leq &\max_{1\leq \left( j,s\right) \leq m_{n}}\left\{ E\left[ \left\Vert
U_{\left( j,s\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right] +2E%
\left[ \left\vert U_{\left( j,s\right) }^{\prime }\rho \varepsilon _{\left(
j,s\right) }\right\vert |\mathcal{F}_{n}^{W}\right] +\rho ^{\prime }\rho E%
\left[ \varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\right\} \text{ }  \notag \\
&\leq &\max_{1\leq \left( j,s\right) \leq m_{n}}\left\{ E\left[ \left\Vert
U_{\left( j,s\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right]
+2\left\Vert \rho \right\Vert _{2}\sqrt{E\left[ \varepsilon _{\left(
j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\sqrt{E\left[ \left\Vert
U_{\left( j,s\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right] }%
\right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ }\left. +\left\Vert \rho \right\Vert
_{2}^{2}E\left[ \varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}%
\right] \right\}  \notag \\
&\leq &2\left\{ \left( \max_{1\leq \left( j,s\right) \leq m_{n}}E\left[
\left\Vert U_{\left( j,s\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}%
\right] \right) +\left\Vert \rho \right\Vert _{2}^{2}\left( \max_{1\leq
\left( j,s\right) \leq m_{n}}E\left[ \varepsilon _{\left( j,s\right) }^{2}|%
\mathcal{F}_{n}^{W}\right] \right) \right\}  \notag \\
&\leq &C<\infty \text{ \ }a.s.  \label{max eig EUU}
\end{eqnarray}

\noindent where the third inequality above follows from applying the CS
inequality while the fourth inequality stems in part from applying the
inequality $\left\vert XY\right\vert \leq \left( 1/2\right) X^{2}+\left(
1/2\right) Y^{2}$. Now, for any $a\in \mathbb{R}^{d}$ such that $\left\Vert
a\right\Vert _{2}=1$; we obtain by applying the triangle and CS
inequalities, expression (\ref{max eig EUU}), as well as part (a) of Lemma
S2-1 and Assumptions 2(i) and 8%
\begin{eqnarray*}
&&a^{\prime }VC\left( \underline{U}^{\prime }A\varepsilon /\sqrt{K_{2,n}}|%
\mathcal{F}_{n}^{W}\right) a \\
&\leq &\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] a^{\prime }E%
\left[ \underline{U}_{\left( j,s\right) }\underline{U}_{\left( j,s\right)
}^{\prime }|\mathcal{F}_{n}^{W}\right] a\right. \\
&&\text{ \ \ \ \ \ \ \ }\left. +\sqrt{a^{\prime }E\left[ \underline{U}%
_{\left( i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}%
_{n}^{W}\right] a}\sqrt{E\left[ \varepsilon _{\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right] }\sqrt{a^{\prime }E\left[ \underline{U}_{\left(
j,s\right) }\underline{U}_{\left( j,s\right) }^{\prime }|\mathcal{F}_{n}^{W}%
\right] a}\sqrt{E\left[ \varepsilon _{\left( j,s\right) }^{2}|\mathcal{F}%
_{n}^{W}\right] }\right) \\
&\leq &\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{ E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \lambda
_{\max }\left( E\left[ \underline{U}_{\left( j,s\right) }\underline{U}%
_{\left( j,s\right) }^{\prime }|\mathcal{F}_{n}^{W}\right] \right) \right. \\
&&\text{ }\left. +\sqrt{\lambda _{\max }\left( E\left[ \underline{U}_{\left(
j,s\right) }\underline{U}_{\left( j,s\right) }^{\prime }|\mathcal{F}_{n}^{W}%
\right] \right) }\sqrt{E\left[ \varepsilon _{\left( i,t\right) }^{2}|%
\mathcal{F}_{n}^{W}\right] }\sqrt{\lambda _{\max }\left( E\left[ \underline{U%
}_{\left( j,s\right) }\underline{U}_{\left( j,s\right) }^{\prime }|\mathcal{F%
}_{n}^{W}\right] \right) }\sqrt{E\left[ \varepsilon _{\left( j,s\right)
}^{2}|\mathcal{F}_{n}^{W}\right] }\right\} \\
&=&O_{a.s.}\left( 1\right) \text{.}
\end{eqnarray*}%
From this, we deduce that $\lambda _{\max }\left[ VC\left( \underline{U}%
^{\prime }A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}_{n}^{W}\right) \right]
=\max_{\left\Vert a\right\Vert =1}a^{\prime }VC\left( \underline{U}^{\prime
}A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}_{n}^{W}\right) a$

\noindent $\leq \overline{C}$ \ $a.s.n.$ Moreover, by applying the law of
iterated expectations and part (i) of Theorem 16.1 of Billingsley (1995)
that there exists a constant $\overline{C}>0$ such that $a^{\prime }VC\left( 
\underline{U}^{\prime }A\varepsilon /\sqrt{K_{2,n}}\right)
a=E_{W_{n}}\left\{ E\left[ \left( a^{\prime }\underline{U}^{\prime
}A\varepsilon \right) ^{2}/K_{2,n}|\mathcal{F}_{n}^{W}\right] \right\} \leq 
\overline{C}$, for all $a\in \mathbb{R}^{d}$ such that $\left\Vert
a\right\Vert =1$, from which we further deduce the unconditional version of
this inequality, i.e., $\lambda _{\max }\left( VC\left[ \underline{U}%
^{\prime }A\varepsilon /\sqrt{K_{2,n}}\right] \right) =\max_{\left\Vert
a\right\Vert =1}a^{\prime }VC\left( \underline{U}^{\prime }A\varepsilon /%
\sqrt{K_{2,n}}\right) a\leq \overline{C}<\infty $, where $\underline{U}%
=U-\varepsilon \rho ^{\prime }$ and $\underline{U}_{\left( i,t\right)
}=U_{\left( i,t\right) }-\rho \varepsilon _{\left( i,t\right) }$.

Furthermore, since $\underline{U}=U-\varepsilon \rho ^{\prime }$, we see
that, by setting $\rho =0$ in the argument given above, we can also show
that there exists a constant $\overline{C}$ such that $\lambda _{\max
}\left( VC\left[ U^{\prime }A\varepsilon /\sqrt{K_{2,n}}|\mathcal{F}_{n}^{W}%
\right] \right) \leq \overline{C}<\infty $ \ $a.s.n.$ and $\lambda _{\max
}\left( VC\left[ U^{\prime }A\varepsilon /\sqrt{K_{2,n}}\right] \right) \leq 
\overline{C}<\infty $ for all $n$ sufficiently large.

Finally, to show part (d), note first that, by straightforward calculations,
we get $\Sigma _{n}=VC\left( \mathcal{Y}_{n}|\mathcal{F}_{n}^{W}\right)
=VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|%
\mathcal{F}_{n}^{W}\right) +VC\left( D_{\mu }^{-1}\underline{U}^{\prime
}A\varepsilon |\mathcal{F}_{n}^{W}\right) =\Sigma _{1,n}+\Sigma _{2,n}$. It
follows by part (a) of this lemma that there exists a positive constant $%
\underline{C}$ such that $\lambda _{\min }\left( \Sigma _{n}\right) \geq
\lambda _{\min }\left[ VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) \right] +\lambda _{\min }%
\left[ VC\left( D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon |\mathcal{F}%
_{n}^{W}\right) \right] \geq \lambda _{\min }\left[ VC\left( \Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) %
\right] $

\noindent $\geq \underline{C}>0$ \ $a.s.n.$, so that $\Sigma _{n}$ is
positive definite $a.s.n$. Moreover, again by part (a) of this lemma, for
any $a\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert =1$, 
\begin{eqnarray*}
a^{\prime }\Sigma _{n}^{-1}a &\leq &\frac{1}{\lambda _{\min }\left\{
VC\left( \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}|%
\mathcal{F}_{n}^{W}\right) +VC\left( D_{\mu }^{-1}\underline{U}^{\prime
}A\varepsilon |\mathcal{F}_{n}^{W}\right) \right\} } \\
&\leq &\frac{1}{\lambda _{\min }\left[ VC\left( \Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon /\sqrt{n}|\mathcal{F}_{n}^{W}\right) \right] }%
\leq \frac{1}{\underline{C}}\leq \overline{C}<\infty \text{ }a.s.n.
\end{eqnarray*}%
where $\overline{C}$ can be taken to be any finite, positive constant such
that $\overline{C}\geq 1/\underline{C}$. $\square $

\bigskip

\noindent \textbf{Lemma S2-4: }Under Assumptions 1-7, the following results
hold: (a) $D_{\mu }^{-1}X^{\prime }A\varphi _{n}=O_{p}\left( \tau
_{n}/K_{1,n}^{\varrho _{g}}\right) $; (b) $D_{\mu }^{-1}X^{\prime
}A\varepsilon =\frac{\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon 
}{\sqrt{n}}+D_{\mu }^{-1}U^{\prime }A\varepsilon +O_{p}\left(
K_{2,n}^{-\varrho _{\gamma }}\right) +O_{p}\left( K_{2,n}^{-\left( \varrho
_{\gamma }-1\right) }n^{-1}\right) +O_{p}\left( \kappa _{n}^{\max }/\left(
\mu _{n}^{\min }K_{1,n}^{\varrho _{f}}\right) \right) =O_{p}\left( \max
\left\{ 1,\sqrt{K_{2,n}}/\left( \mu _{n}^{\min }\right) \right\} \right) $

\noindent

\noindent \textbf{Proof of Lemma S2-4:}

To show part (a), first write $D_{\mu }^{-1}X^{\prime }A\varphi _{n}=D_{\mu
}^{-1}\left( \Upsilon _{n}+\Phi _{n}+Q\Xi +U\right) ^{\prime }A\varphi
_{n}=D_{\mu }^{-1}\Upsilon _{n}^{\prime }A\varphi _{n}+D_{\mu }^{-1}\Phi
_{n}^{\prime }A\varphi _{n}+D_{\mu }^{-1}U^{\prime }A\varphi _{n}$, where
the second equality above follows from the fact that $Q^{\prime }A\varphi
_{n}=0$. We will analyze each term on the right-hand side of the expression
above in turn. Consider first the term $D_{\mu }^{-1}\Upsilon _{n}^{\prime
}A\varphi _{n}$. For $b\in \mathbb{R}^{d}$ such that $\left\Vert
b\right\Vert _{2}=1$, note that, making use of the triangle and CS
inequalities and Assumptions 3(iii), 5, 7(i), and 7(iii); we get%
\begin{eqnarray*}
\left\vert b^{\prime }D_{\mu }^{-1}\Upsilon _{n}^{\prime }A\varphi
_{n}\right\vert &\leq &\left\vert b^{\prime }D_{\mu }^{-1}\Upsilon
_{n}^{\prime }P^{\perp }\varphi _{n}\right\vert +\left\vert b^{\prime
}D_{\mu }^{-1}\Upsilon _{n}^{\prime }M^{\left( Z,Q\right) }D_{\widehat{%
\vartheta }}M^{\left( Z,Q\right) }\varphi _{n}\right\vert \\
&=&\frac{\tau _{n}}{n}\left\vert b^{\prime }\Gamma ^{\prime }P^{\perp
}\left( g-Z_{1}\theta _{K_{1,n}}\right) \right\vert \\
&&+\frac{\tau _{n}}{n}\left\vert b^{\prime }\left( \Gamma -Z_{2}\Pi
^{K_{2,n}}\right) ^{\prime }M^{\left( Z,Q\right) }D_{\widehat{\vartheta }%
}M^{\left( Z,Q\right) }\left( g-Z_{1}\theta ^{K_{1,n}}\right) \right\vert
\end{eqnarray*}%
\begin{eqnarray*}
&\leq &\tau _{n}\sqrt{\frac{b^{\prime }\Gamma ^{\prime }\Gamma b}{n}}\sqrt{%
\frac{m_{n}}{n}}\left\Vert g\left( \cdot \right) -\theta ^{K_{1,n}\prime
}Z_{1}\left( \cdot \right) \right\Vert _{\infty } \\
&&+\tau _{n}\sqrt{\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert 
\widehat{\vartheta }_{\left( i,t\right) }\right\vert ^{2}}\left( \frac{m_{n}%
\sqrt{d}}{n}\right) \left\Vert \gamma \left( \cdot \right) -\Pi
^{K_{2,n}\prime }Z_{2}\left( \cdot \right) \right\Vert _{\infty
,d}\left\Vert g\left( \cdot \right) -\theta ^{K_{1,n}\prime }Z_{1}\left(
\cdot \right) \right\Vert _{\infty } \\
&=&O_{a.s.}\left( \frac{\tau _{n}}{K_{1,n}^{\varrho _{g}}}\right)
+O_{a.s.}\left( \frac{\tau _{n}}{nK_{2,n}^{\left( \varrho _{\gamma
}-1\right) }K_{1,n}^{\varrho _{g}}}\right) =O_{a.s.}\left( \frac{\tau _{n}}{%
K_{1,n}^{\varrho _{g}}}\right)
\end{eqnarray*}%
where $g=\left( g\left( W_{1,\left( 1,1\right) }\right) ,..,g\left(
W_{1,\left( 1,T_{1}\right) }\right) ,...,g\left( W_{1,\left( n,1\right)
}\right) ,..,g\left( W_{1,\left( n,T_{n}\right) }\right) \right) ^{\prime }$%
. Since the above argument holds for all $b\in \mathbb{R}^{d}$ such that $%
\left\Vert b\right\Vert _{2}=1$, we further deduce that $D_{\mu
}^{-1}\Upsilon _{n}^{\prime }A\varphi _{n}=O_{a.s.}\left( \tau
_{n}K_{1,n}^{-\varrho _{g}}\right) $. Next, consider $D_{\mu }^{-1}\Phi
_{n}^{\prime }A\varphi _{n}$. Again, let $b\in \mathbb{R}^{d}$ such that $%
\left\Vert b\right\Vert _{2}=1$, note that, by applying the triangle and CS
inequalities along with Assumptions 3(ii), 4(i), 5, 7(i), and 7(ii); we
obtain%
\begin{eqnarray*}
\left\vert b^{\prime }D_{\mu }^{-1}\Phi _{n}^{\prime }A\varphi
_{n}\right\vert &\leq &\frac{\tau _{n}}{n}\left\vert b^{\prime }D_{\mu
}^{-1}D_{\kappa }F^{\prime }P^{\perp }g\right\vert +\frac{\tau _{n}}{n}%
\left\vert b^{\prime }D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left(
Z,Q\right) }D_{\widehat{\vartheta }}M^{\left( Z,Q\right) }g\right\vert \\
&\leq &\frac{\tau _{n}\left( \kappa _{n}^{\max }\right) }{\left( \mu
_{n}^{\min }\right) }\frac{m_{n}\sqrt{d}}{n}\left\Vert f\left( \cdot \right)
-\Theta ^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert _{\infty
,d}\left\Vert g\left( \cdot \right) -\theta ^{K_{1,n}\prime }Z_{1}\left(
\cdot \right) \right\Vert _{\infty } \\
&&+\frac{\tau _{n}\left( \kappa _{n}^{\max }\right) }{\left( \mu _{n}^{\min
}\right) }\sqrt{\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widehat{%
\vartheta }_{\left( i,t\right) }\right\vert ^{2}}\frac{m_{n}\sqrt{d}}{n}%
\left\{ \left\Vert f\left( \cdot \right) -\Theta ^{K_{1,n}\prime
}Z_{1}\left( \cdot \right) \right\Vert _{\infty ,d}\right. \\
&&\text{ \ \ \ \ }\times \left. \left\Vert g\left( \cdot \right) -\theta
^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert _{\infty }\right\} \\
&=&O_{a.s.}\left( \frac{\tau _{n}\left( \kappa _{n}^{\max }\right) }{\left(
\mu _{n}^{\min }\right) K_{1,n}^{\varrho _{f}+\varrho _{g}}}\right)
=o_{a.s.}\left( 1\right) \text{,}
\end{eqnarray*}%
where $F=\left( f\left( W_{1,\left( 1,1\right) }\right) ,..,f\left(
W_{1,\left( 1,T_{1}\right) }\right) ,...,f\left( W_{1,\left( n,1\right)
}\right) ,..,f\left( W_{1,\left( n,T_{n}\right) }\right) \right) ^{\prime }$
and where $g$ is as defined above. Since the above argument holds for all $%
b\in \mathbb{R}^{d}$ such that $\left\Vert b\right\Vert _{2}=1$, we further
deduce that $D_{\mu }^{-1}\Phi _{n}^{\prime }A\varphi _{n}=O_{a.s.}\left(
\tau _{n}\kappa _{n}^{\max }\left( \mu _{n}^{\min }\right)
^{-1}K_{1,n}^{-\varrho _{f}-\varrho _{g}}\right) =o_{a.s.}\left( 1\right) $.
Now, consider the term $D_{\mu }^{-1}U^{\prime }A\varphi _{n}$. Let $%
u_{b}=UD_{\mu }^{-1}b$, for $b\in \mathbb{R}^{d}$ such that $\left\Vert
b\right\Vert _{2}=1$. Making use of the conditional serial independence
assumption given in Assumption 1, we deduce that $E\left[ u_{b}u_{b}^{\prime
}|\mathcal{F}_{n}^{W}\right] =E\left[ UD_{\mu }^{-1}bb^{\prime }D_{\mu
}^{-1}U^{\prime }|\mathcal{F}_{n}^{W}\right] \leq \left( \mu _{n}^{\min
}\right) ^{-2}\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert
U_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right]
I_{m_{n}}$, where we take $A\leq B$ for two square matrices $A$ and $B$ to
mean that $A-B$ is negative semi-definite, or, alternatively, $B-A$ is
positive semidefinite. Applying the above inequality and Assumptions 2(i),
3(ii), 4(ii), 5, and 7(i); we obtain%
\begin{equation*}
E\left( \left[ b^{\prime }D_{\mu }^{-1}U^{\prime }A\varphi _{n}\right] ^{2}|%
\mathcal{F}_{n}^{W}\right) =\frac{\tau _{n}^{2}}{n}\left( g-Z_{1}\theta
^{K_{1,n}}\right) ^{\prime }AE\left[ u_{b}u_{b}^{\prime }|\mathcal{F}_{n}^{W}%
\right] A\left( g-Z_{1}\theta ^{K_{1,n}}\right)
\end{equation*}%
\begin{eqnarray*}
&\leq &\frac{\tau _{n}^{2}}{n}\frac{\max_{1\leq \left( i,t\right) \leq
m_{n}}E\left[ \left\Vert U_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{%
F}_{n}^{W}\right] \left( g-Z_{1}\theta ^{K_{1,n}}\right) ^{\prime
}A^{2}\left( g-Z_{1}\theta ^{K_{1,n}}\right) }{\left( \mu _{n}^{\min
}\right) ^{2}} \\
&\leq &\frac{m_{n}C\tau _{n}^{2}}{n\left( \mu _{n}^{\min }\right) ^{2}}\left[
1+\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widehat{\vartheta }%
_{\left( i,t\right) }\right\vert ^{2}\right] \left\Vert g\left( \cdot
\right) -\theta ^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert
_{\infty }^{2} \\
&=&O_{a.s.}\left( \frac{\tau _{n}^{2}}{\left( \mu _{n}^{\min }\right)
^{2}K_{1,n}^{2\varrho _{g}}}\right)
\end{eqnarray*}%
Hence, there exists a positive constant $\overline{C}<\infty $ such that for
all $n$ sufficiently large

\noindent $E\left( \left( \mu _{n}^{\min }\right) ^{2}K_{1,n}^{2\varrho
_{g}}\tau _{n}^{-2}\left[ b^{\prime }D_{\mu }^{-1}U^{\prime }A\varphi _{n}%
\right] ^{2}\right) =E_{W_{n}}\left\{ \left( \mu _{n}^{\min }\right)
^{2}K_{1,n}^{2\varrho _{g}}\tau _{n}^{-2}E\left( \left[ b^{\prime }D_{\mu
}^{-1}U^{\prime }A\varphi _{n}\right] ^{2}|\mathcal{F}_{n}^{W}\right)
\right\} $

\noindent $\leq \overline{C}$. It follows by applying Markov's inequality
that $b^{\prime }D_{\mu }^{-1}U^{\prime }A\varphi _{n}=O_{p}\left( \tau
_{n}\left( \mu _{n}^{\min }\right) ^{-1}K_{1,n}^{-\varrho _{g}}\right) $.
\noindent Finally, it follows from these intermediate results that $D_{\mu
}^{-1}X^{\prime }A\varphi _{n}=D_{\mu }^{-1}\Upsilon _{n}^{\prime }A\varphi
_{n}+D_{\mu }^{-1}\Phi _{n}^{\prime }A\varphi _{n}+D_{\mu }^{-1}U^{\prime
}A\varphi _{n}=O_{p}\left( \tau _{n}K_{1,n}^{-\varrho _{g}}\right) $, as
required to show part (a).

Part (b) of this lemma can be proved by generalizing the argument for part
(iv) of Lemma A5 in Chao et al (2012) to our setting here with cluster
sampling, fixed effects, and possibly many covariates. For the sake of
brevity, we will not include an explicit proof here, but the details of a
proof can be obtained from the authors upon request. $\square $

\bigskip

\noindent \textbf{Lemma S2-5: }Under Assumptions 1-7, the following results
hold: (a) $D_{\mu }^{-1}X^{\prime }M^{\left( Z_{1},Q\right) }\varphi
_{n}=O_{p}\left( \tau _{n}/K_{1,n}^{\varrho _{g}}\right) $; (b) $D_{\mu
}^{-1}X^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon =O_{p}\left( n/\mu
_{n}^{\min }\right) $.

\noindent

\noindent \textbf{Proof of Lemma S2-5: }Note first that part (a) can be
shown in a way similar to the proof of part (a) of Lemma S2-4 above. Hence,
for the sake of brevity, we will not include an explicit proof here, but the
details of a proof can be obtained from the authors upon request.

To show part (b), let $b\in \mathbb{R}^{d}$ such that $\left\Vert
b\right\Vert _{2}=1$ and let $u_{b}=UD_{\mu }^{-1}b$, $u_{b,\left(
i,t\right) }=U_{\left( i,t\right) }^{\prime }D_{\mu }^{-1}b$, and $F=\left(
f\left( W_{1,\left( 1,1\right) }\right) ,..,f\left( W_{1,\left(
1,T_{1}\right) }\right) ,...,f\left( W_{1,\left( n,1\right) }\right)
,..,f\left( W_{1,\left( n,T_{n}\right) }\right) \right) ^{\prime }$. Now,
write $b^{\prime }D_{\mu }^{-1}X^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon =b^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}+b^{\prime }D_{\mu }^{-1}D_{\kappa }F^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+u_{b}^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon $. By straightforward calculations and by
applying Assumptions 1, 2(i), 3, 4, 5(i), 5(iii), and 7(ii) as well as the
CS and Markov's inequalities, we can show that $b^{\prime }\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}=O_{p}\left( 1\right) $ and $%
b^{\prime }D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}=O_{p}\left( \kappa _{n}^{\max }\left( \mu _{n}^{\min
}\right) ^{-1}K_{1,n}^{-\varrho _{f}}\right) $. From the CS inequality, we
also obtain $E\left[ \left\vert u_{b}^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon \right\vert |\mathcal{F}_{n}^{W}\right] \leq \sqrt{E\left[
u_{b}^{\prime }M^{\left( Z_{1},Q\right) }u_{b}|\mathcal{F}_{n}^{W}\right] }%
\sqrt{E\left[ \varepsilon ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon |%
\mathcal{F}_{n}^{W}\right] }$. Next, note that, by applying Assumptions 2(i)
and 6(ii), we have $E\left[ \varepsilon ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon |\mathcal{F}_{n}^{W}\right] \leq E\left[ \varepsilon ^{\prime
}\varepsilon |\mathcal{F}_{n}^{W}\right] \leq n\overline{T}\left(
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) =O_{a.s.}\left( n\right) 
$. Similarly, by applying Assumptions 2(i), 3(ii), and 6(ii); we obtain $E%
\left[ u_{b}^{\prime }M^{\left( Z_{1},Q\right) }u_{b}|\mathcal{F}_{n}^{W}%
\right] =E\left[ b^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right)
}UD_{\mu }^{-1}b|\mathcal{F}_{n}^{W}\right] $

\noindent $\leq \overline{T}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E%
\left[ \left\Vert U_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}%
_{n}^{W}\right] \right) \left[ n/\left( \mu _{n}^{\min }\right) ^{2}\right]
=O_{a.s.}\left( n\left( \mu _{n}^{\min }\right) ^{-2}\right) $. It follows
from these results that $E\left[ \left\vert u_{b}^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon \right\vert |\mathcal{F}_{n}^{W}\right]
=O_{a.s.}\left( n\left( \mu _{n}^{\min }\right) ^{-1}\right) $. Hence, by
the law of iterated expectations and Theorem 16.1 of Billingsley (1995),
there exists a constant $\overline{C}<\infty $ such that for all $n$
sufficiently large $E\left[ \left( \left( \mu _{n}^{\min }\right) /n\right)
\left\vert u_{b}^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon \right\vert %
\right] =E_{W}\left( \left( \left( \mu _{n}^{\min }\right) /n\right) E\left[
\left\vert u_{b}^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon \right\vert |%
\mathcal{F}_{n}^{W}\right] \right) $

\noindent $\leq \overline{C}$. It follows by applying Markov's inequality
that $b^{\prime }D_{\mu }^{-1}U^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon =O_{p}\left( n\left( \mu _{n}^{\min }\right) ^{-1}\right) $.
Putting everything together, we have%
\begin{eqnarray*}
b^{\prime }D_{\mu }^{-1}X^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon &=&%
\frac{b^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{%
\sqrt{n}}+\frac{b^{\prime }D_{\mu }^{-1}D_{\kappa }F^{\prime }M^{\left(
Z_{1},Q\right) }\varepsilon }{\sqrt{n}}+b^{\prime }D_{\mu }^{-1}U^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon \\
&=&O_{p}\left( 1\right) +O_{p}\left( \frac{\kappa _{n}^{\max }}{\left( \mu
_{n}^{\min }\right) K_{1,n}^{\varrho _{f}}}\right) +O_{p}\left( \frac{n}{%
\left( \mu _{n}^{\min }\right) }\right) =O_{p}\left( \frac{n}{\left( \mu
_{n}^{\min }\right) }\right)
\end{eqnarray*}

\noindent Since the above argument holds for all $b\in \mathbb{R}^{d}$ such
that $\left\Vert b\right\Vert _{2}=1$, we further deduce that $D_{\mu
}^{-1}X^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon =O_{p}\left( n\left(
\mu _{n}^{\min }\right) ^{-1}\right) $. $\square $

\bigskip

\noindent \textbf{Lemma S2-6: }Suppose that Assumptions 2 and 8 hold. For $%
1\leq p\leq 8$ and for all $n$, there exists a positive constant $C$ such
that $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{%
U}_{\left( i,t\right) }\right\Vert _{2}^{p}|\mathcal{F}_{n}^{W}\right] \leq
C<\infty \ a.s.$, where $\underline{U}_{\left( i,t\right) }=U_{\left(
i,t\right) }-\rho \varepsilon _{\left( i,t\right) }$.

\noindent \textbf{Proof of Lemma S2-6: }Note that, for $1\leq p\leq 8$ and
for any $\left( i,t\right) \in \left\{ 1,....,m_{n}\right\} $, there exists
a positive constant $C$ such that%
\begin{eqnarray*}
E\left[ \left\Vert \underline{U}_{\left( i,t\right) }\right\Vert _{2}^{p}|%
\mathcal{F}_{n}^{W}\right] &=&E\left[ \left( \left\Vert U_{\left( i,t\right)
}-\rho \varepsilon _{\left( i,t\right) }\right\Vert _{2}\right) ^{p}|%
\mathcal{F}_{n}^{W}\right] \\
&\leq &2^{p-1}\left\{ \left( E\left[ \left\Vert U_{\left( i,t\right)
}\right\Vert _{2}^{8}|\mathcal{F}_{n}^{W}\right] \right) ^{p/8}+\left\Vert
\rho \right\Vert _{2}^{p}\left( E\left[ \left\vert \varepsilon _{\left(
i,t\right) }\right\vert ^{8}|\mathcal{F}_{n}^{W}\right] \right)
^{p/8}\right\} \\
&\leq &C<\infty \text{ \ }a.s.\text{,}
\end{eqnarray*}%
where the first inequality follows by applying the triangle inequality, Lo%
\`{e}ve's $c_{r}$ inequality, and Liapunov's inequality in sequence and
where the second inequality follows from applying Assumption 2(i) and and
from the fact that $\rho \in \mathcal{S}_{\rho }$, some compact subset of $%
\mathbb{R}^{d}$ as stated in Assumption 8. Since the upper bound above holds
for all $\left( i,t\right) \in \left\{ 1,....,m_{n}\right\} $, for all $\rho
\in \mathcal{S}_{\rho }$, and for all $n$, it further follows that $%
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}%
_{\left( i,t\right) }\right\Vert _{2}^{p}|\mathcal{F}_{n}^{W}\right] \leq
C<\infty $ \ $a.s.$, as required. $\square $

\medskip

\noindent \textbf{Lemma S2-7: }Under Assumptions 1-7, the following results
hold: (a) $\widehat{\ell }_{L,n}=o_{p}\left( \left[ \mu _{n}^{\min }\right]
^{2}/n\right) $; (b) $\widehat{\ell }_{F,n}=o_{p}\left( \left[ \mu
_{n}^{\min }\right] ^{2}/n\right) $.

\medskip

\noindent \textbf{Proof of Lemma S2-7: }To proceed, first define%
\begin{equation*}
\overline{D}_{n}=\left( 
\begin{array}{cc}
\mu _{n}^{\min } & 0 \\ 
0 & D_{\mu }%
\end{array}%
\right) \text{ and }L_{\delta }=\left( 
\begin{array}{cc}
1 & 0 \\ 
\delta _{0} & I_{d}%
\end{array}%
\right) \text{,}
\end{equation*}%
and note that%
\begin{equation*}
L_{\delta }^{-1}=\left( 
\begin{array}{cc}
1 & 0 \\ 
-\delta _{0} & I_{d}%
\end{array}%
\right)
\end{equation*}%
Now, for any $\beta \in \mathbb{R}^{d+1}$ such that $\left\Vert \beta
\right\Vert _{2}=1$ and for $\overline{X}=\left[ 
\begin{array}{cc}
y & X%
\end{array}%
\right] $, we can write $\beta ^{\prime }\overline{X}^{\prime }A\overline{X}%
\beta =$

\noindent $\beta ^{\prime }L_{\delta }^{\prime }\overline{D}_{n}\left( 
\overline{D}_{n}^{-1}L_{\delta }^{\prime -1}\overline{X}^{\prime }A\overline{%
X}L_{\delta }^{-1}\overline{D}_{n}^{-1}\right) \overline{D}_{n}L_{\delta
}\beta $. Moreover, by direct multiplication,%
\begin{equation*}
\overline{D}_{n}^{-1}L_{\delta }^{\prime -1}\overline{X}^{\prime }A\overline{%
X}L_{\delta }^{-1}\overline{D}_{n}^{-1}=\left( 
\begin{array}{cc}
\left( y-X\delta _{0}\right) ^{\prime }A\left( y-X\delta _{0}\right) /\left(
\mu _{n}^{\min }\right) ^{2} & \left( y-X\delta _{0}\right) ^{\prime
}AXD_{\mu }^{-1}/\left( \mu _{n}^{\min }\right) \\ 
D_{\mu }^{-1}X^{\prime }A\left( y-X\delta _{0}\right) /\left( \mu _{n}^{\min
}\right) & D_{\mu }^{-1}X^{\prime }AXD_{\mu }^{-1}%
\end{array}%
\right) \text{,}
\end{equation*}%
Now, by straightforward but tedious calculations and by applying Assumptions
1, 2(i), 3(ii)-(iii), 4, 5, 6(i), and 7(i)-(iii) as well as Lemmas S2-3(c)
and S2-1(a); we can show that, under the rate condition $\sqrt{K_{2}}/\left(
\mu _{n}^{\min }\right) ^{2}\rightarrow 0$,%
\begin{equation*}
\overline{D}_{n}^{-1}L_{\delta }^{\prime -1}\overline{X}^{\prime }A\overline{%
X}L_{\delta }^{-1}\overline{D}_{n}^{-1}=\left( 
\begin{array}{cc}
0 & 0 \\ 
0 & \Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n%
\end{array}%
\right) +o_{p}\left( 1\right) \text{\footnote{%
Details of the calculations leading to this result can be obtained from the
authors upon request.},}
\end{equation*}%
where, in light of Assumption 3(iii), $\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }\Gamma /n=O_{p}\left( 1\right) $ and $\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma /n$ is positive definite for all $n$ large
sufficiently large. It follows that $\overline{D}_{n}^{-1}L_{\delta
}^{\prime -1}\overline{X}^{\prime }A\overline{X}L_{\delta }^{-1}\overline{D}%
_{n}^{-1}$ is positive semidefinite w.p.a.1, so that $\beta ^{\prime }%
\overline{X}^{\prime }A\overline{X}\beta =\beta ^{\prime }L_{\delta
}^{\prime }\overline{D}_{n}\left( \overline{D}_{n}^{-1}L_{\delta }^{\prime
-1}\overline{X}^{\prime }A\overline{X}L_{\delta }^{-1}\overline{D}%
_{n}^{-1}\right) \overline{D}_{n}L_{\delta }\beta \geq 0$ \ w.p.a.1 for all $%
\beta \in \mathbb{R}^{d+1}$ such that $\left\Vert \beta \right\Vert _{2}=1$.
Moreover, by straightforward calculations, we can show that%
\begin{equation*}
\frac{\beta ^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }%
\overline{X}\beta }{n}=\frac{\beta ^{\prime }L_{\delta ,2}^{\prime }D_{\mu
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma D_{\mu }L_{\delta
,2}\beta }{n^{2}}+\beta ^{\prime }L_{\delta }^{\prime }E\left[ \frac{%
V^{\prime }M^{Q}V}{n}\right] L_{\delta }\beta +o_{p}\left( 1\right) \text{,}
\end{equation*}%
where $V=\left[ 
\begin{array}{cc}
\varepsilon & U%
\end{array}%
\right] $ and $L_{\delta ,2}=\left[ 
\begin{array}{cc}
\delta _{0} & I_{d}%
\end{array}%
\right] $ and where $E\left[ V^{\prime }M^{Q}V/n\right] $ is positive
definite for all $n$ sufficiently large in light of Assumptions 2(ii) and
6(i). Since $L_{\delta }\beta \neq 0$ for all $\beta \in \mathbb{R}^{d+1}$
such that $\left\Vert \beta \right\Vert _{2}=1$, it follows that $\beta
^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\beta
/n>0$ \ w.p.a.1 for all $\beta \in \mathbb{R}^{d+1}$ such that $\left\Vert
\beta \right\Vert _{2}=1$. Hence, with probability approaching one as $%
n\rightarrow \infty $,%
\begin{equation*}
R\left( \beta \right) =\frac{\beta ^{\prime }\overline{X}^{\prime }A%
\overline{X}\beta }{\beta ^{\prime }\overline{X}^{\prime }M^{\left(
Z_{1},Q\right) }\overline{X}\beta }
\end{equation*}%
is a continuous function of $\beta $ for all values of $\beta $ such that $%
\left\Vert \beta \right\Vert _{2}=1$. The Weierstrass extreme value theorem
then implies that there exists some $\widetilde{\beta }$ such that $%
\widetilde{\beta }=\arg \min_{\left\Vert \beta \right\Vert _{2}=1}R\left(
\beta \right) $ w.p.a.1. Next, note that $\widehat{\ell }_{L,n}$ is the
smallest root of the determinantal equation

\noindent $\det \left\{ \overline{X}^{\prime }A\overline{X}-\ell \overline{X}%
^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\right\} =0$; and, thus, $%
\widehat{\ell }_{L,n}$ has the representation 
\begin{equation*}
\widehat{\ell }_{L,n}=R\left( \widetilde{\beta }\right) =\frac{\widetilde{%
\beta }^{\prime }\overline{X}^{\prime }A\overline{X}\widetilde{\beta }}{%
\widetilde{\beta }^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }%
\overline{X}\widetilde{\beta }}=\min_{\left\Vert \beta \right\Vert
_{2}=1}\left( \frac{\beta ^{\prime }\overline{X}^{\prime }A\overline{X}\beta 
}{\beta ^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X%
}\beta }\right) .
\end{equation*}%
Now, let $\delta _{\ast }=\left( 
\begin{array}{cc}
1 & -\delta _{0}^{\prime }%
\end{array}%
\right) ^{\prime }/\left\Vert \left( 
\begin{array}{cc}
1 & -\delta _{0}^{\prime }%
\end{array}%
\right) ^{\prime }\right\Vert _{2}$; and we have, with probability
approaching one as $n\rightarrow \infty $, 
\begin{eqnarray*}
0 &\leq &\widehat{\ell }_{L,n}=\min_{\left\Vert \beta \right\Vert
_{2}=1}\left( \frac{\beta ^{\prime }\overline{X}^{\prime }A\overline{X}\beta 
}{\beta ^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X%
}\beta }\right) \\
&\leq &\frac{\delta _{\ast }^{\prime }\overline{X}^{\prime }A\overline{X}%
\delta _{\ast }}{\delta _{\ast }^{\prime }\overline{X}^{\prime }M^{\left(
Z_{1},Q\right) }\overline{X}\delta _{\ast }} \\
&=&\frac{\left( \mu _{n}^{\min }\right) ^{2}}{n}\left\{ \frac{\left(
y-X\delta _{0}\right) ^{\prime }A\left( y-X\delta _{0}\right) /\sqrt{K_{2,n}}%
}{\left( y-X\delta _{0}\right) M^{\left( Z_{1},Q\right) }\left( y-X\delta
_{0}\right) /n}\right\} \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right)
^{2}} \\
&=&O\left( \frac{\left[ \mu _{n}^{\min }\right] ^{2}}{n}\right) O_{p}\left(
1\right) O_{p}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right)
^{2}}\right) =o_{p}\left( \frac{\left[ \mu _{n}^{\min }\right] ^{2}}{n}%
\right) \text{,}
\end{eqnarray*}%
given the rate condition $\sqrt{K_{2}}/\left( \mu _{n}^{\min }\right)
^{2}\rightarrow 0$. This shows part (a).

For part (b), we use the result in part (a) above and the fact that $%
m_{n}/n\sim 1$ by Assumption 5(i) to obtain%
\begin{equation}
\widehat{\ell }_{F,n}=\frac{\widehat{\ell }_{L,n}-\left( 1-\widehat{\ell }%
_{L,n}\right) C/m_{n}}{1-\left( 1-\widehat{\ell }_{L,n}\right) C/m_{n}}=%
\left[ \widehat{\ell }_{L,n}+O_{p}\left( \frac{1}{n}\right) \right] \left[
1+O_{p}\left( \frac{1}{n}\right) \right] =o_{p}\left( \frac{\left[ \mu
_{n}^{\min }\right] ^{2}}{n}\right) \text{. }\square \text{\textit{\ }}
\label{asy equiv of F and L}
\end{equation}

\medskip

\noindent \textbf{Lemma S2-8: }Let $A$ be as defined above. Suppose that%
{\small \ }i){\small \ }$\left( u_{\left( 1,1\right) ,n},\varepsilon
_{\left( 1,1\right) }\right) ,...,\left( u_{\left( 1,T_{1}\right)
,n},\varepsilon _{\left( 1,T_{1}\right) }\right) ,$

\noindent $\left( u_{\left( 2,1\right) ,n},\varepsilon _{\left( 2,1\right)
,n}\right) ,...,\left( u_{\left( 2,T_{2}\right) ,n},\varepsilon _{\left(
2,T_{2}\right) ,n}\right) ,...,\left( u_{\left( n,1\right) ,n},\varepsilon
_{\left( n,1\right) ,n}\right) ....,\left( u_{\left( n,T_{n}\right)
,n},\varepsilon _{\left( n,T_{n}\right) ,n}\right) ${\small \ }are
independent conditional on $\mathcal{F}_{n}^{W}=\sigma \left( W_{n}\right) $%
; ii)\ there exists a constant $C$\ such that, almost surely for all $n$
sufficiently large, $\max_{1\leq \left( i,t\right) \leq m_{n}}E\left(
u_{\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right) \leq C$, $%
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left( \varepsilon _{\left(
i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right) \leq C$, and $\max_{1\leq
\left( i,t\right) \leq m_{n}}\left\vert \phi _{\left( i,t\right)
,n}\right\vert \leq C$. In addition, define $\overline{\psi }_{\left(
j,s\right) ,n}=E\left[ u_{\left( j,s\right) ,n}\varepsilon _{\left(
j,s\right) ,n}|\mathcal{F}_{n}^{W}\right] $ for $\left( j,s\right)
=1,...,m_{n}$. Then, under Assumptions 5 and 6, the following statements are
true: (a) $K_{2,n}^{-1}\dsum\nolimits_{1\leq \left( j,s\right) <\left(
i,t\right) \leq m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\phi
_{\left( i,t\right) ,n}\left\{ u_{\left( j,s\right) ,n}\varepsilon _{\left(
j,s\right) ,n}-\overline{\psi }_{\left( j,s\right) ,n}\right\} \overset{p}{%
\rightarrow }0$;

\noindent (b) $K_{2,n}^{-1}\dsum\nolimits_{1\leq \text{ }\left( k,\upsilon
\right) <\left( j,s\right) <\left( i,t\right) \leq \text{ }m_{n}}A_{\left(
i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left( k,\upsilon
\right) }\phi _{\left( i,t\right) ,n}\left\{ u_{\left( j,s\right)
,n}\varepsilon _{\left( k,\upsilon \right) ,n}+\varepsilon _{\left(
j,s\right) ,n}u_{\left( k,\upsilon \right) ,n}\right\} \overset{p}{%
\rightarrow }0$;

\noindent (c) $K_{2,n}^{-1}\dsum\nolimits_{1\leq \text{ }\left( k,\upsilon
\right) <\left( j,s\right) <\left( i,t\right) \leq \text{ }m_{n}}A_{\left(
i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left( k,\upsilon
\right) }\phi _{\left( i,t\right) ,n}\varepsilon _{\left( j,s\right)
,n}\varepsilon _{\left( k,\upsilon \right) ,n}\overset{p}{\rightarrow }0$;

\noindent (d) $K_{2,n}^{-1}\dsum\nolimits_{1\leq \text{ }\left( k,\upsilon
\right) <\left( j,s\right) <\left( i,t\right) \leq \text{ }m_{n}}A_{\left(
i,t\right) ,\left( j,s\right) }A_{\left( i,t\right) ,\left( k,\upsilon
\right) }\phi _{\left( i,t\right) ,n}u_{\left( j,s\right) ,n}u_{\left(
k,\upsilon \right) ,n}\overset{p}{\rightarrow }0$.

{\small \noindent }\noindent \noindent

\noindent \textbf{Proof of Lemma S2-8: }This lemma can be proved by
generalizing the argument for parts (i), (iv), (v), and (vi) of Lemma B4 of
Chao et al (2012) to our setting here with cluster sampling, fixed effects,
and possibly many covariates. For the sake of brevity, we will not include
an explicit proof here, but the details of a proof can be obtained from the
authors upon request. $\square $

\medskip

\noindent \textbf{Lemma S2-9: }Let 
\begin{equation*}
\widehat{\Delta }\left( \delta _{0}\right) =-\frac{\left( y-X\delta
_{0}\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta _{0}\right) 
}{2}\left. \frac{\partial }{\partial \delta }\left\{ \frac{\left( y-X\delta
\right) ^{\prime }A\left( y-X\delta \right) }{\left( y-X\delta \right)
^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) }\right\}
\right\vert _{\delta =\delta _{0}}\text{.}
\end{equation*}%
Suppose that Assumptions 1-8 hold; then, $D_{\mu }^{-1}\widehat{\Delta }%
\left( \delta _{0}\right) =\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}\varepsilon /\sqrt{n}+D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon
+o_{p}\left( 1\right) $, where $\underline{U}=U-\varepsilon \rho ^{\prime }$
and where $\rho =\lim_{n\rightarrow \infty }E\left[ U^{\prime
}M^{Q}\varepsilon \right] /E\left[ \varepsilon ^{\prime }M^{Q}\varepsilon %
\right] $.

\bigskip

\noindent \textbf{Proof of Lemma S2-9: }This lemma can be proved by
generalizing the argument of Lemma A8 of Hausman et al (2012) to our setting
here with cluster sampling, fixed effects, and possibly many covariates. For
the sake of brevity, we will not include an explicit proof here, but the
details of a proof can be obtained from the authors upon request.

\bigskip

\noindent \textbf{Lemma S2-10: }Suppose that Assumptions 1-7 are satisfied.
Let $\overline{\delta }_{n}$ be any estimator such that, as $n\rightarrow
\infty $, $D_{\mu }\left( \overline{\delta }_{n}-\delta _{0}\right) /\mu
_{n}^{\min }=o_{p}\left( 1\right) $. Then, $-D_{\mu }^{-1}\left( \partial 
\widehat{\Delta }\left( \overline{\delta }_{n}\right) /\partial \delta
^{\prime }\right) D_{\mu }^{-1}=H_{n}+o_{p}\left( 1\right) $, where $%
H_{n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n$ and where

\noindent $\widehat{\Delta }\left( \delta \right) =-\left[ \left( y-X\delta
\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) /2%
\right] \left[ \partial \widehat{Q}_{FELIM}\left( \delta \right) /\partial
\delta \right] $

\noindent $=X^{\prime }A\left( y-X\delta \right) -\widehat{\ell }\left(
\delta \right) X^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) 
$, with

\noindent $\widehat{\ell }\left( \delta \right) =\left( y-X\delta \right)
^{\prime }A\left( y-X\delta \right) /\left[ \left( y-X\delta \right)
^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) \right] $. In
addition, we also have%
\begin{equation}
D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }\left( \overline{\delta }%
_{n}\right) M^{\left( Z_{1},Q\right) }\right] XD_{\mu
}^{-1}=H_{n}+o_{p}\left( 1\right) \text{.}  \label{LIM denom term}
\end{equation}%
\textbf{Proof of Lemma S2-10: }This lemma can be proved by generalizing the
argument of Lemma A7 of Hausman et al (2012) to our setting here with
cluster sampling, fixed effects, and possibly many covariates. For the sake
of brevity, we will not include an explicit proof here, but the details of a
proof can be obtained from the authors upon request. $\square $

\medskip

\noindent \textbf{Lemma S2-11: }Let $\widehat{\ell }_{L}=Q\left( \widetilde{%
\beta }\right) =\min_{\beta \in \overline{B}}Q\left( \beta \right) $, where $%
Q\left( \beta \right) $ is as defined in Assumption 9. Then, $\widehat{\ell }%
_{L}$ is also the smallest root of the determinantal equation $\det \left[ 
\overline{X}^{\prime }A\overline{X}-\ell \overline{X}^{\prime }M^{\left(
Z_{1},Q\right) }\overline{X}\right] =0$, where $\overline{X}=\left[ y,X%
\right] $. Suppose in addition that condition (11) in Assumption 9 is
satisfied; then, $\widehat{\ell }_{L}$ has the representation 
\begin{equation}
\widehat{\ell }_{L}=\frac{\left( y-X\widehat{\delta }_{L}\right) ^{\prime
}A\left( y-X\widehat{\delta }_{L}\right) }{\left( y-X\widehat{\delta }%
_{L}\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widehat{\delta }%
_{L}\right) }\text{,}  \label{FELIM root}
\end{equation}%
where $\widehat{\delta }_{L}$ denotes the FELIM\ estimator. Moreover, $%
\overline{X}^{\prime }A\left( y-X\widehat{\delta }_{L}\right) -\widehat{\ell 
}_{L}\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widehat{%
\delta }_{L}\right) $

\noindent $=0$. In particular, this implies that $\widehat{\Delta }\left( 
\widehat{\delta }_{L}\right) =0$, where

\noindent $\widehat{\Delta }\left( \delta \right) =-\left[ \left( y-X\delta
\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta \right) /2%
\right] \left( \partial \widehat{Q}_{FELIM}\left( \delta \right) /\partial
\delta \right) $, so that $\widehat{\delta }_{L}$ satisfies the set of
(normalized) first-order conditions for minimizing the variance ratio
objective function $\widehat{Q}_{FELIM}\left( \delta \right) =\left(
y-X\delta \right) ^{\prime }A\left( y-X\delta \right) /\left[ \left(
y-X\delta \right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta
\right) \right] $.

\bigskip

\noindent \textbf{Proof of Lemma S2-11:}

Note that the first-order condition for minimizing the objective function $%
Q\left( \beta \right) $ can be written as $\partial Q\left( \widetilde{\beta 
}\right) /\partial \beta =2\overline{X}^{\prime }A\overline{X}\widetilde{%
\beta }/\left( \widetilde{\beta }^{\prime }\overline{X}^{\prime }M^{\left(
Z_{1},Q\right) }\overline{X}\widetilde{\beta }\right) $

\noindent $-\widetilde{\beta }^{\prime }\overline{X}^{\prime }A\overline{X}%
\widetilde{\beta }\left( 2\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }%
\overline{X}\widetilde{\beta }\right) /\left[ \widetilde{\beta }^{\prime }%
\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\widetilde{\beta 
}\right] ^{2}=0$. Pre-multiplying this first order condition by the factor $%
\frac{1}{2}\widetilde{\beta }^{\prime }\overline{X}^{\prime }M^{\left(
Z_{1},Q\right) }\overline{X}\widetilde{\beta }$, we then obtain%
\begin{equation}
0=\left[ \overline{X}^{\prime }A\overline{X}-\widehat{\ell }_{L}\overline{X}%
^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\right] \widetilde{\beta }
\label{normalized FOC}
\end{equation}%
where $\widehat{\ell }_{L}=Q\left( \widetilde{\beta }\right) =\widetilde{%
\beta }^{\prime }\overline{X}^{\prime }A\overline{X}\widetilde{\beta }/%
\widetilde{\beta }^{\prime }\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }%
\overline{X}\widetilde{\beta }$. It is clear that in order for there to be a
nontrivial solution, i.e., $\widetilde{\beta }\neq 0$ such that equation (%
\ref{normalized FOC}) is true, $\widehat{\ell }_{L}$ must be a root of the
determinantal equation $\det \left[ \overline{X}^{\prime }A\overline{X}-\ell 
\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\right] =0.$
Moreover, since our goal is to minimize the value of the objective function $%
Q\left( \beta \right) $, this implies that we should choose $\widehat{\ell }%
_{L}$ to be the smallest root of this determinantal equation. Now, define $%
\widetilde{\delta }=-\widetilde{\beta }_{2}/\widetilde{\beta }_{1}$, and
rewrite the first-order conditions given by expression (\ref{normalized FOC}%
) as $0=\overline{X}^{\prime }A\overline{X}\widetilde{\beta }-\widehat{\ell }%
_{L}\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\widetilde{%
\beta }=\widetilde{\beta }_{1}\left\{ \overline{X}^{\prime }A\left( y-X%
\widetilde{\delta }\right) -\widehat{\ell }_{L}\overline{X}^{\prime
}M^{\left( Z_{1},Q\right) }\left( y-X\widetilde{\delta }\right) \right\} $,
so that, given the condition that $\left\vert \widetilde{\beta }%
_{1}\right\vert \geq \underline{C}>0$ \ $a.s.n.$ for some constant $%
\underline{C}$ (as stated in Assumption 9), we must have 
\begin{equation}
\overline{X}^{\prime }A\left( y-X\widetilde{\delta }\right) -\widehat{\ell }%
_{L}\overline{X}^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widetilde{%
\delta }\right) =0  \label{FOC2}
\end{equation}%
Since $\overline{X}=\left[ y,X\right] $, we can partition (\ref{FOC2}) into
two sets of equations%
\begin{eqnarray}
0 &=&y^{\prime }A\left( y-X\widetilde{\delta }\right) -\widehat{\ell }%
_{L}y^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widetilde{\delta }%
\right) ,  \label{FOC2_1} \\
0 &=&X^{\prime }A\left( y-X\widetilde{\delta }\right) -\widehat{\ell }%
_{L}X^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widetilde{\delta }%
\right) \text{.}  \label{FOC2_2}
\end{eqnarray}%
Solving (\ref{FOC2_2}) for $\widetilde{\delta }$, we obtain $\widetilde{%
\delta }=\left( X^{\prime }\left[ A-\widehat{\ell }_{L}M^{\left(
Z_{1},Q\right) }\right] X\right) ^{-1}X^{\prime }\left[ A-\widehat{\ell }%
_{L}M^{\left( Z_{1},Q\right) }\right] y=\widehat{\delta }_{L}$, so that the
FELIM estimator $\widehat{\delta }_{L}$ is a solution to the second set of
equations given by (\ref{FOC2_2}). In addition , note that, under condition
(11) in Assumption 9, we have%
\begin{equation*}
\widehat{\ell }_{L}=\frac{\widetilde{\beta }^{\prime }\overline{X}^{\prime }A%
\overline{X}\widetilde{\beta }}{\widetilde{\beta }^{\prime }\overline{X}%
^{\prime }M^{\left( Z_{1},Q\right) }\overline{X}\widetilde{\beta }}=\frac{%
\widetilde{\beta }_{1}\left( y-X\widetilde{\delta }\right) ^{\prime }A\left(
y-X\widetilde{\delta }\right) \widetilde{\beta }_{1}}{\widetilde{\beta }%
_{1}\left( y-X\widetilde{\delta }\right) M^{\left( Z_{1},Q\right) }\left( y-X%
\widehat{\delta }\right) \widetilde{\beta }_{1}}=\frac{\left( y-X\widehat{%
\delta }_{L}\right) ^{\prime }A\left( y-X\widehat{\delta }_{L}\right) }{%
\left( y-X\widehat{\delta }_{L}\right) ^{\prime }M^{\left( Z_{1},Q\right)
}\left( y-X\widehat{\delta }_{L}\right) }\text{.}
\end{equation*}%
which shows (\ref{FELIM root}). Furthermore, note that $\widehat{\delta }%
_{L} $ also satisfies equation (\ref{FOC2_1}) since%
\begin{eqnarray*}
&&y^{\prime }A\left( y-X\widehat{\delta }_{L}\right) -\widehat{\ell }%
_{L}y^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widehat{\delta }%
_{L}\right) \\
&=&\left( y-X\widehat{\delta }_{L}\right) ^{\prime }\left[ A-\widehat{\ell }%
_{L}M^{\left( Z_{1},Q\right) }\right] \left( y-X\widehat{\delta }_{L}\right)
+\widehat{\delta }_{L}^{\prime }X^{\prime }\left[ A-\widehat{\ell }%
_{L}M^{\left( Z_{1},Q\right) }\right] \left( y-X\widehat{\delta }_{L}\right)
\\
&=&\left( y-X\widehat{\delta }_{L}\right) ^{\prime }A\left( y-X\widehat{%
\delta }_{L}\right) -\frac{\left( y-X\widehat{\delta }_{L}\right) ^{\prime
}A\left( y-X\widehat{\delta }_{L}\right) \left( y-X\widehat{\delta }%
_{L}\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widehat{\delta }%
_{L}\right) }{\left( y-X\widehat{\delta }_{L}\right) M^{\left(
Z_{1},Q\right) }\left( y-X\widehat{\delta }_{L}\right) } \\
&&+\widehat{\delta }_{L}^{\prime }X^{\prime }\left[ A-\widehat{\ell }%
_{L}M^{\left( Z_{1},Q\right) }\right] y \\
&&-\widehat{\delta }_{L}^{\prime }X^{\prime }\left[ A-\widehat{\ell }%
_{L}M^{\left( Z_{1},Q\right) }\right] X\left( X^{\prime }\left[ A-\widehat{%
\ell }_{L}M^{\left( Z_{1},Q\right) }\right] X\right) ^{-1}X^{\prime }\left[
A-\widehat{\ell }_{L}M^{\left( Z_{1},Q\right) }\right] y \\
&=&0
\end{eqnarray*}%
from which we further deduce that $\widehat{\delta }_{L}$ is a solution of
the complete set of first-order conditions given by (\ref{FOC2}). Finally,
since $\widehat{\Delta }\left( \widehat{\delta }_{L}\right) =X^{\prime
}A\left( y-X\widehat{\delta }_{L}\right) -\widehat{\ell }_{L}X^{\prime
}M^{\left( Z_{1},Q\right) }\left( y-X\widehat{\delta }_{L}\right) $, the
fact that $\widehat{\delta }_{L}$ is a solution of (\ref{FOC2_2}) directly
imply that $\widehat{\delta }_{L}$ satisfies the set of (normalized)
first-order conditions for minimizing the variance ratio objective function. 
$\square $

\bigskip

\noindent \textbf{Lemma S2-12: }Suppose that Assumptions 1-7 are satisfied.
Then,

\noindent $D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }_{F,n}M^{\left(
Z_{1},Q\right) }\right] XD_{\mu }^{-1}=H_{n}+o_{p}\left( 1\right) $, where $%
H_{n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\Gamma /n$,

\noindent $\widehat{\ell }_{F,n}=\left[ \widehat{\ell }_{L,n}-\left( 1-%
\widehat{\ell }_{L,n}\right) \left( C/m_{n}\right) \right] /\left[ 1-\left(
1-\widehat{\ell }_{L,n}\right) \left( C/m_{n}\right) \right] $, and $%
\widehat{\ell }_{L,n}$ is smallest root of the determinantal equation $\det
\left\{ \overline{X}^{\prime }A\overline{X}-\ell \overline{X}^{\prime
}M^{\left( Z_{1},Q\right) }\overline{X}\right\} =0$, with $\overline{X}=%
\left[ 
\begin{array}{cc}
y & X%
\end{array}%
\right] $.

\medskip

\noindent \textbf{Proof of Lemma S2-12: }The result follows directly from
applying part (b) of Lemma S2-7 and parts (a) and (b) of Lemma S2-2. $%
\square $

\bigskip

\noindent \noindent \textbf{Lemma S2-13: }Suppose that Assumptions 1-8 hold.
Then, $D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }_{F,n}M^{\left(
Z_{1},Q\right) }\right] \left( y-X\delta _{0}\right) =\mathcal{Y}_{n}\left[
1+o_{p}\left( 1\right) \right] $, where $\mathcal{Y}_{n}=\Gamma ^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /\sqrt{n}+D_{\mu }^{-1}\underline{U}%
^{\prime }A\varepsilon $ with $\underline{U}=U-\varepsilon \rho ^{\prime }$
and

\noindent $\rho =\lim_{n\rightarrow \infty }E\left[ U^{\prime
}M^{Q}\varepsilon \right] /E\left[ \varepsilon ^{\prime }M^{Q}\varepsilon %
\right] $.

\bigskip

\noindent \textbf{Proof of Lemma S2-13:}

Note that, by Lemma S2-11 above, $\widehat{\ell }_{L,n}$ has the
representation%
\begin{equation*}
\widehat{\ell }_{L,n}=\frac{\left( y-X\widehat{\delta }_{L,n}\right)
^{\prime }A\left( y-X\widehat{\delta }_{L,n}\right) }{\left( y-X\widehat{%
\delta }_{L,n}\right) ^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\widehat{%
\delta }_{L,n}\right) }\text{.}
\end{equation*}%
Next, from expression (\ref{asy equiv of F and L}), we have $\widehat{\ell }%
_{F,n}=\widehat{\ell }_{L,n}+O_{p}\left( n^{-1}\right) $. It then follows,
by tedious but straightforward calculations\footnote{%
Further details are available from the authors upon request.} and by making
use of Assumptions 1, 2(i), 3-6, 7(i)-(iii), and 8 that 
\begin{eqnarray*}
&&D_{\mu }^{-1}X^{\prime }\left[ A-\widehat{\ell }_{F,n}M^{\left(
Z_{1},Q\right) }\right] \left( y-X\delta _{0}\right) \\
&=&D_{\mu }^{-1}X^{\prime }A\left( y-X\delta _{0}\right) -\widehat{\ell }%
_{L,n}D_{\mu }^{-1}X^{\prime }M^{\left( Z_{1},Q\right) }\left( y-X\delta
_{0}\right) +O_{p}\left( \frac{1}{n}\right) O_{p}\left( \frac{n}{\mu
_{n}^{\min }}\right) \\
&=&\frac{\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{\sqrt{n}}%
+D_{\mu }^{-1}\left( U-\varepsilon \rho ^{\prime }\right) ^{\prime
}A\varepsilon \left[ 1+o_{p}\left( 1\right) \right] +O_{p}\left( \frac{1}{%
\mu _{n}^{\min }}\max \left\{ \sqrt{\frac{K_{2,n}}{n}},\frac{K_{1,n}\sqrt{%
K_{2,n}}}{n}\right\} \right) \\
&&+O_{p}\left( \frac{\sqrt{K_{2,n}}}{n}\right) +O_{p}\left( \frac{1}{%
K_{2,n}^{\varrho _{\gamma }}}\right) +O_{p}\left( \frac{\kappa _{n}^{\max }}{%
\mu _{n}^{\min }}\frac{1}{K_{1,n}^{\varrho _{f}}}\right) +O_{p}\left( \frac{%
\tau _{n}}{K_{1,n}^{\varrho _{g}}}\right) +O_{p}\left( \frac{1}{\mu
_{n}^{\min }}\right) \\
&=&\frac{\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{\sqrt{n}}%
+D_{\mu }^{-1}\underline{U}^{\prime }A\varepsilon \left[ 1+o_{p}\left(
1\right) \right] \text{, where }\underline{U}=U-\varepsilon \rho ^{\prime }%
\text{. }\square
\end{eqnarray*}

\bigskip

\noindent \textbf{Lemma S2-14: }For any $a\in \mathbb{R}^{d}$ such that $%
\left\Vert a\right\Vert =1$, define $b_{1n}=\Sigma _{n}^{-1/2}a$, $%
\underline{u}_{2,\left( i,t\right) ,n}=b_{2n}^{\prime }\underline{U}_{\left(
i,t\right) }$

\noindent $=\sqrt{K_{2,n}}a^{\prime }\Sigma _{n}^{-1/2}D_{\mu }^{-1}%
\underline{U}_{\left( i,t\right) }$, $\sigma _{\left( i,t\right) ,n}^{2}=E%
\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $, $%
\widetilde{\psi }_{\left( i,t\right) ,n}=E\left[ \underline{u}_{2,\left(
i,t\right) ,n}\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] $%
, and $\widetilde{\omega }_{\left( i,t\right) }^{2}=E\left[ \underline{u}%
_{2,\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] $. Suppose that
Assumptions 1-2 and 5-6 are satisfied. Then, the following statements are
true.

\noindent (a) $\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right) -1}\left[
b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }/\sqrt{n}\right] \left( A_{\left( i,t\right) ,\left( j,s\right)
}/\sqrt{K_{2,n}}\right) \left\{ \varepsilon _{\left( j,s\right) }\widetilde{%
\psi }_{\left( i,t\right) ,n}+\underline{u}_{2,\left( j,s\right) }\sigma
_{\left( i,t\right) ,n}^{2}\right\} =O_{p}\left( K_{2,n}^{1/4}/\mu
_{n}^{\min }\right) =o_{p}\left( 1\right) $.

\noindent (b) $\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}\left( A_{\left( i,t\right) ,\left( j,s\right) }^{2}/K_{2,n}\right)
\left( \varepsilon _{\left( j,s\right) }^{2}-\sigma _{\left( j,s\right)
,n}^{2}\right) \widetilde{\omega }_{\left( i,t\right) ,n}^{2}=O_{p}\left(
K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}n^{-1/2}\right) =o_{p}\left(
1\right) $.

\noindent (c) $\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}\left( A_{\left( i,t\right) ,\left( j,s\right) }^{2}/K_{2,n}\right)
\left( \underline{u}_{2,\left( j,s\right) ,n}^{2}-\widetilde{\omega }%
_{\left( j,s\right) ,n}^{2}\right) \sigma _{\left( i,t\right)
,n}^{2}=O_{p}\left( K_{2,n}\left( \mu _{n}^{\min }\right)
^{-2}n^{-1/2}\right) =o_{p}\left( 1\right) $.

\medskip

\noindent \textbf{Proof of Lemma S2-14:}

To show part (a), first let $\mathfrak{W}_{n}=\dsum\nolimits_{\left(
i,t\right) =2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left(
i,t\right) -1}\left[ b_{1n}^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( i,t\right) }/\sqrt{n}\right] \left( A_{\left(
i,t\right) ,\left( j,s\right) }/\sqrt{K_{2,n}}\right) $

\noindent $\times \left\{ \varepsilon _{\left( j,s\right) }\widetilde{\psi }%
_{\left( i,t\right) ,n}+\underline{u}_{2,\left( j,s\right) ,n}\sigma
_{\left( i,t\right) ,n}^{2}\right\} $. By taking expectation and applying
the triangle inequality, we obtain $E\left[ \mathfrak{W}_{n}^{2}|\mathcal{F}%
_{n}^{W}\right] \leq \mathcal{H}_{1}+\mathcal{H}_{2}+\mathcal{H}_{3}+%
\mathcal{H}_{4}$, where $\mathcal{H}_{1}=\dsum\nolimits_{\left( i,t\right)
,\left( k,\upsilon \right) =2}^{m_{n}}\left\vert n^{-1}\left[ b_{1n}^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right]
\right. $

\noindent $\left. \times \left[ b_{1n}^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( k,\upsilon \right) }\right] \widetilde{\psi }%
_{\left( i,t\right) ,n}\widetilde{\psi }_{\left( k,\upsilon \right)
,n}\dsum\nolimits_{\left( j,s\right) =1}^{\min \left\{ \left( i,t\right)
,\left( k,\upsilon \right) \right\} -1}\left( A_{\left( i,t\right) ,\left(
j,s\right) }A_{\left( k,\upsilon \right) ,\left( j,s\right) }\sigma _{\left(
j,s\right) ,n}^{2}/K_{2,n}\right) \right\vert $,

\noindent $\mathcal{H}_{2}=\dsum\nolimits_{\left( i,t\right) ,\left(
k,\upsilon \right) =2}^{m_{n}}\left\vert \left( \left[ b_{1n}^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right] %
\left[ b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
k,\upsilon \right) }\right] /n\right) \sigma _{\left( i,t\right) ,n}^{2}%
\widetilde{\psi }_{\left( k,\upsilon \right) ,n}\right. $

\noindent $\left. \times \dsum\nolimits_{\left( j,s\right) =1}^{\min \left\{
\left( i,t\right) ,\left( k,\upsilon \right) \right\} -1}\left( A_{\left(
i,t\right) ,\left( j,s\right) }A_{\left( k,\upsilon \right) ,\left(
j,s\right) }\widetilde{\psi }_{\left( j,s\right) ,n}/K_{2,n}\right)
\right\vert $, $\mathcal{H}_{3}=\dsum\nolimits_{\left( i,t\right) ,\left(
k,\upsilon \right) =2}^{m_{n}}\left\vert n^{-1}\left[ b_{1n}^{\prime }\Gamma
^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right] \right. $

\noindent $\left. \times \left[ b_{1n}^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( k,\upsilon \right) }\right] \sigma _{\left(
k,\upsilon \right) ,n}^{2}\widetilde{\psi }_{\left( i,t\right)
,n}\dsum\nolimits_{\left( j,s\right) =1}^{\min \left\{ \left( i,t\right)
,\left( k,\upsilon \right) \right\} -1}\left( A_{\left( i,t\right) ,\left(
j,s\right) }A_{\left( k,\upsilon \right) ,\left( j,s\right) }\widetilde{\psi 
}_{\left( j,s\right) ,n}/K_{2,n}\right) \right\vert $,

\noindent $\mathcal{H}_{4}=\dsum\nolimits_{\left( i,t\right) ,\left(
k,\upsilon \right) =2}^{m_{n}}\left\vert \left( \left[ b_{1n}^{\prime
}\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left( i,t\right) }\right] %
\left[ b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
k,\upsilon \right) }\right] /n\right) \sigma _{\left( i,t\right)
,n}^{2}\sigma _{\left( k,\upsilon \right) ,n}^{2}\right. $

\noindent $\left. \times \dsum\nolimits_{\left( j,s\right) =1}^{\min \left\{
\left( i,t\right) ,\left( k,\upsilon \right) \right\} -1}\left( A_{\left(
i,t\right) ,\left( j,s\right) }A_{\left( k,\upsilon \right) ,\left(
j,s\right) }\widetilde{\omega }_{\left( j,s\right) ,n}^{2}/K_{2,n}\right)
\right\vert $. Focusing first on $\mathcal{H}_{1}$, we obtain, by applying
the CS inequality,%
\begin{eqnarray*}
&&\mathcal{H}_{1} \\
&\leq &\sqrt{\dsum\limits_{\left( i,t\right) ,\left( \left( k,\upsilon
\right) \right) =2}^{m_{n}}\frac{b_{1n}^{\prime }\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }e_{\left( i,t\right) }e_{\left( i,t\right) }^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma b_{1n}\widetilde{\psi }_{\left( i,t\right)
,n}^{2}}{n}\frac{b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right)
}e_{\left( k,\upsilon \right) }e_{\left( k,\upsilon \right) }^{\prime
}M^{\left( Z_{1},Q\right) }\Gamma b_{1n}\widetilde{\psi }_{\left( k,\upsilon
\right) ,n}^{2}}{n}} \\
&&\times \sqrt{\dsum\limits_{\left( i,t\right)
=2}^{m_{n}}\dsum\limits_{\left( k,\upsilon \right) =2}^{m_{n}}\left(
\dsum\limits_{\left( j,s\right) =1}^{\min \left\{ \left( i,t\right) ,\left(
k,\upsilon \right) \right\} -1}\frac{A_{\left( i,t\right) ,\left( j,s\right)
}A_{\left( k,\upsilon \right) ,\left( j,s\right) }\sigma _{\left( j,s\right)
,n}^{2}}{K_{2,n}}\right) ^{2}}
\end{eqnarray*}%
Applying the CS inequality, Assumption 2(i), part (d) of Lemma S2-3, and
Lemma S2-6, we obtain%
\begin{eqnarray}
\max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widetilde{\psi }%
_{\left( i,t\right) ,n}\right\vert &=&\max_{1\leq \left( i,t\right) \leq
m_{n}}\sqrt{K_{2,n}}E\left[ \left\vert \varepsilon _{\left( i,t\right) }%
\underline{U}_{\left( i,t\right) }^{\prime }D_{\mu }^{-1}\Sigma
_{n}^{-1/2}a\right\vert |\mathcal{F}_{n}^{W}\right]  \notag \\
&\leq &\frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right) }\sqrt{a^{\prime
}\Sigma _{n}^{-1}a}\sqrt{\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] }\sqrt{%
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}%
_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right] } 
\notag \\
&=&O_{a.s.}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right) }%
\right)  \label{bd on cov of e and u2}
\end{eqnarray}%
Moreover, by direct calculations,

\noindent $\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( k,\upsilon \right) =2}^{m_{n}}\left(
\dsum\nolimits_{\left( j,s\right) =1}^{\min \left\{ \left( i,t\right)
,\left( k,\upsilon \right) \right\} -1}A_{\left( i,t\right) ,\left(
j,s\right) }A_{\left( k,\upsilon \right) ,\left( j,s\right) }\sigma _{\left(
j,s\right) ,n}^{2}/K_{2,n}\right) ^{2}=K_{2,n}^{-2}tr\left\{ LD_{\sigma
^{2}}L^{\prime }LD_{\sigma ^{2}}L^{\prime }\right\} $, where $L$ is the
lower triangular matrix such that $L_{\left( i,t\right) ,\left( j,s\right)
}=A_{\left( i,t\right) ,\left( j,s\right) }\mathbb{I}\left\{ \left(
i,t\right) >\left( j,s\right) \right\} $ and $D_{\sigma ^{2}}=diag\left(
\sigma _{\left( 1,1\right) ,n}^{2},....,\sigma _{\left( n,T_{n}\right)
,n}^{2}\right) =diag\left( \sigma _{1,n}^{2},....,\sigma
_{m_{n},n}^{2}\right) $ and where $\sigma _{\left( i,t\right) ,n}^{2}=E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $ for $%
\left( i,t\right) =1,...,m_{n}$. In addition, by generalizing the results of
Lemma B3 of Chao et al (2012) to our setting here, we can show that $%
\left\Vert LL^{\prime }\right\Vert _{F}=O_{a.s.}\left( \sqrt{K_{2,n}}\right) 
$. Using these results, we further deduce, by applying the CS inequality,
Assumptions 2 and 3(iii), and part (d) of Lemma S2-3 that%
\begin{eqnarray}
\mathcal{H}_{1} &\leq &\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}\left\vert \widetilde{\psi }_{\left( i,t\right) ,n}\right\vert \right)
^{2}\left( \frac{b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n}\right) 
\frac{1}{K_{2,n}}\sqrt{tr\left\{ L^{\prime }LD_{\sigma ^{2}}L^{\prime
}LD_{\sigma ^{2}}\right\} }  \notag \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widetilde{%
\psi }_{\left( i,t\right) ,n}\right\vert \right) ^{2}\left( \frac{%
b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n}\right) \frac{1}{K_{2,n}}%
\left( tr\left\{ L^{\prime }LD_{\sigma ^{2}}^{2}L^{\prime }L\right\} \right)
^{1/4}\left( tr\left\{ D_{\sigma ^{2}}L^{\prime }LL^{\prime }LD_{\sigma
^{2}}\right\} \right) ^{1/4}  \notag
\end{eqnarray}%
\begin{eqnarray}
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widetilde{%
\psi }_{\left( i,t\right) ,n}\right\vert \right) ^{2}\left( \max_{1\leq
\left( i,t\right) \leq m_{n}}\sigma _{\left( i,t\right) ,n}^{2}\right)
\left( \frac{b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n}\right) \frac{%
1}{K_{2,n}}\left\Vert LL^{\prime }\right\Vert _{F}  \notag \\
&=&O_{a.s.}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right) ^{2}}%
\right) \text{. }  \label{parta term 1}
\end{eqnarray}%
Similarly, let $D_{\widetilde{\psi }}=diag\left( \widetilde{\psi }_{\left(
1,1\right) ,n},....,\widetilde{\psi }_{\left( n,T_{n}\right) ,n}\right)
=diag\left( \widetilde{\psi }_{1,n},....,\widetilde{\psi }_{m_{n},n}\right) $%
, we can also show

\begin{eqnarray}
\mathcal{H}_{2} &\leq &\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}\left\vert \widetilde{\psi }_{\left( i,t\right) ,n}\right\vert \right)
\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\sigma _{\left( i,t\right)
,n}^{2}\right) \left( \frac{b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n%
}\right) \frac{1}{K_{2,n}}\sqrt{tr\left\{ L^{\prime }LD_{\widetilde{\psi }%
}L^{\prime }LD_{\widetilde{\psi }}\right\} }  \notag \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert \widetilde{%
\psi }_{\left( i,t\right) ,n}\right\vert \right) ^{2}\left( \max_{1\leq
\left( i,t\right) \leq m_{n}}\sigma _{\left( i,t\right) ,n}^{2}\right)
\left( \frac{b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n}\right) \frac{%
1}{K_{2,n}}\left\Vert LL^{\prime }\right\Vert _{F}  \notag \\
&=&O_{a.s.}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right) ^{2}}%
\right) \text{,}  \label{parta term 2}
\end{eqnarray}%
and%
\begin{eqnarray}
\mathcal{H}_{3} &\leq &\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}\left\vert \widetilde{\psi }_{\left( i,t\right) ,n}\right\vert \right)
^{2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\sigma _{\left(
i,t\right) ,n}^{2}\right) \left( \frac{b_{1n}^{\prime }\Gamma ^{\prime
}\Gamma b_{1n}}{n}\right) \frac{1}{K_{2,n}}\left\Vert LL^{\prime
}\right\Vert _{F}  \notag \\
&=&\text{ }O_{a.s.}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min
}\right) ^{2}}\right) \text{.}  \label{parta term 3}
\end{eqnarray}%
Moreover, let $D_{\widetilde{\omega }^{2}}=diag\left( \widetilde{\omega }%
_{\left( 1,1\right) ,n}^{2},....,\widetilde{\omega }_{\left( n,T_{n}\right)
,n}^{2}\right) =diag\left( \widetilde{\omega }_{1,n}^{2},....,\widetilde{%
\omega }_{m_{n},n}^{2}\right) $, and note that%
\begin{eqnarray}
\mathcal{H}_{4} &\leq &\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}\sigma _{\left( i,t\right) ,n}^{2}\right) ^{2}\left( \frac{%
b_{1n}^{\prime }\Gamma ^{\prime }\Gamma b_{1n}}{n}\right) \frac{1}{K_{2,n}}%
\sqrt{tr\left\{ L^{\prime }LD_{\widetilde{\omega }^{2}}L^{\prime }LD_{%
\widetilde{\omega }^{2}}\right\} }  \notag \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\sigma _{\left(
i,t\right) ,n}^{2}\right) ^{2}\left( \frac{b_{1n}^{\prime }\Gamma ^{\prime
}\Gamma b_{1n}}{n}\right) \left( \max_{1\leq \left( i,t\right) \leq m_{n}}%
\widetilde{\omega }_{\left( i,t\right) ,n}^{2}\right) \frac{\left\Vert
LL^{\prime }\right\Vert _{F}}{K_{2,n}}  \notag \\
&=&O_{a.s.}\left( \frac{\sqrt{K_{2,n}}}{\left( \mu _{n}^{\min }\right) ^{2}}%
\right) \text{,}  \label{parta term 4}
\end{eqnarray}%
where the order of magnitude is calculated by applying Assumptions 2 and
3(iii), part (d) of Lemma S2-3, and the fact that $\left\Vert LL^{\prime
}\right\Vert _{F}=O_{a.s.}\left( \sqrt{K_{2,n}}\right) $ and by making use
of the result%
\begin{eqnarray}
\max_{1\leq \left( i,t\right) \leq m_{n}}\widetilde{\omega }_{\left(
i,t\right) ,n}^{2} &=&\max_{1\leq \left( i,t\right) \leq
m_{n}}K_{2,n}a^{\prime }\Sigma _{n}^{-1/2}D_{\mu }^{-1}E\left[ \underline{U}%
_{\left( i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}%
_{n}^{W}\right] D_{\mu }^{-1}\Sigma _{n}^{-1/2}a  \notag \\
&\leq &\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert 
\underline{U}_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}%
\right] \right) \frac{K_{2,n}a^{\prime }\Sigma _{n}^{-1}a}{\left( \mu
_{n}^{\min }\right) ^{2}}  \notag
\end{eqnarray}%
\begin{equation*}
=O_{a.s.}\left( \frac{K_{2,n}}{\left( \mu _{n}^{\min }\right) ^{2}}\right)
\end{equation*}%
which can be easily deduced from part (d) of Lemma S2-3 and Lemma S2-6.
Combining (\ref{parta term 1})-(\ref{parta term 4}), we obtain $E\left[ 
\mathfrak{W}_{n}^{2}|\mathcal{F}_{n}^{W}\right] =O_{a.s.}\left( \sqrt{K_{2,n}%
}\left( \mu _{n}^{\min }\right) ^{-2}\right) $. Hence, by the law of
iterated expectations and by Theorem 16.1 of Billingsley (1995), there
exists a constant $\overline{C}<\infty $ such that, for all $n$ sufficiently
large, $E\left( \left[ \left( \mu _{n}^{\min }\right) ^{2}/\sqrt{K_{2,n}}%
\right] \mathfrak{W}_{n}^{2}\right) =E_{W_{n}}\left( E\left\{ \left[ \left(
\mu _{n}^{\min }\right) ^{2}/\sqrt{K_{2,n}}\right] \mathfrak{W}_{n}^{2}|%
\mathcal{F}_{n}^{W}\right\} \right) $

\noindent $\leq \overline{C}$. It follows from the Markov's inequality that

\noindent $\mathfrak{W}_{n}=\dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right) -1}\left[
b_{1n}^{\prime }\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }e_{\left(
i,t\right) }/\sqrt{n}\right] \left( A_{\left( i,t\right) ,\left( j,s\right)
}/\sqrt{K_{2,n}}\right) \left\{ \varepsilon _{\left( j,s\right) }\widetilde{%
\psi }_{\left( i,t\right) ,n}+\underline{u}_{2,\left( j,s\right) ,n}\sigma
_{\left( i,t\right) ,n}^{2}\right\} $

\noindent $=O_{p}\left( K_{2,n}^{1/4}/\left( \mu _{n}^{\min }\right) \right)
=o_{p}\left( 1\right) $.

Parts (b) and (c) can be proved by generalizing the argument for parts (ii)
and (iii) of Lemma B4 of Chao et al (2012) to our setting here with cluster
sampling, fixed effects, and possibly many covariates. For the sake of
brevity, we will not include an explicit proof here, but the details of a
proof can be obtained from the authors upon request. $\square $

\qquad \noindent

\noindent \textbf{Lemma S2-15: }Let $\left\{ X_{i,n},\mathcal{F}_{i,n},1\leq
i\leq k_{n},n\geq 1\right\} $ be a square integrable martingale difference
array. Suppose that for all $\epsilon >0$%
\begin{equation}
\dsum\limits_{i=1}^{k_{n}}E\left[ X_{i,n}^{2}\mathbb{I}\left\{ \left\vert
X_{i,n}\right\vert >\epsilon \right\} |\mathcal{F}_{i-1,n}\right] \overset{p}%
{\rightarrow }0  \label{MDA CLT cond 1}
\end{equation}%
and%
\begin{equation}
\dsum\limits_{i=1}^{k_{n}}E\left[ X_{i,n}^{2}|\mathcal{F}_{i-1,n}\right] 
\overset{p}{\rightarrow }1\text{.}  \label{MDA CLT cond 2}
\end{equation}%
Then, $\dsum\nolimits_{i=1}^{k_{n}}X_{i,n}\overset{d}{\rightarrow }N\left(
0,1\right) $.

\bigskip

\noindent \textbf{Proof of Lemma S2-15: }The proof of this central limit
theorem for square integrable martingale difference array is given in G\"{a}%
nsler and Stute (1977). See also Corollary 3.1 in Hall and Heyde (1980).

\medskip

\noindent \textbf{Remark: }Note that a sufficient condition for condition (%
\ref{MDA CLT cond 1}), which we will verify in lieu of (\ref{MDA CLT cond 1}%
) in the proof of Theorems 2 and 3\ in Appendix S1, is the following%
\begin{equation}
\dsum\limits_{i=1}^{k_{n}}E\left[ \left\vert X_{i,n}\right\vert ^{2+\delta }%
\right] \overset{p}{\rightarrow }0\text{, for some }\delta >0.
\label{MDA CLT cond 1b}
\end{equation}

\bigskip

\noindent \textbf{Lemma S2-16: }Let $\widetilde{L}_{n}$ be a sequence of $%
l\times d$, nonrandom matrices (with $l\leq d$) such that $\left\Vert 
\widetilde{L}_{n}\right\Vert _{F}^{2}\leq \overline{C}<\infty $ for some
constant $\overline{C}$, and let $\Sigma _{2,n}=VC\left( D_{\mu }^{-1}%
\underline{U}^{\prime }A\varepsilon |\mathcal{F}_{n}^{W}\right) $

\noindent $=D_{\mu }^{-1}VC\left( \underline{U}^{\prime }A\varepsilon |%
\mathcal{F}_{n}^{W}\right) D_{\mu }^{-1}$. Suppose that there exists a
positive constant $\underline{C}$ such that

\noindent $\lambda _{\min }\left( \left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) \geq \underline{C}>0$ $a.s.n$. Furthermore, let $a\in 
\mathbb{R}^{d}$ such that $\left\Vert a\right\Vert _{2}=1$ and let $%
\underline{u}_{a,\left( i,t\right) ,n}=a^{\prime }\left( \left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }/K_{2,n}\right) ^{-1/2}\widetilde{L}_{n}D_{\mu
}^{-1}\underline{U}_{\left( i,t\right) }$. Suppose that Assumptions 1-2 and
5-6 are satisfied and that $\left( \mu _{n}^{\min }\right)
^{2}/K_{2,n}=o\left( 1\right) $ but

\noindent $\sqrt{K_{2,n}}/\left( \mu _{n}^{\min }\right) ^{2}\rightarrow 0$.
Under these conditions, the following statements are true:

\noindent (a) $\left[ \left( \mu _{n}^{\min }\right) ^{2}/K_{2,n}\right]
\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}\dsum\nolimits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\left( \underline{u}_{a,\left( j,s\right) ,n}^{2}-E\left[ \underline{u}%
_{a,\left( j,s\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] \right) E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]
=O_{p}\left( n^{-1/2}\right) =o_{p}\left( 1\right) $;

\noindent (b) $\left[ \left( \mu _{n}^{\min }\right) ^{2}/K_{2,n}\right]
\dsum\nolimits_{\left( i,t\right) =2}^{m_{n}}\dsum\nolimits_{\left(
j,s\right) =1}^{\left( i,t\right) -1}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\left( \varepsilon _{\left( j,s\right) }^{2}-E\left[ \varepsilon
_{\left( j,s\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) E\left[ 
\underline{u}_{a,\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right] $

\noindent $=O_{p}\left( n^{-1/2}\right) =o_{p}\left( 1\right) $.

\bigskip

\noindent \textbf{Proof of Lemma S2-16:}

To proceed, note first that Lemma S2-6 along with the assumptions on

\noindent $\lambda _{\min }\left( \left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime
}/K_{2,n}\right) $ and $\left\Vert \widetilde{L}_{n}\right\Vert _{F}^{2}$
together imply that%
\begin{eqnarray}
E\left[ \underline{u}_{a,\left( i,t\right) ,n}^{2}|\mathcal{F}_{n}^{W}\right]
&=&a^{\prime }\left( \frac{\left( \mu _{n}^{\min }\right) ^{2}\widetilde{L}%
_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime }}{K_{2,n}}%
\right) ^{-1/2}\widetilde{L}_{n}D_{\mu }^{-1}E\left[ \underline{U}_{\left(
i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}_{n}^{W}%
\right]  \notag \\
&&D_{\mu }^{-1}\widetilde{L}_{n}^{\prime }\left( \frac{\left( \mu _{n}^{\min
}\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }}{K_{2,n}}\right) ^{-1/2}a  \notag \\
&\leq &\frac{1}{\left( \mu _{n}^{\min }\right) ^{2}}\frac{\max_{1\leq \left(
i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}_{\left( i,t\right)
}\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}\right] \left\Vert \widetilde{L}%
_{n}\right\Vert _{F}^{2}}{\lambda _{\min }\left( \left( \mu _{n}^{\min
}\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }/K_{2,n}\right) }  \notag \\
&=&O_{a.s.}\left( \frac{1}{\left( \mu _{n}^{\min }\right) ^{2}}\right)
\label{bound on var u tilde}
\end{eqnarray}%
and that%
\begin{eqnarray}
E\left[ \underline{u}_{a,\left( i,t\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right]
&=&E\left[ \left( a^{\prime }\left( \frac{\left( \mu _{n}^{\min }\right) ^{2}%
\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}_{n}^{\prime }%
}{K_{2,n}}\right) ^{-1/2}\widetilde{L}_{n}D_{\mu }^{-1}\underline{U}_{\left(
i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }D_{\mu }^{-1}%
\widetilde{L}_{n}^{\prime }\right. \right.  \notag \\
&&\text{ \ \ \ \ \ }\left. \left. \times \left( \frac{\left( \mu _{n}^{\min
}\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}\widetilde{L}%
_{n}^{\prime }}{K_{2,n}}\right) ^{-1/2}a\right) ^{2}|\mathcal{F}_{n}^{W}%
\right]  \notag
\end{eqnarray}%
\begin{eqnarray}
&\leq &\frac{1}{\left( \mu _{n}^{\min }\right) ^{4}}\frac{\max_{1\leq \left(
i,t\right) \leq m_{n}}E\left[ \left\Vert \underline{U}_{\left( i,t\right)
}\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}\right] \left\Vert \widetilde{L}%
_{n}\right\Vert _{F}^{4}}{\left[ \lambda _{\min }\left( \left( \mu
_{n}^{\min }\right) ^{2}\widetilde{L}_{n}H_{n}^{-1}\Sigma _{2,n}H_{n}^{-1}%
\widetilde{L}_{n}^{\prime }/K_{2,n}\right) \right] ^{2}}  \notag \\
&=&O_{a.s.}\left( \frac{1}{\left( \mu _{n}^{\min }\right) ^{4}}\right)
\label{bd 4th moment utilde}
\end{eqnarray}

For part (a), define

\noindent $\mathfrak{Z}_{n}=\left[ \left( \mu _{n}^{\min }\right)
^{2}/K_{2,n}\right] \dsum\nolimits_{\left( i,t\right)
=2}^{m_{n}}\dsum\nolimits_{\left( j,s\right) =1}^{\left( i,t\right)
-1}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \underline{u}%
_{a,\left( j,s\right) ,n}^{2}-E\left[ \underline{u}_{a,\left( j,s\right)
,n}^{2}|\mathcal{F}_{n}^{W}\right] \right) E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $, and note that we can apply
Assumption 2(i), part (c) of Lemma S2-1, and the upper bounds given by (\ref%
{bound on var u tilde}) and (\ref{bd 4th moment utilde}) above to obtain%
\begin{eqnarray*}
E\left[ \mathfrak{Z}_{n}^{2}|\mathcal{F}_{n}^{W}\right] &\leq &\frac{\left(
\mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}\dsum\limits_{\left( j,s\right)
=1}^{m_{n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left( k,\upsilon
\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) ,\left( k,\upsilon
\right) \neq \left( j,s\right) }}^{m_{n}}\left( A_{\left( i,t\right) ,\left(
j,s\right) }^{2}A_{\left( k,\upsilon \right) ,\left( j,s\right) }^{2}E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[
\varepsilon _{\left( k,\upsilon \right) }^{2}|\mathcal{F}_{n}^{W}\right]
\right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times \left\{ E\left[ 
\underline{u}_{a,\left( j,s\right) ,n}^{4}|\mathcal{F}_{n}^{W}\right]
+\left( E\left[ \underline{u}_{a,\left( j,s\right) ,n}^{2}|\mathcal{F}%
_{n}^{W}\right] \right) ^{2}\right\} \right) \\
&\leq &O_{a.s.}\left( \frac{\left( \mu _{n}^{\min }\right) ^{4}}{K_{2,n}^{2}}%
\right) O_{a.s.}\left( \frac{K_{2,n}^{2}}{n}\right) O_{a.s.}\left( 1\right)
O_{a.s.}\left( 1\right) O_{a.s.}\left( \frac{1}{\left( \mu _{n}^{\min
}\right) ^{4}}\right) =O_{a.s.}\left( \frac{1}{n}\right)
\end{eqnarray*}%
Hence, the law of iterated expectations and Theorem 16.1 of Billingsley
(1995) imply that there exists a positive constant $\overline{C}<\infty $
such that, for all $n$ sufficiently large, $E\left( n\mathfrak{Z}%
_{n}^{2}\right) =E_{W_{n}}\left( nE\left[ \mathfrak{Z}_{n}^{2}|\mathcal{F}%
_{n}^{W}\right] \right) \leq \overline{C}$. Application of the Markov's
inequality then implies that $\mathfrak{Z}_{n}=O_{p}\left( n^{-1/2}\right)
=o_{p}\left( 1\right) $, which shows part (a).

Part (b) can be shown in a manner similar to part (a). For the sake of
brevity, we will not include an explicit proof here, but the details of a
proof can be obtained from the authors upon request. $\square $

\medskip

\noindent \textbf{Lemma S2-17 \ }Under Assumptions 1-7, $D_{\mu
}^{-1}X^{\prime }AD\left( \varepsilon \circ \varepsilon \right) AXD_{\mu
}^{-1}=\Sigma _{1,n}$

\noindent $+\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right) =1,\left(
i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}\sigma _{\left( i,t\right) }^{2}D_{\mu }^{-1}\Psi _{\left(
j,s\right) }D_{\mu }^{-1}+o_{p}\left( 1\right) $, where $\Sigma
_{1,n}=\Gamma ^{\prime }M^{\left( Z_{1},Q\right) }D_{\sigma ^{2}}M^{\left(
Z_{1},Q\right) }\Gamma /n$, $\sigma _{\left( i,t\right) }^{2}=E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $, $%
D_{\sigma ^{2}}=diag\left( \sigma _{\left( 1,1\right) }^{2},...,\sigma
_{\left( n,T_{n}\right) }^{2}\right) $, and $\Psi _{\left( j,s\right) }=E%
\left[ U_{\left( j,s\right) }U_{\left( j,s\right) }^{\prime }|\mathcal{F}%
_{n}^{W}\right] $.

\noindent \textbf{Proof of Lemma S2-17: }This lemma can be proved by
generalizing an argument similar to Lemma A8 of Chao et al (2012) to our
setting here with cluster sampling, fixed effects, and possibly many
covariates. For the sake of brevity, we will not include an explicit proof
here, but the details of a proof can be obtained from the authors upon
request. $\square $

\medskip

\noindent \textbf{Lemma S2-18 }Suppose that Assumptions 1-8 are satisfied,
and let $\left\{ \widehat{\delta }_{n}\right\} $ be any sequence of
estimators such that $\left\Vert \widehat{\delta }_{n}-\delta
_{0}\right\Vert _{2}\overset{p}{\rightarrow }0$ as $n\rightarrow \infty $,
as long as $\sqrt{K_{2,n}}/\left( \mu _{n}^{\min }\right) ^{2}\rightarrow 0$%
. Also, define the following notations: let $\widehat{\varepsilon }%
=M^{\left( Z,Q\right) }\left( y-X\widehat{\delta }_{n}\right) $, $J=\left[
M^{Q}\circ M^{Q}\right] ^{-1}$, $S_{1}=X^{\prime }AD\left( J\left[ \widehat{%
\varepsilon }\circ \widehat{\varepsilon }\right] \right) AX$, $S_{2}=\left( 
\widehat{\varepsilon }\circ \widehat{\varepsilon }\right) ^{\prime }J\left(
A\circ A\right) J\left( \widehat{\varepsilon }\iota _{d}^{\prime }\circ
M^{\left( Z,Q\right) }X\right) $,

\noindent $\underline{S}_{2}=\left( \widehat{\varepsilon }\circ \widehat{%
\varepsilon }\right) ^{\prime }J\left( A\circ A\right) J\left( \widehat{%
\varepsilon }\iota _{d}^{\prime }\circ \widehat{\underline{U}}\right) $ with 
$\widehat{\underline{U}}=M^{\left( Z,Q\right) }X-\widehat{\varepsilon }%
\widehat{\rho }_{n}^{\prime }$, $S_{3}=\left( \widehat{\varepsilon }\circ 
\widehat{\varepsilon }\right) ^{\prime }J\left( A\circ A\right) J\left( 
\widehat{\varepsilon }\circ \widehat{\varepsilon }\right) $,

\noindent $S_{4}=\left( \widehat{\varepsilon }\iota _{d}^{\prime }\circ
M^{\left( Z,Q\right) }X\right) ^{\prime }J\left( A\circ A\right) J\left( 
\widehat{\varepsilon }\iota _{d}^{\prime }\circ M^{\left( Z,Q\right)
}X\right) $, $\underline{S}_{4}=\left( \widehat{\varepsilon }\iota
_{d}^{\prime }\circ \widehat{\underline{U}}\right) ^{\prime }J\left( A\circ
A\right) J\left( \widehat{\varepsilon }\iota _{d}^{\prime }\circ \widehat{%
\underline{U}}\right) $, and $\Sigma _{1,n}=\Gamma ^{\prime }M^{\left(
Z_{1},Q\right) }D_{\sigma ^{2}}M^{\left( Z_{1},Q\right) }\Gamma /n$. In
addition, define $\sigma _{\left( i,t\right) }^{2}=E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] $, $D_{\sigma
^{2}}=diag\left( \sigma _{\left( 1,1\right) }^{2},...,\sigma _{\left(
n,T_{n}\right) }^{2}\right) $, $\phi _{\left( i,t\right) }=E\left[ U_{\left(
i,t\right) }\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}\right] $, $%
\Psi _{\left( i,t\right) }=E\left[ U_{\left( i,t\right) }U_{\left(
i,t\right) }^{\prime }|\mathcal{F}_{n}^{W}\right] $,

\noindent $\underline{\phi }_{\left( i,t\right) }=E\left[ \underline{U}%
_{\left( i,t\right) }\varepsilon _{\left( i,t\right) }|\mathcal{F}_{n}^{W}%
\right] $, and $\underline{\Psi }_{\left( i,t\right) }=E\left[ \underline{U}%
_{\left( i,t\right) }\underline{U}_{\left( i,t\right) }^{\prime }|\mathcal{F}%
_{n}^{W}\right] $ where $\underline{U}_{\left( i,t\right) }=U_{\left(
i,t\right) }-\rho \varepsilon _{\left( i,t\right) }$ and where for
notational convenience we suppress the dependence of $\sigma _{\left(
i,t\right) }^{2}$, $\phi _{\left( i,t\right) }$, $\Psi _{\left( i,t\right) }$%
, $\underline{\phi }_{\left( i,t\right) }$, and $\underline{\Psi }_{\left(
i,t\right) }$ on $\mathcal{F}_{n}^{W}=\sigma \left( W_{n}\right) $. Then,
under the above conditions, the following statements are true.

\begin{enumerate}
\item[(a)] $D_{\mu }^{-1}S_{1}D_{\mu }^{-1}=\Sigma
_{1,n}+\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right) =1,\left(
i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}\sigma _{\left( i,t\right) }^{2}D_{\mu }^{-1}\Psi _{\left(
j,s\right) }D_{\mu }^{-1}$

$+o_{p}\left( \max \left\{ 1,K_{2,n}\left( \mu _{n}^{\min }\right)
^{-2}\right\} \right) $.

\item[(b)] $S_{3}/K_{2,n}-K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\sigma _{\left(
i,t\right) }^{2}\sigma _{\left( j,s\right) }^{2}=o_{p}\left( 1\right) $.

\item[(c)] $D_{\mu }^{-1}S_{4}D_{\mu }^{-1}-\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}D_{\mu }^{-1}\phi
_{\left( i,t\right) }\phi _{\left( j,s\right) }^{\prime }D_{\mu
}^{-1}=o_{p}\left( K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}\right) $.

\item[(d)] $\left( \mu _{n}^{\min }/K_{2,n}\right) S_{2}D_{\mu }^{-1}-\left(
\mu _{n}^{\min }/K_{2,n}\right) \dsum\nolimits_{\left( i,t\right) ,\left(
j,s\right) =1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left(
i,t\right) ,\left( j,s\right) }^{2}\sigma _{\left( i,t\right) }^{2}\phi
_{\left( j,s\right) }^{\prime }D_{\mu }^{-1}=o_{p}\left( 1\right) $.

\item[(e)] $D_{\mu }^{-1}\widehat{\rho }_{n}=O_{p}\left( \left( \mu
_{n}^{\min }\right) ^{-1}\right) $ and $D_{\mu }^{-1}\left( \widehat{\rho }%
_{n}-\rho \right) =o_{p}\left( \left( \mu _{n}^{\min }\right) ^{-1}\right) $%
, where $\rho =\lim_{n\rightarrow \infty }\rho _{n}=\lim_{n\rightarrow
\infty }\left( E\left[ U^{\prime }M^{Q}\varepsilon \right] /n\right) /\left(
E\left[ \varepsilon ^{\prime }M^{Q}\varepsilon \right] /n\right) $.

\item[(f)] $D_{\mu }^{-1}\underline{S}_{4}D_{\mu
}^{-1}-\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right) =1,\left(
i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}D_{\mu }^{-1}\underline{\phi }_{\left( i,t\right) }%
\underline{\phi }_{\left( j,s\right) }^{\prime }D_{\mu }^{-1}=o_{p}\left(
K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}\right) $.

\item[(g)] $\left( \mu _{n}^{\min }/K_{2,n}\right) -\left( \mu _{n}^{\min
}/K_{2,n}\right) \dsum\nolimits_{\left( i,t\right) ,\left( j,s\right)
=1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}\sigma _{\left( i,t\right) }^{2}\underline{\phi }%
_{\left( j,s\right) }^{\prime }D_{\mu }^{-1}=o_{p}(1)$.
\end{enumerate}

\medskip

\noindent \textbf{Proof of Lemma S2-18:}

To show part (a), note first that, by making use of the decomposition $%
M^{\left( Z,Q\right) }=M^{Q}-P^{Z^{\perp }}$, where $P^{Z^{\perp
}}=M^{Q}Z\left( Z^{\prime }M^{Q}Z\right) ^{-1}Z^{\prime }M^{Q}$, we can,
after some straightforward algebra, write $\widehat{\varepsilon }=-M^{\left(
Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) +M^{\left(
Z,Q\right) }\varphi _{n}+M^{Q}\varepsilon -P^{Z^{\perp }}\varepsilon $, from
which we obtain 
\begin{eqnarray}
J\left[ \widehat{\varepsilon }\circ \widehat{\varepsilon }\right] &=&J\left[
M^{\left( Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) \circ
M^{\left( Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) %
\right] +J\left[ M^{\left( Z,Q\right) }\varphi _{n}\circ M^{\left(
Z,Q\right) }\varphi _{n}\right]  \notag \\
&&+J\left[ M^{Q}\varepsilon \circ M^{Q}\varepsilon \right] +J\left[
P^{Z^{\perp }}\varepsilon \circ P^{Z^{\perp }}\varepsilon \right] -2J\left[
M^{\left( Z,Q\right) }\varphi _{n}\circ M^{\left( Z,Q\right) }X\left( 
\widehat{\delta }_{n}-\delta _{0}\right) \right]  \notag \\
&&-2J\left[ M^{Q}\varepsilon \circ M^{\left( Z,Q\right) }X\left( \widehat{%
\delta }_{n}-\delta _{0}\right) \right] +2J\left[ P^{Z^{\perp }}\varepsilon
\circ M^{\left( Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) %
\right]  \notag \\
&&+2J\left[ M^{Q}\varepsilon \circ M^{\left( Z,Q\right) }\varphi _{n}\right]
-2J\left[ P^{Z^{\perp }}\varepsilon \circ M^{\left( Z,Q\right) }\varphi _{n}%
\right] -2J\left[ M^{Q}\varepsilon \circ P^{Z^{\perp }}\varepsilon \right]
\label{normalized Hadamard prod}
\end{eqnarray}%
where $J=\left[ M^{Q}\circ M^{Q}\right] ^{-1}$. Substituting the right-hand
side of (\ref{normalized Hadamard prod}) into covariance matrix estimator $%
D_{\mu }^{-1}X^{\prime }AD\left( J\left[ \widehat{\varepsilon }\circ 
\widehat{\varepsilon }\right] \right) AXD_{\mu }^{-1}$, we get 
\begin{eqnarray*}
&&D_{\mu }^{-1}S_{1}D_{\mu }^{-1}-D_{\mu }^{-1}X^{\prime }AD\left(
\varepsilon \circ \varepsilon \right) AXD_{\mu }^{-1} \\
&=&\mathcal{T}_{1,n}+\mathcal{T}_{2,n}+\mathcal{T}_{3,n}+\mathcal{T}_{4,n}+%
\mathcal{T}_{5,n}+\mathcal{T}_{6,n}+\mathcal{T}_{7,n}+\mathcal{T}_{8,n}+%
\mathcal{T}_{9,n}+\mathcal{T}_{10,n}\text{,}
\end{eqnarray*}%
where $\mathcal{T}_{1,n}=D_{\mu }^{-1}X^{\prime }AD\left( J\left[ M^{\left(
Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) \circ M^{\left(
Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) \right] \right)
AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{2,n}=D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{\left( Z,Q\right) }\varphi _{n}\circ M^{\left( Z,Q\right) }\varphi _{n}%
\right] \right) AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{3,n}=D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{Q}\varepsilon \circ M^{Q}\varepsilon \right] \right) AXD_{\mu
}^{-1}-D_{\mu }^{-1}X^{\prime }AD\left( \varepsilon \circ \varepsilon
\right) AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{4,n}=D_{\mu }^{-1}X^{\prime }AD\left( J\left[
P^{Z^{\perp }}\varepsilon \circ P^{Z^{\perp }}\varepsilon \right] \right)
AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{5,n}=-2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{\left( Z,Q\right) }\varphi _{n}\circ M^{\left( Z,Q\right) }X\left( 
\widehat{\delta }_{n}-\delta _{0}\right) \right] \right) AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{6,n}=-2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{Q}\varepsilon \circ M^{\left( Z,Q\right) }X\left( \widehat{\delta }%
_{n}-\delta _{0}\right) \right] \right) AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{7,n}=2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
P^{Z^{\perp }}\varepsilon \circ M^{\left( Z,Q\right) }X\left( \widehat{%
\delta }_{n}-\delta _{0}\right) \right] \right) AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{8,n}=2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{Q}\varepsilon \circ M^{\left( Z,Q\right) }\varphi _{n}\right] \right)
AXD_{\mu }^{-1}$,

\noindent $\mathcal{T}_{9,n}=-2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
P^{Z^{\perp }}\varepsilon \circ M^{\left( Z,Q\right) }\varphi _{n}\right]
\right) AXD_{\mu }^{-1}$, and

\noindent $\mathcal{T}_{10,n}=-2D_{\mu }^{-1}X^{\prime }AD\left( J\left[
M^{Q}\varepsilon \circ P^{Z^{\perp }}\varepsilon \right] \right) AXD_{\mu
}^{-1}$.

Consider the term $\mathcal{T}_{1,n}$. Let $\mathcal{G}_{i}=\left\{ \left(
\ell ,h\right) :\ell =i\text{ and }h=1,...,T_{i}\right\} $; and note that
for any $a,b\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert
_{2}=\left\Vert b\right\Vert _{2}=1$, we have%
\begin{eqnarray*}
\left\vert a^{\prime }\mathcal{T}_{1,n}b\right\vert &=&\left\vert
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}\dsum\limits_{\left( j,s\right)
\neq \left( i,t\right) }\dsum\limits_{\left( k,v\right) \neq \left(
i,t\right) }\dsum\limits_{\left( p,q\right) =1}^{m_{n}}a^{\prime }D_{\mu
}^{-1}X^{\prime }e_{\left( j,s\right) }A_{\left( i,t\right) ,\left(
j,s\right) }J_{\left( i,t\right) ,\left( p,q\right) }\right. \\
&&\left. \times e_{\left( p,q\right) }^{\prime }M^{\left( Z,Q\right)
}X\left( \widehat{\delta }_{n}-\delta _{0}\right) \left( \widehat{\delta }%
_{n}-\delta _{0}\right) ^{\prime }X^{\prime }M^{\left( Z,Q\right) }e_{\left(
p,q\right) }A_{\left( i,t\right) ,\left( k,v\right) }e_{\left( k,v\right)
}^{\prime }XD_{\mu }^{-1}b\right\vert \\
&=&\left\vert \dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{\left( j,s\right) \neq \left( i,t\right) }a^{\prime
}D_{\mu }^{-1}X^{\prime }e_{\left( j,s\right) }A_{\left( i,t\right) ,\left(
j,s\right) }\right. \\
&&\times \left( \dsum\limits_{\left( p,q\right) =1}^{m_{n}}J_{\left(
i,t\right) ,\left( p,q\right) }\mathbb{I}\left\{ \left( p,q\right) \in 
\mathcal{G}_{i}\right\} e_{\left( p,q\right) }^{\prime }M^{\left( Z,Q\right)
}X\left( \widehat{\delta }_{n}-\delta _{0}\right) \left( \widehat{\delta }%
_{n}-\delta _{0}\right) ^{\prime }X^{\prime }M^{\left( Z,Q\right) }e_{\left(
p,q\right) }\right) \\
&&\times \left. \dsum\limits_{\left( k,v\right) \neq \left( i,t\right)
}A_{\left( i,t\right) ,\left( k,v\right) }e_{\left( k,v\right) }^{\prime
}XD_{\mu }^{-1}b\right\vert
\end{eqnarray*}%
where $J_{\left( i,t\right) ,\left( p,q\right) }$ is the element in the $%
\left( i,t\right) ^{th}$ row and the $\left( p,q\right) ^{th}$ column of the
matrix $J$ for $\left( i,t\right) ,\left( p,q\right) \in \left\{
1,...,m_{n}\right\} $, where $\mathbb{I}\left\{ \cdot \right\} $ denotes an
indicator function, where $e_{\left( j,s\right) }$ is an $m_{n}\times 1$
elementary vector whose $\left( j,s\right) ^{th}$ component is $1$ and all
other components are $0$, and where the second equality above follows from
the fact that $J_{\left( i,t\right) ,\left( p,q\right) }=0$ for $p\neq i$
due to the sparsity (or block diagonal nature) of $J$. Now, let $D^{\Sigma
J}\left( M^{\left( Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right) }\right) $
be a diagonal matrix whose $\left( i,t\right) ^{th}$ diagonal element is
given by $\dsum\nolimits_{q=1}^{T_{i}}\left\vert J_{\left( i,t\right)
,\left( i,q\right) }\right\vert e_{\left( i,q\right) }^{\prime }M^{\left(
Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right) }e_{\left( i,q\right) }$.
Applying the triangle inequality and the inequality $\left\vert
XY\right\vert \leq \left( 1/2\right) X^{2}+\left( 1/2\right) Y^{2}$, we then
obtain

\begin{eqnarray}
\left\vert a^{\prime }\mathcal{T}_{1,n}b\right\vert &=&\text{ }\left\vert
\dsum\limits_{\left( i,t\right) =1}^{m_{n}}a^{\prime }D_{\mu }^{-1}X^{\prime
}Ae_{\left( i,t\right) }\dsum\limits_{q=1}^{T_{i}}J_{\left( i,t\right)
,\left( i,q\right) }e_{\left( i,q\right) }^{\prime }M^{\left( Z,Q\right)
}X\left( \widehat{\delta }_{n}-\delta _{0}\right) \left( \widehat{\delta }%
_{n}-\delta _{0}\right) ^{\prime }\right.  \notag \\
&&\left. \times X^{\prime }M^{\left( Z,Q\right) }e_{\left( i,q\right)
}e_{\left( i,t\right) }^{\prime }AXD_{\mu }^{-1}b\right\vert  \notag \\
&\leq &\frac{1}{2}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{q=1}^{T_{i}}\left\{ \left\vert J_{\left( i,t\right)
,\left( i,q\right) }\right\vert e_{\left( i,q\right) }^{\prime }M^{\left(
Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) \left( \widehat{%
\delta }_{n}-\delta _{0}\right) ^{\prime }X^{\prime }M^{\left( Z,Q\right)
}e_{\left( i,q\right) }\right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ }\left. \times a^{\prime }D_{\mu
}^{-1}X^{\prime }Ae_{\left( i,t\right) }e_{\left( i,t\right) }^{\prime
}AXD_{\mu }^{-1}a\right\}  \notag \\
&&+\frac{1}{2}\dsum\limits_{\left( i,t\right)
=1}^{m_{n}}\dsum\limits_{q=1}^{T_{i}}\left\{ \left\vert J_{\left( i,t\right)
,\left( i,q\right) }\right\vert e_{\left( i,q\right) }^{\prime }M^{\left(
Z,Q\right) }X\left( \widehat{\delta }_{n}-\delta _{0}\right) \left( \widehat{%
\delta }_{n}-\delta _{0}\right) ^{\prime }X^{\prime }M^{\left( Z,Q\right)
}e_{\left( i,q\right) }\right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times b^{\prime }D_{\mu
}^{-1}X^{\prime }Ae_{\left( i,t\right) }e_{\left( i,t\right) }^{\prime
}AXD_{\mu }^{-1}b\right\}  \notag \\
&\leq &\frac{1}{2}\left\Vert \widehat{\delta }_{n}-\delta _{0}\right\Vert
_{2}^{2}a^{\prime }D_{\mu }^{-1}X^{\prime }AD^{\Sigma J}\left( M^{\left(
Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right) }\right) AXD_{\mu }^{-1}a 
\notag \\
&&+\frac{1}{2}\left\Vert \widehat{\delta }_{n}-\delta _{0}\right\Vert
_{2}^{2}b^{\prime }D_{\mu }^{-1}X^{\prime }AD^{\Sigma J}\left( M^{\left(
Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right) }\right) AXD_{\mu }^{-1}b
\label{T1n}
\end{eqnarray}%
By tedious but straightforward calculations, we can show that

\noindent $D_{\mu }^{-1}X^{\prime }AD^{\Sigma J}\left( M^{\left( Z,Q\right)
}XX^{\prime }M^{\left( Z,Q\right) }\right) AXD_{\mu }^{-1}=O_{p}\left( \max
\left\{ 1,K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}\right\} \right) $%
\footnote{%
Details are available from the authors upon request.}. Hence, given the
assumption that $\left\Vert \widehat{\delta }_{n}-\delta _{0}\right\Vert
_{2}^{2}\overset{p}{\rightarrow }0$, it follows from expression (\ref{T1n})
that

\noindent $\left\vert a^{\prime }\mathcal{T}_{1,n}b\right\vert =o_{p}\left(
\max \left\{ 1,K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}\right\} \right) $%
. Since the above argument holds for all $a,b\in \mathbb{R}^{d}$ such that $%
\left\Vert a\right\Vert _{2}=\left\Vert b\right\Vert _{2}=1$, we further
deduce that $\mathcal{T}_{1,n}=o_{p}\left( \max \left\{ 1,K_{2,n}\left( \mu
_{n}^{\min }\right) ^{-2}\right\} \right) $. By following a similar method
of proof, we can also show that $\mathcal{T}_{k,n}=o_{p}\left( \max \left\{
1,K_{2,n}\left( \mu _{n}^{\min }\right) ^{-2}\right\} \right) $ for $%
k=2,...,10$. For the sake of brevity, we will not provide detailed proofs
for these other terms. Detailed argument for these other terms can be
obtained from the authors upon request.

The fact that $\mathcal{T}_{k,n}=o_{p}\left( \max \left\{ 1,K_{2,n}\left(
\mu _{n}^{\min }\right) ^{-2}\right\} \right) $ for each $k=\left\{
1,...,10\right\} $ further implies that%
\begin{equation}
D_{\mu }^{-1}S_{1}D_{\mu }^{-1}-D_{\mu }^{-1}X^{\prime }AD\left( \varepsilon
\circ \varepsilon \right) AXD_{\mu }^{-1}=\dsum\limits_{k=1}^{10}\mathcal{T}%
_{k,n}=o_{p}\left( \max \left\{ 1,\frac{K_{2,n}}{\left( \mu _{n}^{\min
}\right) ^{2}}\right\} \right)  \label{Cov Matrix term 1 part 1}
\end{equation}%
Moreover, by the result of Lemma S2-17, 
\begin{equation}
D_{\mu }^{-1}X^{\prime }AD\left( \varepsilon \circ \varepsilon \right)
AXD_{\mu }^{-1}=\Sigma _{1,n}+\dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\sigma _{\left(
i,t\right) }^{2}D_{\mu }^{-1}\Psi _{\left( j,s\right) }D_{\mu
}^{-1}+o_{p}(1).  \label{Cov Matrix term 1 part 2}
\end{equation}%
Combining (\ref{Cov Matrix term 1 part 1}) and (\ref{Cov Matrix term 1 part
2}), we further obtain

\noindent $D_{\mu }^{-1}S_{1}D_{\mu }^{-1}=\Sigma _{1,n}+$ $%
\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right) =1,\left( i,t\right)
\neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\sigma _{\left( i,t\right) }^{2}D_{\mu }^{-1}\Psi _{\left( j,s\right)
}D_{\mu }^{-1}+o_{p}\left( \max \left\{ 1,K_{2,n}\left( \mu _{n}^{\min
}\right) ^{-2}\right\} \right) $, which shows part (a).

To show part (b), write $S_{3}/K_{2,n}-$ $K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\sigma _{\left(
i,t\right) }^{2}\sigma _{\left( j,s\right) }^{2}=\mathcal{A}+\mathfrak{A}$,
where $\mathcal{A}=S_{3}/K_{2,n}-K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\varepsilon _{\left(
i,t\right) }^{2}\varepsilon _{\left( j,s\right) }^{2}$ and where

\noindent $\mathfrak{A}=K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \varepsilon
_{\left( i,t\right) }^{2}\varepsilon _{\left( j,s\right) }^{2}-\sigma
_{\left( i,t\right) }^{2}\sigma _{\left( j,s\right) }^{2}\right) $. To
analyze the term $\mathcal{A}$, note that, by direct calculation, we can
obtain the following decomposition

\begin{eqnarray}
\mathcal{A} &=&\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{
\dsum\limits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left( i,h\right) }\left[
e_{\left( i,h\right) }^{\prime }M^{\left( Z,Q\right) }\left( y-X\widehat{%
\delta }_{n}\right) \right] ^{2}\right.  \notag \\
&&\left. \text{\ }\times \dsum\limits_{v=1}^{T_{j}}J_{\left( j,s\right)
,\left( j,v\right) }\left[ \left( y-X\widehat{\delta }_{n}\right) ^{\prime
}M^{\left( Z,Q\right) }e_{\left( j,v\right) }\right] ^{2}\right\} -\frac{1}{%
K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ %
\left( i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}\varepsilon _{\left( i,t\right) }^{2}\varepsilon
_{\left( j,s\right) }^{2}  \notag \\
&=&\dsum\limits_{\ell =1}^{6}\mathcal{A}_{\ell }
\label{decomposition into A terms}
\end{eqnarray}

\noindent where $\mathcal{A}_{1}=4K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}$

\noindent $\times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left(
i,h\right) }\left( e_{\left( i,h\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
i,h\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) \left( e_{\left( i,h\right) }^{\prime
}M^{Q}\varepsilon \right) $

\noindent $\times \dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left(
j,v\right) }\left( e_{\left( j,v\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) \left( e_{\left( j,v\right) }^{\prime
}M^{Q}\varepsilon \right) $,

\noindent $\mathcal{A}_{2}=K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}$

\noindent $\times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left(
i,h\right) }\left( e_{\left( i,h\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
i,h\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) ^{2}$

\noindent $\times \dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left(
j,v\right) }\left( e_{\left( j,v\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) ^{2}$,

\noindent $\mathcal{A}_{3}=-4K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left( j,v\right)
}\left( e_{\left( j,v\right) }^{\prime }M^{Q}\varepsilon \right) ^{2}$

\noindent $\times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left(
i,h\right) }\left( e_{\left( i,h\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
i,h\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) \left( e_{\left( i,h\right) }^{\prime
}M^{Q}\varepsilon \right) $,

\noindent $\mathcal{A}_{4}=2K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left( j,v\right)
}\left( e_{\left( j,v\right) }^{\prime }M^{Q}\varepsilon \right) ^{2}$

\noindent $\times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left(
i,h\right) }\left( e_{\left( i,h\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
i,h\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) ^{2}$

\noindent , $\mathcal{A}_{5}=-4K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}$

\noindent $\times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right) ,\left(
i,h\right) }\left( e_{\left( i,h\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
i,h\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) ^{2}$

\noindent $\times \dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left(
j,v\right) }\left( e_{\left( j,v\right) }^{\prime }\left[ P^{Z^{\perp
}}\varepsilon -M^{\left( Z,Q\right) }\varphi _{n}\right] +e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) \left( e_{\left( j,v\right) }^{\prime
}M^{Q}\varepsilon \right) $,

\noindent and $\mathcal{A}_{6}=K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right)
,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}\left\{ A_{\left( i,t\right) ,\left( j,s\right) }^{2}\right. $

\noindent $\left. \times \dsum\nolimits_{h=1}^{T_{i}}J_{\left( i,t\right)
,\left( i,h\right) }\left( e_{\left( i,h\right) }^{\prime }M^{Q}\varepsilon
\right) ^{2}\dsum\nolimits_{v=1}^{T_{j}}J_{\left( j,s\right) ,\left(
j,v\right) }\left( e_{\left( j,v\right) }^{\prime }M^{Q}\varepsilon \right)
^{2}\right\} $

\noindent $-K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right)
=1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}\varepsilon _{\left( i,t\right) }^{2}\varepsilon
_{\left( j,s\right) }^{2}$.

Consider now the $\mathcal{A}_{1}$ term. By applying the triangle inequality
and the inequality $\left\vert XY\right\vert \leq \left( 1/2\right)
X^{2}+\left( 1/2\right) Y^{2}$ and collecting like terms, we get 
\begin{eqnarray*}
\left\vert \mathcal{A}_{1}\right\vert &\leq &\frac{12}{K_{2,n}}\dsum\limits 
_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right)
\neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( j,v\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}\left( e_{\left( i,h\right)
}^{\prime }P^{Z^{\perp }}\varepsilon \right) ^{2} \\
&&+\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left\{ \left( e_{\left(
i,h\right) }^{\prime }M^{Q}\varepsilon \right) ^{2}\right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \text{\ }\times \left( e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }%
_{n}-\delta _{0}\right] \right) ^{2}\right\} \\
&&+\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}\left( e_{\left( j,v\right)
}^{\prime }M^{\left( Z,Q\right) }\varphi _{n}\right) ^{2} \\
&=&\mathcal{A}_{1,1}+\mathcal{A}_{1,2}+\mathcal{A}_{1,3},\text{ }(say).
\end{eqnarray*}%
Clearly $\mathcal{A}_{1,k}\geq 0$ for $k=1,2,3$. Next, note that, after some
straightforward moment calculations and after applying the triangle
inequality as well as Assumptions 1, 2(i), 5, and 6 and part (a) of Lemma
S2-1; we obtain%
\begin{eqnarray}
\text{ }E\left[ \mathcal{A}_{1,1}|\mathcal{F}_{n}^{W}\right] &=&\frac{12}{%
K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ %
\left( i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert
\dsum\limits_{q=1}^{T_{j}}M_{\left( j,v\right) ,\left( j,q\right) }^{Q} 
\notag \\
&&\times \dsum\limits_{g=1}^{T_{j}}M_{\left( j,v\right) ,\left( j,g\right)
}^{Q}\dsum\limits_{\left( l,r\right) =1}^{m_{n}}P_{\left( i,h\right) ,\left(
l,r\right) }^{Z^{\perp }}\dsum\limits_{\left( k,c\right)
=1}^{m_{n}}P_{\left( i,h\right) ,\left( k,c\right) }^{Z^{\perp }}E\left[
\varepsilon _{\left( l,r\right) }\varepsilon _{\left( k,c\right)
}\varepsilon _{\left( j,q\right) }\varepsilon _{\left( j,g\right) }|\mathcal{%
F}_{n}^{W}\right]  \notag \\
&\leq &\frac{12\overline{T}^{3}}{K_{2,n}}\left( \max_{1\leq \left(
i,t\right) ,\left( j,s\right) \leq m_{n}}\left\vert M_{\left( i,t\right)
,\left( j,s\right) }^{Q}\right\vert \right) ^{2}\left( \max_{1\leq \left(
j,s\right) \leq m_{n}}E\left[ \varepsilon _{\left( j,s\right) }^{4}|\mathcal{%
F}_{n}^{W}\right] \right)  \notag \\
&&\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}P_{\left(
i,t\right) ,\left( i,t\right) }^{Z^{\perp }}\right) ^{2}\left( \max_{1\leq
\left( i,t\right) ,\left( j,s\right) \leq m_{n}}\left\vert J_{\left(
i,t\right) ,\left( j,s\right) }\right\vert \right) ^{2}\dsum\limits 
_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right)
\neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}  \notag \\
&&+\frac{12\overline{T}^{3}}{K_{2,n}}\left( \max_{1\leq \left( i,t\right)
,\left( j,s\right) \leq m_{n}}\left\vert M_{\left( i,t\right) ,\left(
j,s\right) }^{Q}\right\vert \right) ^{2}\left( \max_{1\leq \left( i,t\right)
,\left( j,s\right) \leq m_{n}}\left\vert J_{\left( i,t\right) ,\left(
j,s\right) }\right\vert \right) ^{2}  \notag \\
&&\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) ^{2}\left(
\max_{1\leq \left( i,t\right) \leq m_{n}}P_{\left( i,t\right) ,\left(
i,t\right) }^{Z^{\perp }}\right) \dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}  \notag \\
&&+24\overline{T}^{4}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right)
\leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right) }\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right) \leq
m_{n}}\left\vert M_{\left( i,t\right) ,\left( j,s\right) }^{Q}\right\vert
\right) ^{2}  \notag \\
&&\text{ \ \ \ \ }\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E%
\left[ \varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right]
\right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}P_{\left(
i,t\right) ,\left( i,t\right) }^{Z^{\perp }}\right) ^{2}\frac{1}{K_{2,n}}%
\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}  \notag \\
&=&O_{a.s.}\left( \frac{K_{n}^{2}}{n^{2}}\right) +O_{a.s.}\left( \frac{K_{n}%
}{n}\right) +O_{a.s.}\left( \frac{K_{n}^{2}}{n^{2}}\right) =O_{a.s.}\left( 
\frac{K_{n}}{n}\right) \text{.}  \label{EA11}
\end{eqnarray}%
It follows from the law of iterated expectations and Theorem 16.1 of
Billingsley (1995) that there exists a constant $\overline{C}>0$ such that
for all $n$ sufficiently large $E\left[ \left( n/K_{n}\right) \left\vert 
\mathcal{A}_{1,1}\right\vert \right] =E_{W}\left( \frac{n}{K_{n}}E\left[
\left\vert \mathcal{A}_{1,1}\right\vert |\mathcal{F}_{n}^{W}\right] \right)
\leq \overline{C}<\infty $. Application of Markov's inequality then allows
us to further deduce that%
\begin{equation}
\mathcal{A}_{1,1}=O_{p}\left( \frac{K_{n}}{n}\right) =o_{p}\left( 1\right) .
\label{A11}
\end{equation}

Consider next the subterm $\mathcal{A}_{1,2}$. Here, by applying the CS
inequality, we have 
\begin{eqnarray*}
\mathcal{A}_{1,2} &=&\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left(
i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left(
j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}\left( e_{\left( j,v\right)
}^{\prime }M^{\left( Z,Q\right) }X\left[ \widehat{\delta }_{n}-\delta _{0}%
\right] \right) ^{2} \\
&\leq &\frac{12\left\Vert \widehat{\delta }_{n}-\delta _{0}\right\Vert
_{2}^{2}}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\{ \left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}\right. \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right)
}e_{\left( j,v\right) }\right\} \\
&\leq &\left\Vert \widehat{\delta }_{n}-\delta _{0}\right\Vert _{2}^{2}%
\mathcal{A}_{1,2}^{\ast }\text{,}
\end{eqnarray*}

\noindent where $\mathcal{A}_{1,2}^{\ast
}=12K_{2,n}^{-1}\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right)
=1,\left( i,t\right) \neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right)
,\left( j,s\right) }^{2}\dsum\nolimits_{h=1}^{T_{i}}\left\vert J_{\left(
i,t\right) ,\left( i,h\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}$

\noindent $\times \dsum\nolimits_{v=1}^{T_{j}}\left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert e_{\left( j,v\right) }^{\prime
}M^{\left( Z,Q\right) }XX^{\prime }M^{\left( Z,Q\right) }e_{\left(
j,v\right) }$. Next, by applying the inequality

\noindent $\left( x+y\right) ^{\prime }\left( x+y\right) \leq 2x^{\prime
}x+2y^{\prime }y$ twice, we obtain%
\begin{eqnarray*}
0 &\leq &\mathcal{A}_{1,2}^{\ast } \\
&\leq &\frac{24}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2} \\
&&\times e_{\left( j,v\right) }^{\prime }M^{\left( Z,Q\right)
}X_{1}X_{1}^{\prime }M^{\left( Z,Q\right) }e_{\left( j,v\right) } \\
&&+\frac{48}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}e_{\left( j,v\right) }^{\prime
}M^{Q}UU^{\prime }M^{Q}e_{\left( j,v\right) } \\
&&+\frac{48}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}e_{\left( j,v\right) }^{\prime
}P^{Z^{\perp }}UU^{\prime }P^{Z^{\perp }}e_{\left( j,v\right) } \\
&=&\mathcal{A}_{1,2,1}^{\ast }+\mathcal{A}_{1,2,2}^{\ast }+\mathcal{A}%
_{1,2,3}^{\ast },\text{ }\left( say\right) ,
\end{eqnarray*}%
where $X_{1}=\Gamma D_{\mu }/\sqrt{n}+FD_{\kappa }/\sqrt{n}$, $M^{\left(
Z,Q\right) }=M^{Q}-P^{Z^{\perp }}$, $P^{Z^{\perp }}=M^{Q}Z\left( Z^{\prime
}M^{Q}Z\right) ^{-1}Z^{\prime }M^{Q}$, $M^{Q}=I_{m_{n}}-Q\left( Q^{\prime
}Q\right) ^{-1}Q^{\prime }$, and $F=\left( f\left( W_{1,\left( 1,1\right)
}\right) ,..,f\left( W_{1,\left( 1,T_{1}\right) }\right) ,...,f\left(
W_{1,\left( n,1\right) }\right) ,..,f\left( W_{1,\left( n,T_{n}\right)
}\right) \right) ^{\prime }$. By applying Assumptions 3(i), 4(i), 5, 7(ii),
and 7(iii) and part (a) of Lemma S2-1, it is straightforward to show that $%
\dsum\nolimits_{\left( i,t\right) ,\left( j,s\right) =1,\left( i,t\right)
\neq \left( j,s\right) }^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2} $

\noindent $\times \dsum\nolimits_{v=1}^{T_{j}}e_{\left( j,v\right) }^{\prime
}M^{\left( Z,Q\right) }X_{1}X_{1}^{\prime }M^{\left( Z,Q\right) }e_{\left(
j,v\right) }=O_{a.s.}\left( K_{2,n}\max \left\{ \left( \mu _{n}^{\max
}\right) ^{2}K_{2,n}^{-2\varrho _{\gamma }},\left( \kappa _{n}^{\max
}\right) ^{2}K_{1,n}^{-2\varrho _{f}}\right\} \right) $\footnote{%
Details are available from the authors upon request.}. Making use of this
result as well as Assumptions 1, 2(i), 5, and 6; we then obtain%
\begin{eqnarray*}
0 &\leq &E\left[ \mathcal{A}_{1,2,1}^{\ast }|\mathcal{F}_{n}^{W}\right] \\
&\leq &\frac{24}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \\
&&\times e_{\left( j,v\right) }^{\prime }M^{\left( Z,Q\right)
}X_{1}X_{1}^{\prime }M^{\left( Z,Q\right) }e_{\left( j,v\right) }e_{\left(
i,h\right) }^{\prime }M^{Q}E\left[ \varepsilon \varepsilon ^{\prime }|%
\mathcal{F}_{n}^{W}\right] M^{Q}e_{\left( i,h\right) } \\
&\leq &24\overline{T}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right)
\leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right) }\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}\left\vert
M_{\left( i,t\right) ,\left( i,t\right) }^{Q}\right\vert \right) \left(
\max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) \\
&&\times \frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{v=1}^{T_{j}}e_{\left( j,v\right) }^{\prime }M^{\left(
Z,Q\right) }X_{1}X_{1}^{\prime }M^{\left( Z,Q\right) }e_{\left( j,v\right) }
\\
&=&O_{a.s.}\left( \max \left\{ \frac{\left( \mu _{n}^{\max }\right) ^{2}}{%
K_{2,n}^{2\varrho _{\gamma }}},\frac{\left( \kappa _{n}^{\max }\right) ^{2}}{%
K_{1,n}^{2\varrho _{f}}}\right\} \right) =O_{a.s.}\left( 1\right) \text{
(given Assumption 7(ii) and (iii)).}
\end{eqnarray*}%
Moreover, applying part (a) of Lemma S2-1 and Assumptions 2(i), 5, and 6; we
get%
\begin{eqnarray*}
0 &\leq &E\left[ \mathcal{A}_{1,2,2}^{\ast }|\mathcal{F}_{n}^{W}\right] \\
&\leq &\frac{48}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert
\dsum\limits_{g=1}^{T_{i}}\dsum\limits_{r=1}^{T_{i}}\left\vert M_{\left(
i,h\right) ,\left( i,g\right) }^{Q}\right\vert \left\vert M_{\left(
i,h\right) ,\left( i,r\right) }^{Q}\right\vert \\
&&\times \dsum\limits_{q=1}^{T_{j}}\dsum\limits_{c=1}^{T_{j}}\left\vert
M_{\left( j,v\right) ,\left( j,q\right) }^{Q}\right\vert \left\vert
M_{\left( j,v\right) ,\left( j,c\right) }^{Q}\right\vert E\left[ \left\vert
\varepsilon _{\left( i,g\right) }\varepsilon _{\left( i,r\right) }U_{\left(
j,q\right) }^{\prime }U_{\left( j,c\right) }\right\vert |\mathcal{F}_{n}^{W}%
\right] \\
&\leq &48\overline{T}^{6}\left( \max_{1\leq \left( i,t\right) ,\left(
j,s\right) \leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right)
}\right\vert \right) ^{2}\left( \max_{1\leq \left( i,t\right) ,\left(
j,s\right) \leq m_{n}}\left\vert M_{\left( i,t\right) ,\left( j,s\right)
}^{Q}\right\vert \right) ^{4}\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \right) ^{1/2} \\
&&\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert
U_{\left( i,t\right) }\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}\right]
\right) ^{1/2}\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2} \\
&=&O_{a.s.}\left( 1\right) \text{.}
\end{eqnarray*}%
In addition, for $E\left[ \mathcal{A}_{1,2,3}^{\ast }|\mathcal{F}_{n}^{W}%
\right] $, we have, by straightforward calculations using Assumption 1 and
the triangle inequality,%
\begin{eqnarray}
0 &\leq &E\left[ \mathcal{A}_{1,2,3}^{\ast }|\mathcal{F}_{n}^{W}\right] 
\notag \\
&\leq &\frac{48}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{
\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert J_{\left(
i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left( j,s\right)
,\left( j,v\right) }\right\vert \right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times
\dsum\limits_{g=1}^{T_{i}}\left( M_{\left( i,h\right) ,\left( i,g\right)
}^{Q}\right) ^{2}\left( P_{\left( j,v\right) ,\left( i,g\right) }^{Z^{\perp
}}\right) ^{2}E\left[ \varepsilon _{\left( i,g\right) }^{2}U_{\left(
i,g\right) }^{\prime }U_{\left( i,g\right) }|\mathcal{F}_{n}^{W}\right]
\right\}  \notag \\
&&\text{ }+\frac{48}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{
\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert J_{\left(
i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left( j,s\right)
,\left( j,v\right) }\right\vert \dsum\limits_{g=1}^{T_{i}}\left( M_{\left(
i,h\right) ,\left( i,g\right) }^{Q}\right) ^{2}\right.  \notag \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ }\left. \times
\dsum\limits_{\left( k,q\right) =1}^{m_{n}}\left( P_{\left( j,v\right)
,\left( k,q\right) }^{Z^{\perp }}\right) ^{2}E\left[ \varepsilon _{\left(
i,g\right) }^{2}|\mathcal{F}_{n}^{W}\right] E\left[ U_{\left( k,q\right)
}^{\prime }U_{\left( k,q\right) }|\mathcal{F}_{n}^{W}\right] \right\}  \notag
\\
&&\text{ }+\frac{96}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right)
,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left\{
\dsum\limits_{h=1}^{T_{i}}\left\vert J_{\left( i,t\right) ,\left( i,h\right)
}\right\vert \dsum\limits_{g=1}^{T_{i}}\dsum\limits_{r=1}^{T_{i}}\left\vert
M_{\left( i,h\right) ,\left( i,g\right) }^{Q}\right\vert \left\vert
M_{\left( i,h\right) ,\left( i,r\right) }^{Q}\right\vert \right.  \notag \\
&&\text{ \ \ \ \ \ \ }\left. \times \dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( j,s\right) ,\left( j,v\right) }\right\vert \left\vert P_{\left(
j,v\right) ,\left( i,g\right) }^{Z^{\perp }}\right\vert \left\vert P_{\left(
j,v\right) ,\left( i,r\right) }^{Z^{\perp }}\right\vert E\left[ \left\vert
\varepsilon _{\left( i,g\right) }\varepsilon _{\left( i,r\right) }U_{\left(
i,g\right) }^{\prime }U_{\left( i,r\right) }\right\vert |\mathcal{F}_{n}^{W}%
\right] \right\}  \label{EA*123 bd}
\end{eqnarray}%
Applying the CS inequality to (\ref{EA*123 bd}) and making use of part (a)
of Lemma S2-1 as well as Assumptions 2(i), 5, and 6; we further obtain%
\begin{eqnarray*}
&&E\left[ \mathcal{A}_{1,2,3}^{\ast }|\mathcal{F}_{n}^{W}\right] \\
&\leq &48\overline{T}^{3}\left( \max_{1\leq \left( i,t\right) ,\left(
j,s\right) \leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right)
}\right\vert \right) ^{2}\left( \max_{1\leq \left( i,t\right) ,\left(
j,s\right) \leq m_{n}}\left\vert M_{\left( i,t\right) ,\left( j,s\right)
}^{Q}\right\vert \right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}E\left[ \varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}%
\right] \right) ^{1/2} \\
&&\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \left\Vert
U_{\left( i,t\right) }\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}\right]
\right) ^{1/2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}P_{\left(
i,t\right) ,\left( i,t\right) }^{Z^{\perp }}\right) ^{2}\frac{1}{K_{2,n}}%
\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2} \\
&&+48\overline{T}^{3}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right)
\leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right) }\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right) \leq
m_{n}}\left\vert M_{\left( i,t\right) ,\left( j,s\right) }^{Q}\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) \\
&&\text{ \ }\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\left\Vert U_{\left( i,t\right) }\right\Vert _{2}^{2}|\mathcal{F}_{n}^{W}%
\right] \right) \left( \max_{1\leq \left( i,t\right) \leq m_{n}}P_{\left(
i,t\right) ,\left( i,t\right) }^{Z^{\perp }}\right) \frac{1}{K_{2,n}}%
\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}
\end{eqnarray*}
\begin{eqnarray*}
&&+96\overline{T}^{4}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right)
\leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right) }\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) ,\left( j,s\right) \leq
m_{n}}\left\vert M_{\left( i,t\right) ,\left( j,s\right) }^{Q}\right\vert
\right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\varepsilon _{\left( i,t\right) }^{4}|\mathcal{F}_{n}^{W}\right] \right)
^{1/2} \\
&&\text{ \ \ }\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[
\left\Vert U_{\left( i,t\right) }\right\Vert _{2}^{4}|\mathcal{F}_{n}^{W}%
\right] \right) ^{1/2}\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}P_{\left( i,t\right) ,\left( i,t\right) }^{Z^{\perp }}\right) ^{2}%
\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2} \\
&=&O_{a.s.}\left( \frac{K_{n}^{2}}{n^{2}}\right) +O_{a.s.}\left( \frac{K_{n}%
}{n}\right) +O_{a.s.}\left( \frac{K_{n}^{2}}{n^{2}}\right) =O_{a.s.}\left( 
\frac{K_{n}}{n}\right) =o_{a.s.}\left( 1\right) .
\end{eqnarray*}%
These calculations imply that $0\leq E\left[ \mathcal{A}_{1,2}^{\ast }|%
\mathcal{F}_{n}^{W}\right] \leq E\left[ \mathcal{A}_{1,2,1}^{\ast }|\mathcal{%
F}_{n}^{W}\right] +E\left[ \mathcal{A}_{1,2,2}^{\ast }|\mathcal{F}_{n}^{W}%
\right] +E\left[ \mathcal{A}_{1,2,3}^{\ast }|\mathcal{F}_{n}^{W}\right] $

\noindent $=O_{a.s.}\left( 1\right) +O_{a.s.}\left( 1\right) +o_{a.s.}\left(
1\right) =O_{a.s.}\left( 1\right) $. It follows by the law of iterated
expectations and Theorem 16.1 of Billingsley (1995) that there exists a
constant $\overline{C}<\infty $ such that for all $n$ sufficiently large $E%
\left[ \mathcal{A}_{1,2}^{\ast }\right] =E_{W_{n}}\left( E\left[ \mathcal{A}%
_{1,2,}^{\ast }|\mathcal{F}_{n}^{W}\right] \right) \leq \overline{C}$.
Markov's inequality then implies that%
\begin{equation}
\mathcal{A}_{1,2}^{\ast }=O_{p}\left( 1\right) .  \label{A*12}
\end{equation}%
from which we further deduce that 
\begin{equation}
\mathcal{A}_{1,2}\leq \left\Vert \widehat{\delta }_{n}-\delta
_{0}\right\Vert _{2}^{2}\mathcal{A}_{1,2}^{\ast }=o_{p}\left( 1\right)
O_{p}\left( 1\right) =o_{p}\left( 1\right) \text{.}  \label{A12}
\end{equation}%
\qquad

\noindent \qquad Turning our attention to $\mathcal{A}_{1,3}$, observe that%
\begin{eqnarray*}
\mathcal{A}_{1,3} &=&\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left(
i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left(
j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}\left[ e_{\left( j,v\right)
}^{\prime }M^{\left( Z,Q\right) }\varphi _{n}\right] ^{2} \\
&=&\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2} \\
&&\text{ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\ \ \ \ \ \ \ \ }\times \left[ \frac{\tau _{n}}{\sqrt{n}}e_{\left(
j,v\right) }^{\prime }M^{\left( Z,Q\right) }\left( g-Z_{1}\theta
^{K_{1,n}}\right) \right] ^{2} \\
&\leq &\frac{m_{n}\tau _{n}^{2}}{n}\left\Vert g\left( \cdot \right) -\theta
^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert _{\infty }^{2}%
\mathcal{A}_{1,3}^{\ast }
\end{eqnarray*}%
where $\mathcal{A}_{1,3}^{\ast }=12K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\nolimits_{h=1}^{T_{i}}\dsum\nolimits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert \left( e_{\left( i,h\right)
}^{\prime }M^{Q}\varepsilon \right) ^{2}$ and where $g=\left( g\left(
W_{1,\left( 1,1\right) }\right) ,..,g\left( W_{1,\left( 1,T_{1}\right)
}\right) ,...,g\left( W_{1,\left( n,1\right) }\right) ,..,g\left(
W_{1,\left( n,T_{n}\right) }\right) \right) ^{\prime }$. Next, note that, by
Lemma S2-1(a) and Assumptions 1, 2(i), 5, and 6,%
\begin{eqnarray}
\text{ }E\left[ \mathcal{A}_{1,3}^{\ast }|\mathcal{F}_{n}^{W}\right] &=&%
\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{i}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert E\left[ \left( e_{\left(
i,h\right) }^{\prime }M^{Q}\varepsilon \right) ^{2}|\mathcal{F}_{n}^{W}%
\right]  \notag \\
&=&\frac{12}{K_{2,n}}\dsum\limits_{\substack{ \left( i,t\right) ,\left(
j,s\right) =1  \\ \left( i,t\right) \neq \left( j,s\right) }}%
^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\dsum\limits_{h=1}^{T_{i}}\dsum\limits_{v=1}^{T_{j}}\left\vert
J_{\left( i,t\right) ,\left( i,h\right) }\right\vert \left\vert J_{\left(
j,s\right) ,\left( j,v\right) }\right\vert e_{\left( i,h\right) }^{\prime
}M^{Q}E\left[ \varepsilon \varepsilon ^{\prime }|\mathcal{F}_{n}^{W}\right]
M^{Q}e_{\left( i,h\right) }  \notag \\
&\leq &12\overline{T}^{2}\left( \max_{1\leq \left( i,t\right) ,\left(
j,s\right) \leq m_{n}}\left\vert J_{\left( i,t\right) ,\left( j,s\right)
}\right\vert \right) ^{2}\left( \max_{1\leq \left( i,t\right) \leq
m_{n}}M_{\left( i,t\right) ,\left( i,t\right) }^{Q}\right)  \notag \\
&&\times \left( \max_{1\leq \left( i,t\right) \leq m_{n}}E\left[ \varepsilon
_{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right) \frac{1}{K_{2,n}%
}\dsum\limits_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left(
i,t\right) \neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left(
j,s\right) }^{2}  \notag \\
&=&O_{a.s.}\left( 1\right)  \label{EA*13}
\end{eqnarray}%
It follows by the law of iterated expectations and Theorem 16.1 of
Billingsley (1995) that there exists a constant $\overline{C}<\infty $ such
that for all $n$ sufficiently large $E\left[ \mathcal{A}_{1,3}^{\ast }\right]
=E_{W_{n}}\left( E\left[ \mathcal{A}_{1,3}^{\ast }|\mathcal{F}_{n}^{W}\right]
\right) $

\noindent $\leq \overline{C}$. Markov's inequality then implies that $%
\mathcal{A}_{1,3}^{\ast }=O_{p}\left( 1\right) $, from which we further
deduce using Assumption 7(i) that%
\begin{equation}
\mathcal{A}_{1,3}\leq \frac{m_{n}}{n}\tau _{n}^{2}\left\Vert g\left( \cdot
\right) -\theta ^{K_{1,n}\prime }Z_{1}\left( \cdot \right) \right\Vert
_{\infty }^{2}\mathcal{A}_{1,3}^{\ast }=O_{p}\left( \frac{\tau _{n}^{2}}{%
K_{1,n}^{2\varrho _{g}}}\right) =o_{p}\left( 1\right) \text{. }  \label{A13}
\end{equation}

\noindent \qquad Putting things together, we note that the results given in
expressions (\ref{A11}), (\ref{A12}), and (\ref{A13}) imply that 
\begin{equation*}
\mathcal{A}_{1}=\mathcal{A}_{1,1}+\mathcal{A}_{1,2}+\mathcal{A}%
_{1,3}=O_{p}\left( \frac{K_{n}}{n}\right) +o_{p}\left( 1\right) +o_{p}\left(
1\right) =o_{p}\left( 1\right) \text{. \ }
\end{equation*}

By a similar method of proof, we can also show that $\mathcal{A}%
_{k}=o_{p}\left( 1\right) $ for $k=2,...,6$. For the sake of brevity, we
will not provide detailed proofs for these other terms. Detailed argument
for these other terms can be obtained from the authors upon request. It then
follows from equation (\ref{decomposition into A terms}) that 
\begin{eqnarray}
\mathcal{A} &=&\frac{S_{3}}{K_{2,n}}-\frac{1}{K_{2,n}}\dsum\limits 
_{\substack{ \left( i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right)
\neq \left( j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right)
}^{2}\varepsilon _{\left( i,t\right) }^{2}\varepsilon _{\left( j,s\right)
}^{2}  \notag \\
&=&\mathcal{A}_{1}+\mathcal{A}_{2}+\mathcal{A}_{3}+\mathcal{A}_{4}+\mathcal{A%
}_{5}+\mathcal{A}_{6}=o_{p}\left( 1\right) \text{.}  \label{Result for A}
\end{eqnarray}

Now, for the term $\mathfrak{A}=K_{2,n}^{-1}\dsum\nolimits_{\left(
i,t\right) ,\left( j,s\right) =1,\left( i,t\right) \neq \left( j,s\right)
}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\left( \varepsilon
_{\left( i,t\right) }^{2}\varepsilon _{\left( j,s\right) }^{2}-\sigma
_{\left( i,t\right) }^{2}\sigma _{\left( j,s\right) }^{2}\right) $, it can
be shown by straightforward calculation and by using Assumptions 1, 2(i), 5
and 6 and Lemma S2-1(a) that $E\left[ \mathfrak{A}^{2}|\mathcal{F}_{n}^{W}%
\right] =O_{a.s.}\left( n^{-1}\right) $.\footnote{%
For the sake of brevity, we have not provided the details of all the
calculations here. A detailed argument can be obtained from the authors upon
request.} It then follows by application of the law of iterated
expectations, Theorem 16.1 of Billingsley (1995), and the Markov's
inequality that 
\begin{equation}
\mathfrak{A}=O_{p}\left( \frac{1}{\sqrt{n}}\right) \text{.}
\label{Result for A Fraktur}
\end{equation}%
Combining (\ref{Result for A}) and (\ref{Result for A Fraktur}), we get%
\begin{equation*}
\frac{S_{3}}{K_{2,n}}-\frac{1}{K_{2,n}}\dsum\limits_{\substack{ \left(
i,t\right) ,\left( j,s\right) =1  \\ \left( i,t\right) \neq \left(
j,s\right) }}^{m_{n}}A_{\left( i,t\right) ,\left( j,s\right) }^{2}\sigma
_{\left( i,t\right) }^{2}\sigma _{\left( j,s\right) }^{2}=\mathcal{A}+%
\mathfrak{A}=o_{p}\left( 1\right) +O_{p}\left( \frac{1}{\sqrt{n}}\right)
=o_{p}\left( 1\right) \text{,}
\end{equation*}%
which shows part (b).

The proofs for parts (c) and (d) are similar to that of part (b). Hence, for
the sake of brevity, we will not provide detailed proofs of these parts
here. Detailed proofs of these parts can be obtained from the authors upon
request.

Turning our attention to part (e), note first that, we can write%
\begin{eqnarray*}
\widehat{\rho }_{n}-\rho &=&\frac{X^{\prime }M^{\left( Z,Q\right) }\left( y-X%
\widehat{\delta }_{n}\right) /n}{\left( y-X\widehat{\delta }_{n}\right)
^{\prime }M^{\left( Z,Q\right) }\left( y-X\widehat{\delta }_{n}\right) /n}%
-\rho \\
&=&\frac{X^{\prime }M^{\left( Z,Q\right) }\left( y-X\widehat{\delta }%
_{n}\right) /n-U^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon /n+U^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon /n}{\left( y-X\widehat{\delta }%
_{n}\right) ^{\prime }M^{\left( Z,Q\right) }\left( y-X\widehat{\delta }%
_{n}\right) /n-\varepsilon ^{\prime }M^{Q}\varepsilon /n+\varepsilon
^{\prime }M^{Q}\varepsilon /n}-\rho
\end{eqnarray*}%
where $\rho =\lim_{n\rightarrow \infty }\rho _{n}=\lim_{n\rightarrow \infty
}\left( E\left[ U^{\prime }M^{Q}\varepsilon \right] /n\right) /\left( E\left[
\varepsilon ^{\prime }M^{Q}\varepsilon \right] /n\right) $. By
straightforward asymptotic analysis\footnote{%
Further details are available from the authors upon request.}, we can show
that%
\begin{eqnarray*}
\frac{X^{\prime }M^{\left( Z,Q\right) }\left( y-X\widehat{\delta }%
_{n}\right) }{n}-\frac{U^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{n}
&=&o_{p}\left( 1\right) \text{,} \\
\frac{\left( y-X\widehat{\delta }_{n}\right) ^{\prime }M^{\left( Z,Q\right)
}\left( y-X\widehat{\delta }_{n}\right) }{n}-\frac{\varepsilon ^{\prime
}M^{\left( Z_{1},Q\right) }\varepsilon }{n} &=&o_{p}\left( 1\right) \text{,}
\\
\frac{\varepsilon ^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{n}-\frac{%
\varepsilon ^{\prime }M^{Q}\varepsilon }{n} &=&o_{p}\left( 1\right) \text{,}
\end{eqnarray*}%
\begin{equation*}
\frac{U^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon }{\varepsilon
^{\prime }M^{Q}\varepsilon }-\rho =O_{p}\left( \max \left\{ \frac{1}{\sqrt{n}%
},\frac{K_{1,n}}{n}\right\} \right) =o_{p}\left( 1\right) \text{.}
\end{equation*}%
Next, note that, under Assumption 6(i), $T_{i}\geq 3$ for all $i$, so that $%
\frac{T_{i}-1}{T_{i}}\geq \frac{2}{3}$ \ for all $i$. Hence, by Assumption
2(ii), there exists a positive constant $\underline{C}$ such that $E\left[
\varepsilon _{\left( i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \geq 
\underline{C}>0$ $a.s.$, so that%
\begin{eqnarray*}
\frac{E\left[ \varepsilon ^{\prime }M^{Q}\varepsilon \right] }{n} &=&\frac{1%
}{n}E_{W_{n}}\left\{ \dsum\limits_{i=1}^{n}\dsum\limits_{t=1}^{T_{i}}\left[
\left( \frac{T_{i}-1}{T_{i}}\right) \right] E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right\} \\
&\geq &\frac{2}{3n}E_{W_{n}}\left\{
\dsum\limits_{i=1}^{n}\dsum\limits_{t=1}^{T_{i}}E\left[ \varepsilon _{\left(
i,t\right) }^{2}|\mathcal{F}_{n}^{W}\right] \right\} \\
&\geq &\frac{2}{3n}E_{W_{n}}\left[ \dsum\limits_{i=1}^{n}\dsum%
\limits_{t=1}^{T_{i}}\underline{C}\right] =\frac{2}{3}\frac{m_{n}}{n}%
\underline{C}\text{ \ }\left( \text{since }m_{n}=\dsum\limits_{i=1}^{n}T_{i}%
\text{ }\right) \\
&\geq &\frac{2}{3}\underline{C}>0
\end{eqnarray*}%
for all $n$ sufficiently large. It follows from these results that $\widehat{%
\rho }_{n}-\rho =\left[ U^{\prime }M^{\left( Z_{1},Q\right) }\varepsilon
/\left( \varepsilon ^{\prime }M^{Q}\varepsilon \right) \right] -\rho
+o_{p}\left( 1\right) =o_{p}\left( 1\right) $ and $\left\Vert \widehat{\rho }%
_{n}\right\Vert _{2}\leq \left\Vert \widehat{\rho }_{n}-\rho \right\Vert
_{2}+\left\Vert \rho \right\Vert _{2}=O_{p}\left( 1\right) $.

Now, let $a\in \mathbb{R}^{d}$ such that $\left\Vert a\right\Vert _{2}=1$;
and note that, by applying the CS inequality, we have%
\begin{eqnarray}
\left\vert a^{\prime }D_{\mu }^{-1}\widehat{\rho }_{n}\right\vert &\leq &%
\frac{1}{\left( \mu _{n}^{\min }\right) }\left\Vert \widehat{\rho }%
_{n}\right\Vert _{2}=O_{p}\left( \frac{1}{\left( \mu _{n}^{\min }\right) }%
\right) \text{,}  \label{normalized rho hat} \\
\left\vert a^{\prime }D_{\mu }^{-1}\left( \widehat{\rho }_{n}-\rho \right)
\right\vert &\leq &\frac{1}{\left( \mu _{n}^{\min }\right) }\left\Vert 
\widehat{\rho }_{n}-\rho \right\Vert _{2}=o_{p}\left( \frac{1}{\left( \mu
_{n}^{\min }\right) }\right) ,  \label{centered rho hat}
\end{eqnarray}%
Since the argument above holds for any $a\in \mathbb{R}^{d}$ such that $%
\left\Vert a\right\Vert _{2}=1$, we further deduce that $D_{\mu }^{-1}%
\widehat{\rho }_{n}=O_{p}\left( \left( \mu _{n}^{\min }\right) ^{-1}\right) $
and $D_{\mu }^{-1}\left( \widehat{\rho }_{n}-\rho \right) =o_{p}\left(
\left( \mu _{n}^{\min }\right) ^{-1}\right) $, which shows part (e).

Part (f) can be shown by applying the results of parts (b), (c), (d), and
(e) of this lemma as well as part (a) of Lemma S2-1 and Assumptions 2(i) and
3(ii). Part (g), on the other hand, can be proved by applying the results of
parts (b), (d), and (e) of this lemma. For the sake of brevity, we will not
provide detailed proofs of these parts here, but proofs of these parts can
be obtained from the authors upon request. $\square $

\begin{thebibliography}{9}
\bibitem{} Billingsley, P. (1995). \textit{Probability and Measure. }New
York: John Wiley \& Sons.

\bibitem{} G\"{a}nsler, P. and W. Stute (1977). \textit{%
Wahrscheinlichkeitstheorie}. New York: Springer-Verlag.

\bibitem{} Hall, P. and C. Heyde (1980). \textit{Martingale Limit Theory and
Its Applications. }New York: Academic Press.
\end{thebibliography}

\end{document}
