%2multibyte Version: 5.50.0.2960 CodePage: 936

\documentclass[final,notitlepage]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{caption}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.50.0.2960}
%TCIDATA{Codepage=936}
%TCIDATA{<META NAME="SaveForMode" CONTENT="1">}
%TCIDATA{BibliographyScheme=Manual}
%TCIDATA{Created=Wed May 15 17:28:15 2002}
%TCIDATA{LastRevised=Monday, August 24, 2020 11:35:14}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{<META NAME="DocumentShell" CONTENT="General\Blank Document">}
%TCIDATA{Language=American English}
%TCIDATA{CSTFile=LaTeX article (bright).cst}
%TCIDATA{PageSetup=65,65,72,72,0}
%TCIDATA{AllPages=
%H=36
%F=29,\PARA{038<p type="texpara" tag="Body Text" > \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  }
%}


\newtheorem{theorem}{Theorem}[section]
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
\input{tcilatex}
\textwidth=16.0cm
\oddsidemargin=0cm \evensidemargin=0cm
\topmargin=-20pt
\numberwithin{equation}{section}
\baselineskip=100pt
\textheight=21cm
\def\baselinestretch{1.2}
\begin{document}

\title{}

\begin{center}
{\LARGE \vspace{1in} Supplemental Appendix: Robust Forecast Superiority
Testing with an Application to Assessing Pools of Expert Forecasters} *

{\Large Valentina Corradi}$^{1}${\Large , Sainan Jin}$^{2},$ {\Large and
Norman R. Swanson}$^{3}\medskip $

$^{1}$University of Surrey, $^{2}$Singapore Management University, and $^{3}$%
Rutgers University

\bigskip

August 2020

\bigskip

Abstract
\end{center}

\noindent {In this supplemental appendix, additional theoretical results are
presented for the case of recursive estimation error. Additionally,
auxiliary Monte Carlo experiments are reported on, and further empirical
results are tabulated based on the empirical illustration in the paper. }

{\small \bigskip \bigskip \bigskip }

\noindent 

\noindent \textit{Keywords}: Robust Forecast Evaluation, Many Moment
Inequalities, Bootstrap, Estimation Error, Combination Forecasts, Survey of
Professional Forecasters.

\noindent \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

\noindent *{\footnotesize Valentina Corradi, School of Economics, University
of Surrey, Guildford, Surrey, GU2 7XH, UK, v.corradi@surrey.ac.uk; Jin
Sainan, School of \ Economics, Singapore Management University, 90 Stamford
Road, \#05-30, Singapore 178903, snjin@smu.edu.sg; and Norman R. Swanson,
Department of Economics, Rutgers University, 75 Hamilton Street, New
Brunswick, NJ 08901, USA, nswanson@econ.rutgers.edu. We are grateful to
Kevin Lee, Patrick Marsh, Luis Martins, Jams Mitchell, Alessia Paccagini,
Paulo Parente, Ivan Petrella, Valerio Poti, Barbara Rossi, Simon Van Norden,
Claudio Zoli, and to the participants at the 2018 NBER-NSF Times Series
Conference, the 2016 European Meeting of the Econometric Society, Conference
for 50 years of Keynes College at Kent University, and seminars at Mannheim
University, the University of Nottingham, University College Dublin,
Instituto Universit\'{a}rio de Lisboa, Universita' di Verona and the Warwick
Business School for useful comments and suggestions. Additionally, many
thanks are owed to Mingmian Cheng for excellent research assistance.}

\setcounter{page}{0} \thispagestyle{empty}

{\small \newpage }

\section{Forecast Superiority Tests in the Presence of Recursive Estimation
Error}

\subsection{\noindent The Statistic}

This Supplement extends all Lemmas and Theorems in the paper to the case in
which there is non vanishing, recursive estimation error.

Let $T=R+n.$ At each point in time, $t>R,$ update model parameter estimates
prior to the construction of each new forecast, using all the available
information.\footnote{%
In the rolling estimation case, we use only the most recent $R$ observations
to re-estiamte the forecasdting model, for each $t>R.$ The rolling case can
be treated analogously, and it is omitted only for brevity.}

For $j=1,...,k,$ use the first $R$ observations to compute $\widehat{\theta }%
_{j,R},$ and construct the first prediction error: 
\begin{equation*}
\widehat{e}_{j,R+1}=X_{R+1}-\phi _{j}\left( Z_{j,R},\widehat{\theta }%
_{j,R}\right) ,
\end{equation*}%
where $Z_{j,R}$ contains lags of $X$ as well as other regressors. Then, use
the first $R+1$ observations to construct%
\begin{equation*}
\widehat{e}_{j,R+2}=X_{R+2}-\phi _{j}\left( Z_{j,R+1},\widehat{\theta }%
_{j,R+1}\right) .
\end{equation*}%
Proceed in the same manner until a sequence of $n$ prediction errors has
been constructed, defined as: 
\begin{equation}
\widehat{e}_{j,t+1}=X_{t+1}-\phi _{j}\left( Z_{j,t},\widehat{\theta }%
_{j,t}\right) ,  \label{e-hat}
\end{equation}%
for $t=R,...,R+n-1,$ where $\widehat{\theta }_{j,t}$ is the estimator
computed using observations up to time $t.$ In the sequel, assume that $%
\widehat{\theta }_{j,t}$ is a recursive $m-$estimator, so that:%
\begin{equation}
\widehat{\theta }_{j,t}=\arg \min_{\theta _{j}\in \Theta _{j}}\frac{1}{t}%
\sum_{i=2}^{t}m_{j}(X_{i},Z_{j,i-1},\theta _{j}),\text{ \ \ }R\leq t\leq n-1,%
\text{ }j=1,...,k,  \label{theta-i}
\end{equation}%
and%
\begin{equation*}
\theta _{j}^{\dagger }=\arg \min_{\theta _{j}\in \Theta
_{j}}E(m_{j}(X_{i},Z_{j,i-1},\theta _{j})).
\end{equation*}%
For $x\geq 0,$ define:%
\begin{equation}
\widetilde{G}_{j,n}^{+}(x)=\frac{1}{n}\sum_{t=R}^{T-1}\left( 1\left\{ 
\widehat{e}_{j,t+1}\leq x\right\} -1\left\{ \widehat{e}_{1,t+1}\leq
x\right\} \right) =\left( \widetilde{F}_{j,n}(x)-\widetilde{F}%
_{1,n}(x)\right)  \label{Gtilde}
\end{equation}%
and%
\begin{eqnarray}
\widetilde{C}_{j,n}^{+}(x) &=&\int_{x}^{\infty }\left( \widetilde{F}%
_{j,n}(t)-\widetilde{F}_{1,n}(t)\right) dt  \notag \\
&=&\frac{1}{n}\sum_{t=R}^{T-1}\left\{ \left[ \left( \widehat{e}%
_{1,t+1}-x\right) \right] _{+}-\left[ \left( \widehat{e}_{j,t+1}-x\right) %
\right] _{+}\right\} .  \label{Ctilde}
\end{eqnarray}%
Define the following forecast superiority test statistics:%
\begin{equation*}
\widetilde{S}_{n}^{G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max
\left\{ 0,\frac{\sqrt{n}\widetilde{G}_{j,n}^{+}(x)}{\widetilde{\sigma }%
_{jj,n}^{G+}(x)+\varepsilon }\right\} \right) ^{2}\mathrm{d}Q(x)
\end{equation*}%
and%
\begin{equation*}
\widetilde{S}_{n}^{C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max
\left\{ 0,\frac{\sqrt{n}\widetilde{C}_{j,n}^{+}(x)}{\widetilde{\sigma }%
_{jj,n}^{C+}(x)+\varepsilon }\right\} \right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
where $\widetilde{\sigma }_{jj,n}^{G+}(x)$ and $\widetilde{\sigma }%
_{jj,n}^{C+}(x)$ include terms accounting for the contribution of parameter
estimation error to asymptotic covariance. Here, $\widetilde{\sigma }%
_{jj,n}^{2,G+}(x)$ is defined as:%
\begin{eqnarray*}
\widetilde{\sigma }_{jj,n}^{2,G+}(x) &=&\widehat{\sigma }_{jj,n}^{2,G+}(x)+2%
\widehat{\Pi }\widehat{f}_{1,n,h}^{2}(x)\widehat{A}_{1}\widehat{\Sigma }_{11}%
\widehat{A}_{1}^{\prime }+2\widehat{\Pi }\widehat{f}_{j,n,h}^{2}(x)\widehat{A%
}_{j}\widehat{\Sigma }_{jj}\widehat{A}_{j}^{\prime } \\
&&-4\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widehat{A}_{1}\widehat{\Sigma }_{1j}%
\widehat{A}_{j}^{\prime }\widehat{f}_{j,n,h}(x)+2\widehat{\Pi }\widehat{f}%
_{1,n,h}(x)\widehat{A}_{1}\widehat{\Sigma }_{u1}(x)-2\widehat{\Pi }\widehat{f%
}_{j,n,h}(x)\widehat{A}_{j}\widehat{\Sigma }_{uj}(x),
\end{eqnarray*}%
where $\widehat{\sigma }_{jj,n}^{2,G+}(x)$ is defined as in the text, but
using only the last $n$ observations, $\widehat{\Pi }=1-\frac{R}{n}\ln
\left( 1+\frac{n}{R}\right) ,$%
\begin{equation*}
\widehat{f}_{j,n,h}(x)=\frac{1}{nh}\sum_{t=R+1}^{n}K\left( \frac{\widehat{e}%
_{j,t}-x}{h}\right) ,
\end{equation*}%
\begin{equation*}
\widehat{A}_{j}=\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t+1},\widehat{\theta }_{j,R}\right) ^{\prime }\left( \frac{1%
}{R}\sum_{t=1}^{R}\nabla _{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{%
\theta }_{j,R})\right) ^{-1},
\end{equation*}%
\begin{eqnarray*}
\widehat{\Sigma }_{jj} &=&\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})^{\prime } \\
&&+2\frac{1}{n}\sum_{\tau =1}^{l_{n}}\sum_{t=R+\tau +1}^{T}w_{\tau }\nabla
_{\theta _{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\nabla _{\theta
_{j}}m_{j}(X_{t-\tau },Z_{t-\tau ,i-1},\widehat{\theta }_{j,R})^{\prime },
\end{eqnarray*}%
and%
\begin{eqnarray*}
\widehat{\Sigma }_{uj}(x) &=&\frac{1}{n}\sum_{t=R+1}^{T}\nabla _{\theta
_{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\left( \left( 1\left\{ 
\widehat{e}_{j,t}\leq x\right\} -\frac{1}{n}\sum_{t=1}^{n}1\left\{ \widehat{e%
}_{j,t}\leq x\right\} \right) \right. \\
&&\left. -\left( 1\left\{ \widehat{e}_{1,t}\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{1,t}\leq x\right\} \right) \right) \\
&&+2\frac{1}{n}\sum_{\tau =1}^{l_{n}}\sum_{t=R+\tau +1}^{T}w_{\tau }\nabla
_{\theta _{j}}m_{j}(X_{t},Z_{t,i-1},\widehat{\theta }_{j,R})\left( \left(
1\left\{ \widehat{e}_{j,t-\tau }\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{j,t-\tau }\leq x\right\} \right) \right.
\\
&&\left. -\left( 1\left\{ \widehat{e}_{1,t-\tau }\leq x\right\} -\frac{1}{n}%
\sum_{t=1}^{n}1\left\{ \widehat{e}_{1,t-\tau }\leq x\right\} \right) \right)
,
\end{eqnarray*}%
By noting that%
\begin{eqnarray}
&&\widetilde{C}_{j,n}^{+}(x)  \notag \\
&=&\frac{1}{n}\sum_{t=R}^{T-1}\left( \left[ \left( e_{1,t+1}-x\right) \right]
_{+}-\left[ \left( e_{j,t+1}-x\right) \right] _{+}\right)  \notag \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( \widehat{e}_{1,t}-e_{1,t}\right)
1\left\{ e_{1,t}\geq x\right\} -\left( \widehat{e}_{j,t}-e_{j,t}\right)
1\left\{ e_{j,t}\geq x\right\} \right)  \notag \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( e_{1,t}-x\right) \left( 1\left\{ 
\widehat{e}_{1,t}\geq x\right\} -1\left\{ \widehat{e}_{1,t}\geq x\right\}
\right) -\left( e_{j,t}-x\right) \left( 1\left\{ \widehat{e}_{j,t}\geq
x\right\} -1\left\{ e_{j,t}\geq x\right\} \right) \right)  \label{C-PEE} \\
&&+\frac{1}{n}\sum_{t=R}^{T-1}\left( \left( \widehat{e}_{1,t}-e_{1,t}\right)
\left( 1\left\{ \widehat{e}_{1,t}\geq x\right\} -1\left\{ e_{1,t}\geq
x\right\} \right) -\left( \widehat{e}_{j,t}-e_{j,t}\right) \left( 1\left\{ 
\widehat{e}_{j,t}\geq x\right\} -1\left\{ e_{j,t}\geq x\right\} \right)
\right)  \notag
\end{eqnarray}%
we see that $\widetilde{\sigma }_{jj,n}^{2,C+}(x)$ is defined as:%
\begin{eqnarray*}
&&\widetilde{\sigma }_{jj,n}^{2,G+}(x) \\
&=&\widehat{\sigma }_{jj,n}^{2,C+}(x)+2\widehat{\Pi }\widehat{f}%
_{1,n,h}^{2}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }_{11}\widetilde{A}%
_{1}^{\prime }(x)+2\widehat{\Pi }\widehat{f}_{j,n,h}^{2}(x)\widetilde{A}%
_{j}(x)\widehat{\Sigma }_{jj}\widetilde{A}_{j}^{\prime }(x) \\
&&-4\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }%
_{1j}\widetilde{A}_{j}^{\prime }(x)\widehat{f}_{j,n,h}(x)+2\widehat{\Pi }%
\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }_{u1}(x)-2%
\widehat{\Pi }\widehat{f}_{j,n,h}(x)\widetilde{A}_{j}(x)\widehat{\Sigma }%
_{uj}(x) \\
&&+2\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{11}\widetilde{B}%
_{1}^{\prime }(x)+2\widehat{\Pi }\widetilde{B}_{j}^{\prime }(x)\widehat{%
\Sigma }_{jj}\widetilde{B}_{j}^{\prime }(x)-4\widehat{\Pi }\widetilde{B}%
_{1}(x)\widehat{\Sigma }_{1j}\widetilde{B}_{j}^{\prime }(x) \\
&&+2\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{u1}(x)-2\widehat{%
\Pi }\widetilde{B}_{j}(x)^{\prime }\widehat{\Sigma }_{uj}(x) \\
&&+2\widehat{\Pi }\widehat{f}_{1,n,h}(x)\widetilde{A}_{1}(x)\widehat{\Sigma }%
_{11}\widetilde{B}_{1}^{\prime }(x)+2\widehat{\Pi }\widehat{f}_{j,n,h}(x)%
\widetilde{A}_{j}\widehat{\Sigma }_{jj}\widetilde{B}_{j}^{\prime }(x)-2%
\widehat{\Pi }\widetilde{B}_{1}(x)\widehat{\Sigma }_{1j}\widetilde{A}%
_{j}^{\prime }(x)\widehat{f}_{j,n,h}(x) \\
&&-2\widehat{\Pi }\widetilde{B}_{j}(x)\widehat{\Sigma }_{1j}\widetilde{A}%
_{1}^{\prime }(x)\widehat{f}_{1,n,h}(x),
\end{eqnarray*}%
where $\widehat{\sigma }_{jj,n}^{2,C+}(x)$ is defined as in the statement of
Lemma 1, but computed using only the last $n$ observations. Also, 
\begin{equation*}
\widetilde{A}_{j}(x)=\frac{1}{n}\sum_{t=R+1}^{T}\left( \widehat{e}%
_{t+1,j}-x\right) \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\widehat{%
\theta }_{j,R}\right) ^{\prime }\left( \frac{1}{R}\sum_{t=1}^{R}\nabla
_{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{\theta }_{j,R})\right) ^{-1}
\end{equation*}%
and%
\begin{equation*}
\widetilde{B}_{j}(x)=\frac{1}{n}\sum_{t=R+1}^{T}1\left\{ \widehat{e}%
_{t+1,j}>x\right\} \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\widehat{%
\theta }_{j,R}\right) ^{\prime }\left( \frac{1}{R}\sum_{t=1}^{R}\nabla
_{\theta _{j}}^{2}m_{j}(X_{t},Z_{j,t-1},\widehat{\theta }_{j,R})\right)
^{-1}.
\end{equation*}%
In order to formalize the case of asymptotically non-vanishing parameter
estimation error, we require the following assumptions.

\noindent \textbf{Assumption A5: }$\phi _{j}$ is twice continuously
differentiable on the interior of $\Theta _{j}$ and the elements of $\nabla
_{\theta _{j}}\phi _{j}(Z_{j,i-1},\theta _{i})$ and $\nabla _{\theta
_{j}}^{2}\phi _{j}(Z_{j,i-1},\theta _{i})$ are $p-$dominated on $\Theta
_{i}, $ for $j=1,...,k,$ with $p>4.$

\noindent \textbf{Assumption A6: }For $j=1,...,k:$ (i) $\theta _{j}^{\dagger
}$ is uniquely identified (i.e. $E(m_{j}(X_{t},Z_{j,t-1},\theta
_{j}))>E(m_{j}(X_{t},Z_{j,t-1},\theta _{j}^{\dagger })),$ for any $\theta
_{j}\neq \theta _{j}^{\dagger });$ (ii) $m_{j}$ is twice continuously
differentiable on the interior of $\Theta _{j};$ (iii) the elements of $%
\nabla _{\theta _{j}}m_{j}$ and $\nabla _{\theta _{j}}^{2}m_{j}$ are $p-$%
dominated on $\Theta _{j},$ with $p>4$; and (iii) $E\left( -\nabla _{\theta
_{j}}^{2}m_{j}(\theta _{j})\right) $ is positive definite, uniformly on $%
\Theta _{j}.$\footnote{%
We say that $\nabla _{\theta _{j}}\ln f_{j}(y_{t},Z^{t-1},\theta _{j})$ is $%
2r-$dominated on $\Theta _{j}$ if its $v-th$ element, $v=1,...,\varrho (j),$
is such that $\left\vert \nabla _{\theta _{j}}\ln f_{j}(y_{t},Z^{t-1},\theta
_{j})\right\vert _{v}\leq D_{t},$ and $E(\left\vert D_{t}\right\vert
^{2r})<\infty .$ For more details on domination conditions, see Gallant and
White (1988, pp. 33).}

\noindent \textbf{Assumption A7: }$T=R+n,$ and as $T\rightarrow \infty ,$ $%
n/R\rightarrow \pi ,$ with $0\leq \pi <\infty .$

As explained earlier, it is crucial to have a consistent estimator of the
variance of the moment conditions. Otherwise, bootstrap critical values are
not scale invariant. Hence, we need to construct estimators which properly
capture parameters estimation error, regardless the fact that we rely on
bootstrap critical values. GMS tests in the presence of non-vanishing
estimation error have been considered in Coroneo, Corradi and
Santos-Monteiro (2019). We have the following result.

\noindent \textbf{Lemma 3: }\textit{Let Assumptions A1-A3, and A5-A7 hold.
If }$l_{n}\approx n^{\delta }$\textit{\ }$\delta <\frac{1}{2},$\textit{\ as
defined in Assumption A1}$,$\textit{\ then:}

\textit{\noindent (i) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{jj,n}^{2,G+}(x)-\omega _{jj}^{2,G+}(x)\right\vert =o_{p}\left(
1\right) ,$\textit{\ with }$\omega _{jj}^{2G+}(x)=avar\left( \sqrt{n}%
\widetilde{G}_{j,n}^{+}(x)\right) ;$\textit{\ and}

\textit{\noindent (ii) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{jj,n}^{2,C+}(x)-\omega _{jj}^{2,C+}(x)\right\vert =o_{p}\left(
1\right) $\textit{, with }$\omega _{jj}^{2C+}(x)=avar\left( \sqrt{n}%
\widetilde{C}_{j,n}^{+}(x)\right) .$

\noindent Lemma 3 mirrors Lemma 1 for the case of non-vanishing estimation
error. In order to provide the analog of Theorem 1, we need define the
counterparts of $S_{n}^{\dag G+}$ and $S_{n}^{\dag C+}$ which take into
account of parameter estimation error. Let $\overline{\Omega }^{G+}\left(
x,x\right) =\Omega ^{G+}\left( x,x\right) +\varepsilon I_{k-1},$ where $%
\Omega ^{G+}\left( x,x\right) =[\omega _{ij,n}^{2,G+}(x)].$ Also,

\begin{equation*}
\mathcal{D}^{G+}(x)=\mathrm{diag}\Omega ^{G+}\left( x,x\right) ,
\end{equation*}%
\begin{equation*}
h_{1,n}^{G+}(x)=\mathcal{D}^{G+}(x)^{-1/2}\left( \sqrt{n}G_{2}^{+}(x),...,%
\sqrt{n}G_{k}^{+}(x)\right) ^{\prime },
\end{equation*}

\begin{equation*}
h_{2}^{G+}(x,x^{\prime })=\mathcal{D}^{G+}(x)^{-1/2}\overline{\Omega }%
^{G+}\left( x,x^{\prime }\right) \mathcal{D}^{G+}(x^{\prime })^{-1/2},
\end{equation*}%
and%
\begin{equation*}
v^{G+}(.)=(v_{2}^{G+}(.),...,v_{k}^{G+}(.))^{\prime }.
\end{equation*}%
Here, $v^{G+}(.)$ is a ($k-1)-$dimensional zero mean Gaussian process with
correlation $h_{2}^{G+}(x,x^{\prime })$. Also, let $\mathcal{D}%
^{C+}(x),h_{1,n}^{C+}(x),h_{2}^{C+}(x,x^{\prime }),$ and $v^{C+}(.)$ be
defined analogously by replacing $\Omega ^{G+}\left( x,x\right)
,G_{2}^{+}(x),...,G_{k}^{+}(x)$ with $\Omega ^{C+}\left( x,x\right)
,C_{2}^{+}(x),...,C_{k}^{+}(x).$

Finally, define%
\begin{equation*}
S_{n}^{\ddag G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max \left\{ 0,%
\frac{v_{j}^{G+}(x)+h_{j,1,n}^{G+}(x)}{\sqrt{h_{jj,2}^{G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
where $h_{jj,2}^{G+}(x)$ is the $jj-$th element of $h_{2}^{G+}(x,x)$, and
let 
\begin{equation*}
S_{n}^{\ddag C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\left( \max \left\{ 0,%
\frac{v_{j}^{C+}(x)+h_{j,1,n}^{C+}(x)}{\sqrt{h_{jj,2}^{C+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
which is defined analogously, by replacing $v_{j}^{G+}(x),h_{j,1,n}^{G+}(x),$
and $h_{jj,2}^{G+}(x)$ with $v_{j}^{C+}(x),h_{j,1,n}^{C+}(x),$ and $%
h_{jj,2}^{C+}(x).$ The following result holds.

\noindent \textbf{Theorem 5: }\textit{Let Assumptions A1-A7 hold. }

\textit{(i) Under }$H_{0}^{G+},$\textit{\ there exist }$\delta >0$\textit{\
such that:}%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{G+}}\left[
P\left( \widetilde{S}_{n}^{G+}>a_{h_{A,n}}^{G+}\right) -P\left( S_{n}^{\ddag
G+}+\delta >a_{h_{A,n}}^{G+}\right) \right] \leq 0
\end{equation*}%
\textit{and}%
\begin{equation*}
\lim \inf_{n\rightarrow \infty }\inf_{P\in \mathcal{P}_{0}^{G+}}\left[
P\left( \widetilde{S}_{n}^{G+}>a_{h_{A,n}}^{G+}\right) -P\left( S_{n}^{\ddag
G+}-\delta >a_{h_{A,n}}^{G+}\right) \right] \geq 0.
\end{equation*}%
\textit{\noindent \qquad }

\textit{(ii) Under }$H_{0}^{C+},$\textit{\ there exist }$\delta >0$\textit{\
such that:}%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( \widetilde{S}_{n}^{C+}>a_{h_{A,n}}^{C+}\right) -P\left( S_{n}^{\ddag
C+}+\delta >a_{h_{A,n}}^{C+}\right) \right] \leq 0
\end{equation*}%
\textit{and}%
\begin{equation*}
\lim \inf_{n\rightarrow \infty }\inf_{P\in \mathcal{P}_{0}^{C+}}\left[
P\left( \widetilde{S}_{n}^{C+}>a_{h_{A,n}}^{C+}\right) -P\left( S_{n}^{\ddag
C+}-\delta >a_{h_{A,n}}^{C+}\right) \right] \geq 0.
\end{equation*}%
Theorem 5 provides upper and lower bounds for $P\left( \widetilde{S}%
_{n}^{G+}>a_{h_{A,n}}^{G+}\right) $ and $P\left( \widetilde{S}%
_{n}^{C+}>a_{h_{A,n}}^{C+}\right) ,$ uniformly, over the probabilities under
the null $H_{0}^{G+}$ and $H_{0}^{C+},$ respectively.

\subsection{Bootstrap Estimators}

When computing recursive $m-$estimators, it is important to note that
earlier observations are used more frequently than temporally subsequent
observations. On the other hand, in the standard block bootstrap, all blocks
from the original sample have the same probability of being selected,
regardless of the dates of the observations in the blocks. Thus, the
bootstrap estimator, say $\widehat{\theta }_{j,t}^{\ast },$ which is
constructed as a direct analog of $\widehat{\theta }_{j,t}$ in (\ref{theta-i}%
)$,$ is characterized by a location bias that can be either positive or
negative, depending on the sample that we observe. In order to circumvent
this problem, Corradi and Swanson (2007) suggest a re-centering of the
bootstrap score which ensures that the new bootstrap estimator is
asymptotically unbiased. Also, assume that $T=R+n=b_{T}l_{T},$ with $%
b_{T}=b_{n}\frac{T}{n}$ and $l_{T}=l_{n}\frac{T}{n},$ and define:%
\begin{equation*}
\widetilde{\theta }_{j,t}^{\ast }=\arg \min_{\theta _{j}\in \Theta _{j}}%
\frac{1}{t}\sum_{i=1}^{t}\left( m_{j}(X_{i}^{\ast },Z_{j,i-1}^{\ast },\theta
_{j})-\theta _{j}^{\prime }\left( \frac{1}{T}\sum_{k=1}^{T-1}\nabla _{\theta
_{j}}m_{j}(X_{k},Z_{j,k-1},\widehat{\theta }_{j,t})\right) \right) ,
\end{equation*}%
where $X_{i}^{\ast },Z_{j,i-1}^{\ast }$ are resampled via the
\textquotedblleft standard\textquotedblright\ block bootstrap outlined in
the previous section, but with block length $l_{T}.$ Theorem 1 in Corradi
and Swanson (2007) establish that $\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( 
\widetilde{\theta }_{j,t}^{\ast }-\widehat{\theta }_{j,t}\right) $ has the
same limiting distribution as $\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dagger }\right) ,$ conditional of the
sample.

With a slight abuse of notation, let $u_{j,t}^{\ast }(x)=1\{e_{j,t}^{\ast
}\leq x\}-\frac{1}{T}\sum_{t=1}^{T}1\{\widehat{e}_{j,t}\leq x\}$ and $\eta
_{j,t}^{\ast }(x)=\left[ e_{j,t}^{\ast }-x\right] _{+}-\frac{1}{T}%
\sum_{t=1}^{n}\left[ \widehat{e}_{j,t}-x\right] _{+},$ with $e_{j,t+1}^{\ast
}=X_{t+1}^{\ast }-\phi _{j}\left( Z_{j,t}^{\ast },\widehat{\theta }%
_{j,t}\right) $, and let $\widehat{u}_{j,t}^{\ast }(x)=1\{\widehat{e}%
_{j,t}^{\ast }\leq x\}-\frac{1}{T}\sum_{t=1}^{T}1\{\widehat{e}_{j,t}\leq x\}$
and $\widehat{\eta }_{j,t}^{\ast }(x)=\left[ \widehat{e}_{j,t}^{\ast }-x%
\right] _{+}-\frac{1}{T}\sum_{t=1}^{n}\left[ \widehat{e}_{j,t}-x\right]
_{+}, $ with $\widehat{e}_{j,t+1}^{\ast }=X_{t+1}^{\ast }-\phi _{j}\left(
Z_{j,t}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) .$ Our first goal
is to construct the bootstrap counterparts of $\widetilde{\sigma }%
_{jj,n}^{2,G+}(x)$ and $\widetilde{\sigma }_{jj,n}^{2,C+}(x)$, called $%
\widetilde{\sigma }_{jj,n}^{\ast 2,G+}(x)$ and $\widetilde{\sigma }%
_{jj,n}^{\ast 2,C+}(x).$ Define:

\begin{eqnarray*}
&&\widetilde{\sigma }_{jj,n}^{\ast 2,G+}(x) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{u}_{j,t}^{\ast }(x)-\widehat{u}_{1,t}^{\ast
}(x)\right) \right) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) \right) +%
\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}%
\left( \widehat{f}_{j,n,h}^{\ast }(x)\widehat{PEE^{\ast }}_{j,t}-\widehat{f}%
_{1,n,h}^{\ast }(x)\widehat{PEE^{\ast }}_{1,t}\right) \right) \\
&&-2\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) ,\frac{1%
}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{f}_{j,n,h}^{\ast }(x)\widehat{%
PEE^{\ast }}_{j,t}-\widehat{f}_{1,n,h}^{\ast }(x)\widehat{PEE^{\ast }}%
_{1,t}\right) \right) ,
\end{eqnarray*}%
and%
\begin{eqnarray*}
&&\widetilde{\sigma }_{jj,n}^{\ast 2,C+}(x) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{\eta }_{j,t}^{\ast }(x)-\widehat{\eta }%
_{1,t}^{\ast }(x)\right) \right) \\
&=&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast }(x)\right)
\right) \\
&&+\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \left[ \widehat{f}_{j,n,h}^{\ast }\widehat{PEE}%
_{j,t}^{\ast }-x\right] _{+}-\left[ \widehat{f}_{1,n,h}^{\ast }\widehat{PEE}%
_{1,t}^{\ast }-x\right] _{+}\right) \right) + \\
&&-2\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast }(x)\right) ,%
\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \left[ \widehat{f}_{j,n,h}^{\ast }%
\widehat{PEE}_{j,t}^{\ast }-x\right] _{+}-\left[ \widehat{f}_{1,n,h}^{\ast }%
\widehat{PEE}_{1,t}^{\ast }-x\right] _{+}\right) \right) ,
\end{eqnarray*}%
where \textrm{avar}$^{\ast }$ and $\mathrm{acov}^{\ast }$ denote asymptotic
variances and covariances, with respect to the bootstrap probability
measure, $\widehat{f}_{j,n,h}^{\ast }$ is an estimator of the density of $%
e_{j}$ based on the resampled observations, and $\widehat{PEE^{\ast }}_{j,t}$
is an estimator of:%
\begin{eqnarray}
PEE_{j,t}^{\ast } &=&\mathrm{E}^{\ast }\left( \nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) \right) 
\mathrm{E}^{\ast }\left( \nabla _{\theta }^{2}m_{j}\left( X_{i}^{\ast
},Z_{j,i-1}^{\ast },\widetilde{\theta }_{j,t}^{\ast }\right) \right)  \notag
\\
&&\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta }m_{j}\left( X_{i}^{\ast
},Z_{j,i-1}^{\ast },\widehat{\theta }_{j,t}\right) -\frac{1}{T}%
\sum_{i=1}^{T}\nabla _{\theta _{j}}m_{j}(X_{k},Z_{j,k-1},\widehat{\theta }%
_{j,t}\right) .  \label{PEE*}
\end{eqnarray}%
Closed form expressions for $\widehat{PEE^{\ast }}_{j,t}$, $\widehat{\mathrm{%
avar}^{\ast }}$, and $\widehat{\mathrm{acov}^{\ast }}$ are given in the
proof of Lemma 4.

\noindent \textbf{Lemma 4: }\textit{Let Assumptions A1-A3 and A5-A7 hold.
Then, if }$l_{n}\approx n^{\delta }$\textit{\ }$\delta <\frac{1}{2},$\textit{%
\ and }$\beta $\textit{\ the mixing coefficient in Assumption A1 is such
that }$\beta >\frac{6\delta }{1-2\delta }:$

\textit{\noindent (i) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{jj,n}^{\ast 2,G+}(x)-\mathrm{E}^{\ast }\left( \widetilde{\sigma }%
_{jj,n}^{\ast 2,G+}(x)\right) \right\vert =o_{p}\left( 1\right) $\textit{\
and}

\textit{\noindent (ii) }$\sup_{x\in \mathcal{X}^{+}}\left\vert \widetilde{%
\sigma }_{jj,n}^{\ast 2,C+}(x)-\mathrm{E}^{\ast }\left( \widetilde{\sigma }%
_{jj,n}^{\ast 2,C+}(x)\right) \right\vert =o_{p}\left( 1\right) .$\textit{\ }

\subsection{Bootstrap Critical Values}

The bootstrap statistics in the non-vanishing recursive parameter estimation
error case are:%
\begin{equation}
\widetilde{S}_{n}^{\ast G+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left(
\left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast G+}(x)-\widetilde{\phi }%
_{j,n}^{G+}(x)}{\sqrt{\widetilde{\overline{h}}_{2,jj}^{\ast G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),  \label{SG*PEE}
\end{equation}%
where$\widetilde{\text{ }\overline{h}}_{2,jj}^{\ast G+}(x)$ is the $jj$
element of $\widetilde{D}_{n}^{-1/2,G+}(x)\overline{\widetilde{\Sigma }}%
_{n}^{\ast G+}\left( x,x\right) \widetilde{D}_{n}^{-1/2,G+}(x),$ with $%
\widetilde{D}_{n}^{G+}(x)=\mathrm{diag}\widetilde{\Sigma }_{n}^{\ast
G+}\left( x,x\right) ,$ $\widetilde{\Sigma }_{n}^{\ast G+}\left( x,x\right)
=[\widetilde{\sigma }_{ij,n}^{\ast 2,G+}(x)]$ $i,j=1,...,k,$ and $\overline{%
\widetilde{\Sigma }}_{n}^{\ast G+}=\widetilde{\Sigma }_{n}^{\ast
G+}+\varepsilon I_{k-1}.$ Also,%
\begin{eqnarray*}
\widetilde{v}_{n}^{\ast G+}(x) &=&\sqrt{n}\widetilde{D}_{n}^{-1/2,G+}(x)%
\frac{1}{\sqrt{n}}\sum_{i=R+1}^{n}\left( \left( 1\left\{ \widehat{e}%
_{j,i}^{\ast }\leq x\right\} -1\left\{ \widehat{e}_{1,i}^{\ast }\leq
x\right\} \right) \right. \\
&&\left. \frac{1}{T}\sum_{t=1}^{T}\left( 1\left\{ \widehat{e}_{j,t}\leq
x\right\} -1\left\{ \widehat{e}_{1,t}\leq x\right\} \right) \right)
\end{eqnarray*}%
and for $\widetilde{\xi }_{j,n}^{G+}(x)=\kappa _{n}^{-1}n^{1/2}\overline{%
\widetilde{D}}_{jj,n}^{-1/2,G+}(x)\widetilde{G}_{j,n}^{+}(x),$%
\begin{equation}
\widetilde{\phi }_{j,n}^{G+}(x)=c_{n}1\left\{ \widetilde{\xi }%
_{j,n}^{G+}(x)<-1\right\} .  \label{phi-pee}
\end{equation}%
Finally, also define 
\begin{equation*}
\widetilde{S}_{n}^{\ast C+}=\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left(
\left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast C+}(x)-\widetilde{\phi }%
_{j,n}^{C+}(x)}{\sqrt{\widetilde{\overline{h}}_{2,jj}^{\ast C+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x),
\end{equation*}%
where $\widetilde{v}_{n}^{\ast C+}(x),\widetilde{D}_{n}^{C^{+}}(x),%
\widetilde{\xi }_{j,n}^{C+}(x),$ and $\widetilde{\phi }_{j,n}^{C+}(x)$ are
defined analogously to $\widetilde{v}_{n}^{\ast G+}(x),\widetilde{D}%
_{n}^{G+}(x),\widetilde{\xi }_{j,n}^{G+}(x),$ and $\widetilde{\phi }%
_{j,n}^{G+}(x)$. It is immediate to see that estimation error contributes to
the bootstrap statistics not only as a scaling factor, but also in
determining which moment conditions are binding. This is why we need an
estimator of the variance, even if inference is based on bootstrap critical
values.

We now define the GMS bootstrap critical values for the case of
non-vanishing recursive estimation error. Let $\widetilde{c}_{n,B,1-\alpha
}^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},\overline{h}_{2,n}^{\ast
G+}\right) $ be the $(1-\alpha )$-th \ critical value of $\widetilde{S}%
_{n}^{\ast G+},$ based on $B$ bootstrap replications, with $\widetilde{\phi }%
_{n}^{G+}$ as in (\ref{phi-pee}) and $\widetilde{\overline{h}}_{2,jj}^{\ast
G+}(x)$ as in (\ref{SG*PEE}). The ($1-\alpha )$-th GMS bootstrap critical
value, $\widetilde{c}_{0,n,1-\alpha }^{\ast G+}\left( \widetilde{\phi }%
_{n}^{G+},\overline{h}_{2,n}^{\ast G+}\right) $ is defined as:%
\begin{equation*}
\widetilde{c}_{0,n,1-\alpha }^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},%
\overline{h}_{B,n}^{\ast G+}\right) =\lim_{B\rightarrow \infty }\widetilde{c}%
_{n,B,1-\alpha +\eta }^{\ast G+}\left( \widetilde{\phi }_{n}^{G+},\overline{h%
}_{2,n}^{\ast G+}\right) +\eta ,
\end{equation*}%
for arbitrarily small $\eta >0$. Also, $\widetilde{c}_{n,B,1-\alpha +\eta
}^{\ast C+}\left( \widetilde{\phi }_{n}^{C+},\overline{h}_{2,n}^{\ast
C+}\right) $ and $\widetilde{c}_{0,n,1-\alpha }^{\ast C+}\left( \widetilde{%
\phi }_{n}^{C+},\overline{h}_{B,n}^{\ast C+}\right) $ are defined
analogously. The following result then holds.

\noindent \textbf{Theorem 6: }Let Assumptions A1-A7 hold, and let $%
l_{n}\rightarrow \infty $ and $l_{n}n^{\frac{1}{3}-\varepsilon }\rightarrow
0,$ as $n\rightarrow \infty .$ Under $H_{0}^{G+}:$

\noindent (i) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow \infty $
and $c_{n}/\kappa _{n}\rightarrow 0,$ then%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{G+}}P\left( 
\widetilde{S}_{n}^{G+}\geq \widetilde{c}_{n,B,1-\alpha +\eta }^{\ast
C+}\left( \widetilde{\phi }_{n}^{C+},\overline{h}_{2,n}^{\ast C+}\right)
\right) \leq \alpha ;
\end{equation*}%
\noindent and (ii) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow
\infty ,$ $c_{n}\rightarrow \infty $, $\sqrt{n}/\kappa _{n}\rightarrow
\infty $ and $Q\left( \mathcal{B}^{G+}\right) >0,$ $\mathcal{B}^{G+}$ as in (%
\ref{BG+}), then%
\begin{equation*}
\lim_{\eta \rightarrow 0}\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{%
P}_{0}^{G+}}P\left( \widetilde{S}_{n}^{G+}\geq \widetilde{c}_{n,B,1-\alpha
+\eta }^{\ast C+}\left( \widetilde{\phi }_{n}^{C+},\overline{h}_{2,n}^{\ast
C+}\right) \right) =\alpha .
\end{equation*}%
\noindent Also, under $H_{0}^{C+},$

\noindent (iii) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow \infty 
$ and $c_{n}/\kappa _{n}\rightarrow 0,$ then%
\begin{equation*}
\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{P}_{0}^{C+}}P\left( 
\widetilde{S}_{n}^{C+}\geq \widetilde{c}_{0,n,1-\alpha }^{\ast C+}\left( 
\widetilde{\phi }_{n}^{C+},\overline{h}_{B,n}^{\ast C+}\right) \right) \leq
\alpha ;
\end{equation*}%
\noindent and (iv) if as $n\rightarrow \infty ,$ $\kappa _{n}\rightarrow
\infty ,$ $c_{n}\rightarrow \infty $, $\sqrt{n}/\kappa _{n}\rightarrow
\infty $ and $Q\left( \mathcal{B}^{C+}\right) >0,$ $\mathcal{B}^{C+}$ as in (%
\ref{BC+}), then%
\begin{equation*}
\lim_{\eta \rightarrow 0}\lim \sup_{n\rightarrow \infty }\sup_{P\in \mathcal{%
P}_{0}^{C+}}P\left( \widetilde{S}_{n}^{C+}\geq \widetilde{c}_{0,n,1-\alpha
}^{\ast C+}\left( \widetilde{\phi }_{n}^{C+},\overline{h}_{B,n}^{\ast
C+}\right) \right) =\alpha .
\end{equation*}

Statements (i) and (iii) of Theorem 6 establish that inference based on GMS
bootstrap critical values has uniform correct size, in the parameter
estimation error case. Statements (ii) and (iv) of the theorem establish
that inference based on the GMS bootstrap critical values is asymptotically
non-conservative, whenever $Q\left( \mathcal{B}^{+}\right) >0$ or $Q\left( 
\mathcal{B}^{C+}\right) >0.$

\subsection{Proofs}

\noindent \textbf{Proof of Lemma 3: (i)} Letting $\overline{F}_{j}(x)=\frac{1%
}{n}\sum_{t=R}^{T-1}1\left\{ \widehat{e}_{j,t+1}\leq x\right\} ,$ by an
intermediate value expansion, in the case of a recursive estimation scheme,
we have that%
\begin{eqnarray}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ \widehat{e}_{j,t+1}\leq
x\right\} -F_{j}(x)\right)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ \widehat{%
e}_{j,t+1}\leq x\right\} -1\left\{ e_{j,t+1}\leq x\right\} \right)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{
e_{j,t+1}\leq x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{%
\theta }_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag
}\right) \right\} \right. \right.  \notag \\
&&\left. \left. -F_{j}\left( x-\nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( \widehat{\theta }%
_{j,t}-\theta _{j}^{\dag }\right) \right) \right) -\frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\} -F_{j}(x)\right)
\right)  \notag \\
&&+\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( F_{j}\left( x-\nabla _{\theta
_{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) \right) -F_{j}(x)\right) 
\notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) +\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( F_{j}\left(
x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }%
_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right)
\right) -F_{j}(x)\right)  \notag \\
&&+o_{p}(1)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) -f_{j}(x)\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\nabla _{\theta
_{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) \left( 
\widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) +o_{p}(1)  \notag \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right)  \label{der} \\
&&-f_{j}(x)\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\nabla _{\theta _{j}}\phi
_{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right) ^{\prime }\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) ^{-1}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) +o_{p}(1)  \notag
\\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x\right\}
-F_{j}(x)\right) -f_{j}(x)\widehat{A}_{j}\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}%
\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) +o_{p}(1)  \notag
\end{eqnarray}%
where the $o_{p}(1)$ term on the RHS of the third equality in (\ref{der})
comes from the fact that%
\begin{eqnarray*}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( 1\left\{ e_{j,t+1}\leq x-\nabla
_{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\overline{\theta }_{j,t}\right)
\left( \widehat{\theta }_{j,t}-\theta _{j}^{\dag }\right) \right\} \right. \\
&&\left. -F_{j}\left( x-\nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},%
\overline{\theta }_{j,t}\right) \left( \widehat{\theta }_{j,t}-\theta
_{j}^{\dag }\right) \right) \right) -\frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}%
\left( 1\left\{ e_{j,t+1}\leq x\right\} -F_{j}(x)\right) =o_{p}(1),
\end{eqnarray*}%
because of stochastic equicontinuity.

Hence,%
\begin{eqnarray*}
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ \widehat{e}_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{ 
\widehat{e}_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \right) \\
&=&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \right) \\
&&+f_{1}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{1}}m_{1}(X_{i},Z_{1,i-1},\theta
_{1}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left( Z_{1,t+1},\theta
_{1}^{\dagger }\right) \right) \\
&&+f_{j}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\theta _{j}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{var}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\theta
_{j}^{\dagger }\right) \right) \\
&&-2f_{1}(x)f_{j}(x)\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{j}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{1}}m_{1}(X_{i},Z_{1,i-1},\theta
_{1}^{\dagger })\right) \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\frac{1}{t}%
\sum_{i=1}^{t}\left( \nabla _{\theta _{j}}m_{j}(X_{i},Z_{j,i-1},\theta
_{j}^{\dagger })\right) \right) \\
&&\left( \mathrm{E}\left( \nabla _{\theta
_{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right) ^{-1}%
\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left( Z_{j,t+1},\theta
_{j}^{\dagger }\right) \right) \\
&&+2f_{1}(x)\mathrm{E}\left( \nabla _{\theta _{1}}\phi _{1}\left(
Z_{1,t+1},\theta _{1}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{1}}^{2}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{1}}m_{1}(X_{i},Z_{1,i-1},\theta _{1}^{\dagger })\right) \right) \\
&&-2f_{j}(x)^{2}\mathrm{E}\left( \nabla _{\theta _{j}}\phi _{j}\left(
Z_{j,t+1},\theta _{j}^{\dagger }\right) \right) ^{\prime }\left( \mathrm{E}%
\left( \nabla _{\theta _{j}}^{2}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger
})\right) \right) ^{-1} \\
&&\mathrm{cov}\left( \frac{1}{\sqrt{n}}\sum_{t=R}^{T-1}\left( \left(
1\left\{ e_{1,t+1}\leq x\right\} -F_{1}(x)\right) -\left( 1\left\{
e_{j,t+1}\leq x\right\} -F_{j}(x)\right) \right) \frac{1}{\sqrt{n}}%
\sum_{t=R}^{T-1}\frac{1}{t}\sum_{i=1}^{t}\left( \nabla _{\theta
_{j}}m_{j}(X_{i},Z_{j,i-1},\theta _{j}^{\dagger })\right) \right)
\end{eqnarray*}%
(ii) Recalling (\ref{C-PEE}) by a similar argument as in part (i).

\medskip

\noindent \textbf{Proof of Theorem 5:}

\noindent Given Lemma 3, the statement follows by the same argument as in
Theorem 1.

\medskip

\noindent \textbf{Proof of Lemma 4:}

\noindent (i) Note that $\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{%
\sqrt{n}}\sum_{t=R}^{n-1}\left( \eta _{j,t}^{\ast }(x)-\eta _{1,t}^{\ast
}(x)\right) \right) =\widehat{\sigma }_{jj,n}^{2\ast G+}(x)$ as defined in (%
\ref{sigmaG*}), $\widehat{PEE^{\ast }}_{j,t}$ is defined as $PEE_{j,t}^{\ast
}$ with \textrm{E}$^{\ast }$ replaced by an average, also%
\begin{eqnarray*}
&&\widehat{\mathrm{avar}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-\widehat{PEE^{\ast }}%
_{1,t}\right) \right) \\
&=&\frac{1}{b_{n}}\sum_{k=1}^{b_{n}}\left( \frac{1}{l_{n}^{1/2}}%
\sum_{i=1}^{l_{n}}\left( \widehat{PEE^{\ast }}_{j,(k-1)\NEG{l}_{n}+i}-%
\widehat{PEE^{\ast }}_{j,(k-1)\NEG{l}_{n}+i}\right) \right) ^{2}
\end{eqnarray*}%
and by Theorem 1 in Corradi and Swanson (2007),%
\begin{eqnarray*}
&&\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-%
\widehat{PEE^{\ast }}_{1,t}\right) \\
&=&\frac{1}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE}_{j,t}-\widehat{PEE}%
_{1,t}\right) +o_{p}(1)^{\ast }.
\end{eqnarray*}%
\begin{eqnarray*}
&&\widehat{\mathrm{acov}^{\ast }}\left( \frac{1}{\sqrt{n}}%
\sum_{t=R}^{n-1}\left( u_{j,t}^{\ast }(x)-u_{1,t}^{\ast }(x)\right) ,\frac{1%
}{\sqrt{n}}\sum_{t=R}^{n-1}\left( \widehat{PEE^{\ast }}_{j,t}-\widehat{%
PEE^{\ast }}_{1,t}\right) \right) \\
&=&\frac{1}{b_{n}}\sum_{k=1}^{b_{n}}\left( \frac{1}{l_{n}^{1/2}}%
\sum_{i=1}^{l_{n}}\left( \widehat{PEE^{\ast }}_{j,(k-1)\NEG{l}_{n}+i}-%
\widehat{PEE^{\ast }}_{j,(k-1)\NEG{l}_{n}+i}\right) \right. \\
&&\left. \frac{1}{l_{n}^{1/2}}\sum_{i=1}^{l_{n}}\left( u_{j,t}^{\ast
}(x)-u_{1,t}^{\ast }(x)\right) \right)
\end{eqnarray*}%
and for $h\rightarrow 0,$ $nh\rightarrow \infty ,$ $\widehat{f}%
_{j,n,h}^{\ast }(x)=\widehat{f}_{j,n,h}(x)+o_{p^{\ast
}}(1)=f(x)+o_{p}(1)+o_{p^{\ast }}(1).$ The statement then follow by the same
argument as in Lemma 2 and Lemma 3.

\noindent (ii) By a similar argument as in Part (i).

\medskip

\noindent \textbf{Proof of Theorem 6:}

\noindent (i) By a similar argument as in the proof of Theorem 2 in Corradi
and Swanson (2007),%
\begin{eqnarray*}
\widetilde{S}_{n}^{\ast G+} &=&\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max
\left( \left\{ 0,\frac{\widetilde{v}_{j,n}^{\ast G+}(x)-\widetilde{\phi }%
_{j,n}^{G+}(x)}{\sqrt{\widetilde{\overline{h}}_{2,jj}^{\ast G+}(x)}}\right\}
\right) ^{2}\mathrm{d}Q(x) \\
&=&\int_{\mathcal{X}^{+}}\sum_{j=2}^{k}\max \left( \left\{ 0,\frac{%
\widetilde{v}_{j,n}^{G+}(x)-\widetilde{\phi }_{j,n}^{G+}(x)}{\sqrt{%
\widetilde{\overline{h}}_{2,jj}^{G+}(x)}}\right\} \right) ^{2}\mathrm{d}%
Q(x)+o_{p^{\ast }}(1)
\end{eqnarray*}%
The statement then follows from Lemma 4 and Theorem 2.

\noindent (ii) By a similar argument as in Part (i).

\section{Additional Monte Carlo Experimental Results}

In this section, experimental results are tabulated for the following DGPs:

\begin{equation*}
\widetilde{e}_{i,t}=\varrho \widetilde{e}_{i,t-1}+(1-\varrho
^{2})^{1/2}\eta_{i,t}, \text{with }\eta _{kt}\sim i.i.d.N(0,1)\text{ }%
i=1,...,5
\end{equation*}

\noindent DGP11: $e_{1t}$ $=\widetilde{e}_{1,t}$ and $e_{kt}$ $=\widetilde{e}%
_{k,t},$ $k=2,3,4,5.$

\noindent DGP12: $e_{1t}$ $=\widetilde{e}_{1,t}$, $e_{kt}$ $=\widetilde{e}%
_{k,t},$ $k=2,3$ and $e_{kt}$ $=1.4\widetilde{e}_{k,t},$ $k=4,5$

\noindent DGP13: $e_{1t}$ $=\widetilde{e}_{1,t}$, $e_{kt}$ $=0.8\widetilde{e}%
_{k,t},$ $k=2,3$ and $e_{kt}$ $=1.2\widetilde{e}_{k,t},$ $k=4,5.$

\noindent DGP14: $e_{1t}$ $=\widetilde{e}_{1,t}$, $e_{kt}$ $=\widetilde{e}%
_{k,t},$ $k=2,3$ and $e_{kt}$ $=0.6\widetilde{e}_{k,t},$ $k=4,5.$

See Tables S1 and S2 for tabulated results, and Section 4 of the paper for
complete details regarding the experiments that were run.

\section{Additional Empirical Results}

Tables S3 and S4 gather root mean square forecast errors associated with the
models reported on in Tables 3 and 4 of the paper. See Section 5 of the
paper for a complete discussion.

\newpage

\bigskip

\begin{center}
\linespread{1.1}

\begin{table}[tbp]
\caption{Supplemental S1 -- Monte Carlo Results for $JCS_n^{G+}$, $JCS_n^{G-}
$, $JCS_n^{C+}$, and $JCS_n^{C-}$ Forecast Superiority Tests$^*$}{\centering}
\par
\hspace{1cm}
\par
\begin{tabular}{cc|cccc|cccc}
\hline\hline
$DGP$ & $n$ & {$J_n = 0.20$} & {$J_n = 0.35$} & {$J_n = 0.50$} & {$J_n =
0.65 $} & {$J_n = 0.20$} & {$J_n = 0.35$} & {$J_n = 0.50$} & {$J_n = 0.65$}
\\ 
&  & \multicolumn{4}{c}{GL Forecast Superiority} & \multicolumn{4}{c}{CL
Forecast Superiority} \\ \hline
&  & \multicolumn{8}{c}{$Empirical$ $Size$} \\ \hline
DGP11 & 300 & 0.125 & 0.125 & 0.135 & 0.150 & 0.115 & 0.135 & 0.140 & 0.150
\\ 
& 600 & 0.115 & 0.130 & 0.160 & 0.155 & 0.115 & 0.115 & 0.130 & 0.145 \\ 
& 900 & 0.095 & 0.115 & 0.110 & 0.115 & 0.125 & 0.135 & 0.165 & 0.170 \\ 
\hline
DGP12 & 300 & 0.075 & 0.070 & 0.075 & 0.090 & 0.030 & 0.030 & 0.050 & 0.055
\\ 
& 600 & 0.085 & 0.090 & 0.100 & 0.110 & 0.015 & 0.025 & 0.030 & 0.035 \\ 
& 900 & 0.070 & 0.065 & 0.080 & 0.090 & 0.020 & 0.020 & 0.020 & 0.025 \\ 
\hline
&  & \multicolumn{8}{c}{$Empirical$ $Power$} \\ \hline
DGP13 & 300 & 0.385 & 0.430 & 0.460 & 0.490 & 0.650 & 0.660 & 0.700 & 0.745
\\ 
& 600 & 0.720 & 0.735 & 0.745 & 0.780 & 0.945 & 0.950 & 0.960 & 0.965 \\ 
& 900 & 0.900 & 0.915 & 0.920 & 0.935 & 0.995 & 1.000 & 1.000 & 1.000 \\ 
\hline
DGP14 & 300 & 0.995 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.1in}
{\noindent $^{*}$ Notes: Entries denote rejection frequencies of ($JCS_n^{G+}$,$JCS_n^{G-}$) tests (i.e., GL forecast superiority) and ($JCS_n^{C+}$,$JCS_n^{C-}$) tests 
(i.e., CL forecast superiority) under a variety of data generating processes 
denoted by DGP11-DGP14. In DGP11-DGP12, no alternative outperforms the benchmark model. In DGP13-DGP14, at least one alternative model outperfroms the benchmark model.
Sample sizes include $n$=300, 600, and 900 observations, as indicated in the second column of entries in the table. Nominal test size is 10\%, and tests 
are carried out using critical values 
constructed for values of $J_n$ including 0.20, 0.35, 0.50, and 0.65. See Section 4 for complete details.}
\end{minipage}
\end{table}

\begin{table}[tbp]
\caption{Supplemental S2 -- Monte Carlo Results for $S_n^{G+}$, $S_n^{G-}$, $%
S_n^{C+}$, and $S_n^{C-}$ Forecast Superiority Tests$^*$}{\centering}
\par
\hspace{1cm}
\par
\begin{tabular}{cc|cccc|cccc}
\hline\hline
$DGP$ & $n$ & {$\eta = 0.0015$} & {$\eta = 0.002$} & {$\eta = 0.0025$} & {$%
\eta = 0.003$} & {$\eta = 0.0015$} & {$\eta = 0.002$} & {$\eta = 0.0025$} & {%
$\eta = 0.003$} \\ 
&  & \multicolumn{4}{c}{GL Forecast Superiority} & \multicolumn{4}{c}{CL
Forecast Superiority} \\ \hline
&  & \multicolumn{8}{c}{$Empirical$ $Size$} \\ \hline
DGP11 & 300 & 0.075 & 0.075 & 0.075 & 0.074 & 0.082 & 0.082 & 0.082 & 0.082
\\ 
& 600 & 0.114 & 0.114 & 0.113 & 0.112 & 0.099 & 0.099 & 0.098 & 0.098 \\ 
& 900 & 0.123 & 0.122 & 0.122 & 0.120 & 0.153 & 0.153 & 0.152 & 0.151 \\ 
DGP12 & 300 & 0.034 & 0.033 & 0.032 & 0.031 & 0.040 & 0.037 & 0.037 & 0.036
\\ 
& 600 & 0.076 & 0.076 & 0.076 & 0.075 & 0.102 & 0.102 & 0.102 & 0.102 \\ 
& 900 & 0.092 & 0.092 & 0.091 & 0.091 & 0.110 & 0.110 & 0.110 & 0.110 \\ 
\hline
&  & \multicolumn{8}{c}{$Empirical$ $Power$} \\ \hline
DGP13 & 300 & 0.896 & 0.895 & 0.895 & 0.895 & 0.952 & 0.952 & 0.952 & 0.952
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
DGP14 & 300 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000
\\ 
& 600 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
& 900 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 & 1.000 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.1in}
{\noindent $^{*}$ Notes: Entries denote rejection frequencies of ($S_n^{G+}$,$S_n^{G-}$) tests (i.e., GL forecast superiority) and ($S_n^{C+}$,$S_n^{C-}$) tests (i.e., CL forecast superiority) under a variety of data generating processes 
denoted by DGP11-DGP14. In DGP11-DGP12, no alternative outperforms the benchmark model. In DGP13-DGP14, at least one alternative model outperfroms the benchmark model.
Sample sizes include $n$=300, 600, and 900 observations, as indicated in the second column of entries in the table. Nominal test size is 10\%, and tests are carried out using critical values 
constructed for values of $\eta$ including 0.0015, 0.002, 0.0025, and 0.0030. See Section 4 for complete details.}
\end{minipage}
\end{table}

\bigskip

\linespread{1.05}

\linespread{1.05} 
\begin{table}[tbp]
\caption{Supplemental S3 -- SPF Forecast Pooling Analysis of Quarterly
Nominal GDP Using Mean Benchmark Model and Mean Expert Pool Predictions$^*$}{%
\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007237 & 0.012054 & 0.016277 & 0.020490 & 0.024470 \\ 
& alternative 2 & 0.007264 & 0.012108 & 0.016358 & 0.020575 & 0.024611 \\ 
& alternative 3 & 0.007272 & 0.012141 & 0.016328 & 0.020596 & 0.025276 \\ 
\hline
Group 2 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
\hline
Group 3 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007517 & 0.012197 & 0.015320 & 0.019503 & 0.023099 \\ 
& alternative 2 & 0.007128 & 0.012595 & 0.016236 & 0.021133 & 0.024353 \\ 
& alternative 3 & 0.007110 & 0.012292 & 0.016534 & 0.020799 & 0.024430 \\ 
\hline
Group 4 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007811 & 0.011955 & 0.015533 & 0.019227 & 0.022871 \\ 
& alternative 2 & 0.006971 & 0.012656 & 0.017039 & 0.021008 & 0.026196 \\ 
& alternative 3 & 0.007306 & 0.012764 & 0.017251 & 0.021353 & 0.025116 \\ 
\hline
Group 5 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007257 & 0.012007 & 0.015367 & 0.019122 & 0.023173 \\ 
& alternative 2 & 0.007197 & 0.012401 & 0.016357 & 0.020830 & 0.024536 \\ 
& alternative 3 & 0.007219 & 0.012215 & 0.016720 & 0.020553 & 0.024250 \\ 
\hline
Group 6 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007237 & 0.012054 & 0.016277 & 0.020490 & 0.024470 \\ 
& alternative 2 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 3 & 0.007517 & 0.012197 & 0.015320 & 0.019503 & 0.023099 \\ 
& alternative 4 & 0.007811 & 0.011955 & 0.015533 & 0.019227 & 0.022871 \\ 
& alternative 5 & 0.007257 & 0.012007 & 0.015367 & 0.019122 & 0.023173 \\ 
\hline
Group 7 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007264 & 0.012108 & 0.016358 & 0.020575 & 0.024611 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007128 & 0.012595 & 0.016236 & 0.021133 & 0.024353 \\ 
& alternative 4 & 0.006971 & 0.012656 & 0.017039 & 0.021008 & 0.026196 \\ 
& alternative 5 & 0.007197 & 0.012401 & 0.016357 & 0.020830 & 0.024536 \\ 
\hline
Group 8 & benchmark & 0.007245 & 0.012063 & 0.016287 & 0.020506 & 0.024494
\\ 
& alternative 1 & 0.007272 & 0.012141 & 0.016328 & 0.020596 & 0.025276 \\ 
& alternative 2 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
& alternative 3 & 0.007110 & 0.012292 & 0.016534 & 0.020799 & 0.024430 \\ 
& alternative 4 & 0.007306 & 0.012764 & 0.017251 & 0.021353 & 0.025116 \\ 
& alternative 5 & 0.007219 & 0.012215 & 0.016720 & 0.020553 & 0.024250 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}
\vspace{0.05in}
{\noindent $^{*}$ Notes: Entries are root mean square forecast errors (RMSFEs) of benchmark and alternative forecasting models for $h=${0,1,2,3,4}. Rejections of the null of no forecast superiority based on 
$S_n$ tests at a 10\% level are denoted by a superscipt on the benchmark model RMSFE - a 1 denotes rejection based on the GL test, and a 2 denotes rejection based on the CL test. 
Analogous rejections based on application of the $J_n$ test are denoted by superscipts 3 and 4. 
See Section 5 for complete details.}
\end{minipage}
\end{table}

\bigskip

\begin{table}[tbp]
\caption{Supplemental S4 -- SPF Forecast Pooling Analysis of Quarterly
Nominal GDP Using Median Benchmark Model and Median Expert Pool Predictions$%
^*$}{\centering}
\par
\hspace{0.3cm}
\par
\begin{tabular}{cl|lllll}
\hline\hline
$Group$ & $Model$ & \multicolumn{5}{c}{$Forecast$ $Horizon$} \\ \hline
&  & $h=0$ & $h=1$ & $h=2$ & $h=3$ & $h=4$ \\ \hline\hline
Group 1 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007301 & 0.012033 & 0.016464 & 0.020595 & 0.024785 \\ 
& alternative 2 & 0.007340 & 0.012049 & 0.016419 & 0.020566 & 0.024728 \\ 
& alternative 3 & 0.007364 & 0.012067 & 0.016498 & 0.020632 & 0.025513 \\ 
\hline
Group 2 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
\hline
Group 3 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007825 & 0.012264 & 0.016016 & 0.019494 & 0.023117 \\ 
& alternative 2 & 0.007291 & 0.012425 & 0.016649 & 0.021205 & 0.024090 \\ 
& alternative 3 & 0.007372 & 0.012410 & 0.016961 & 0.021245 & 0.024526 \\ 
\hline
Group 4 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007893 & 0.012048 & 0.015983 & 0.018890 & 0.022662 \\ 
& alternative 2 & 0.007058 & 0.012606 & 0.017110 & 0.021198 & 0.026052 \\ 
& alternative 3 & 0.007231 & 0.012771 & 0.017289 & 0.021477 & 0.025041 \\ 
\hline
Group 5 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007459 & 0.011891 & 0.015555 & 0.019435 & 0.023215 \\ 
& alternative 2 & 0.007197 & 0.012299 & 0.016555 & 0.020814 & 0.024760 \\ 
& alternative 3 & 0.007400 & 0.012164 & 0.017024 & 0.020698 & 0.024653 \\ 
\hline
Group 6 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007301 & 0.012033 & 0.016464 & 0.020595 & 0.024785 \\ 
& alternative 2 & 0.008794 & 0.012535 & 0.016267 & 0.021398 & 0.026000 \\ 
& alternative 3 & 0.007825 & 0.012264 & 0.016016 & 0.019494 & 0.023117 \\ 
& alternative 4 & 0.007893 & 0.012048 & 0.015983 & 0.018890 & 0.022662 \\ 
& alternative 5 & 0.007459 & 0.011891 & 0.015555 & 0.019435 & 0.023215 \\ 
\hline
Group 7 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007340 & 0.012049 & 0.016419 & 0.020566 & 0.024728 \\ 
& alternative 2 & 0.007476 & 0.012562 & 0.017698 & 0.021331 & 0.028178 \\ 
& alternative 3 & 0.007291 & 0.012425 & 0.016649 & 0.021205 & 0.024090 \\ 
& alternative 4 & 0.007058 & 0.012606 & 0.017110 & 0.021198 & 0.026052 \\ 
& alternative 5 & 0.007197 & 0.012299 & 0.016555 & 0.020814 & 0.024760 \\ 
\hline
Group 8 & benchmark & 0.007300 & 0.012036 & 0.016398 & 0.020542 & 0.024699
\\ 
& alternative 1 & 0.007364 & 0.012067 & 0.016498 & 0.020632 & 0.025513 \\ 
& alternative 2 & 0.007602 & 0.012931 & 0.018113 & 0.022797 & 0.025831 \\ 
& alternative 3 & 0.007372 & 0.012410 & 0.016961 & 0.021245 & 0.024526 \\ 
& alternative 4 & 0.007231 & 0.012771 & 0.017289 & 0.021477 & 0.025041 \\ 
& alternative 5 & 0.007400 & 0.012164 & 0.017024 & 0.020698 & 0.024653 \\ 
\hline\hline
\end{tabular}%
\par
\begin{minipage}{1\columnwidth}

{\noindent $^{*}$ Notes: See notes to Table Supplemental S3.}
\end{minipage}
\end{table}
\end{center}

\end{document}
